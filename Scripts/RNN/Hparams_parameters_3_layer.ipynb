{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hparams_parameters 3 layer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGMDajrg_Oa1"
      },
      "source": [
        "Grid search code so we can tune hyper-parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyEQkMNVxPS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c7cc3f-1acd-41e3-ee41-8e2652cfc78d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "#@title Import Statements`\n",
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pprint import pprint # For pretty printing\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.7/dist-packages (1.54.1)\n",
            "Requirement already satisfied: pyjwt<2.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.7.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.2.12)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HfS1LmNxiMp"
      },
      "source": [
        "#@title Function Definitions\n",
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "        print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Commiting files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtfMMwOdxmt_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "3b7ed120-fb72-46e4-9f95-b60b68b48c71"
      },
      "source": [
        "# Import all of Mike's lyrics. PATKEY: 5ae2446bd5828c9e27deb3865118d9e783aa6e15\n",
        "import_github()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bc341381dc04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import all of Mike's lyrics. PATKEY: 5ae2446bd5828c9e27deb3865118d9e783aa6e15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimport_github\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-12a08b33c8fa>\u001b[0m in \u001b[0;36mimport_github\u001b[0;34m(path_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGithub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your PAT key \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enter your PAT Key.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MikeMNelhams\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmain_branch_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Main Branch: Yes or No? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Github' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VjURXnfxsKJ"
      },
      "source": [
        "Text = open(\"AllLyrics.txt\", \"r\").read()\n",
        "# turn text to lower case to reduce vocabulary\n",
        "Text = Text.lower()\n",
        "with open(\"AllLyrics.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "# bars is a list containing each line in dataset in lowercase\n",
        "bars = [x.strip().lower() for x in content]\n",
        "stripped_bars = [word.split() for word in bars]\n",
        "# Vocabulary is a list of all words in the dataset\n",
        "Vocabulary = ''.join([i for i in Text]).replace(\"\\n\",\" \").split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLpVvXPQ2rHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4873db-eaee-430c-d396-e51b89c9f308"
      },
      "source": [
        "# The numbers of bars in our dataset, 5283\n",
        "no_of_bars = len(bars)\n",
        "print(no_of_bars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRyjmQeyaAG"
      },
      "source": [
        "# word_count is a function creating a list of words ranked in order of most used\n",
        "# could think about removing certain words to create more accurate raps as model won't learn well from words used very infrequently\n",
        "def word_count(lyrics):\n",
        "  a = {}\n",
        "  for word in Vocabulary:\n",
        "    if word in a:\n",
        "      a[word] += 1\n",
        "    else:\n",
        "      a[word] = 1\n",
        "  return a\n",
        "word_dict = word_count(Vocabulary)\n",
        "sort_dict = sorted(word_dict.items(), key = lambda x: x[1], reverse = True)\n",
        "# Top 20 words\n",
        "sort_dict1 = sort_dict[:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIweRLWJqfax"
      },
      "source": [
        "# Need to create dictionary listing the words in alphabetical order so we can assign unique integers to each word\n",
        "# Neural networks take in integers, not words\n",
        "words = sorted(list(set(Vocabulary)))\n",
        "# Create a dictionary whereby we can convert integers into words\n",
        "word_to_int = { words[i] : i for i in range(len(words))}\n",
        "# Need to reverse this at the end to reverse numbers back into words\n",
        "int_to_word = { i : words[i] for i in range(len(words))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzfKleYrygft"
      },
      "source": [
        "# create a function that converts bars into a sequence of unique integers\n",
        "def words_to_integers(bar, Vocabulary):\n",
        "  encode = []\n",
        "  # Need to strip bar into single list of words within bar\n",
        "  stripped_bar = [word.split() for word in bar]\n",
        "  for i in range(no_of_bars):\n",
        "    seq = []\n",
        "    seq.append([word_to_int[word] for word in stripped_bar[i]])\n",
        "    encode.append(seq)\n",
        "\n",
        "  encode = sum(encode, [])\n",
        "  return encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RHaklDv5tZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738bbf47-33a0-4490-97c6-61613a1deb2b"
      },
      "source": [
        "# Number of unique words in our dataset\n",
        "vocab_size = len(words) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJs1kvSVAhZO"
      },
      "source": [
        "# Define a function that converts sentences into a sequence of corresponding integers\n",
        "def sentence_to_integer(bar):\n",
        "    stripped_bar = [word.split() for word in [bar]]\n",
        "    stripped_bar = sum(stripped_bar, [])\n",
        "    seq = []\n",
        "    seq.append([word_to_int[word] for word in stripped_bar])\n",
        "    seq = sum(seq, [])\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Log92J2ZdU"
      },
      "source": [
        "# Want to convert bars into n-grams of increasing length\n",
        "# So we start with the first two words and create lists of increasing length after adding the next word\n",
        "sequences = []\n",
        "for line in bars:\n",
        "    token_list = sentence_to_integer(line)\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i+1]\n",
        "        sequences.append(n_gram_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABAKPmzErRT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb3eed8-9f6b-4768-c25c-00f95418c588"
      },
      "source": [
        "# We need to pad each line so that each line is of equal length for our model\n",
        "# Thus need to find max length of a bar so we can pad all bars to this length\n",
        "padding_length = max([len(line) for line in sequences])\n",
        "print(padding_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PV9FBI_EsEQ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences = np.array(pad_sequences(sequences, maxlen = padding_length, padding = 'pre'))\n",
        "# Remove last word from each line\n",
        "x_train = sequences[:,:-1]\n",
        "no_train_ex = len(x_train)\n",
        "# Last word is used as the label\n",
        "y_train = sequences[:,-1]\n",
        "# one hot encode the the outputs \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dnpWnIJwMpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e648bc68-668d-47a3-b59a-9bf7305d698b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
        "x_train, x_val, y_train, y_val  = train_test_split(x_train,y_train, test_size=0.2)\n",
        "print(no_train_ex, 'no. of bars')\n",
        "print(len(x_train), 'train examples')\n",
        "print(len(x_val), 'validation examples')\n",
        "print(len(x_test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85829 no. of bars\n",
            "54930 train examples\n",
            "13733 validation examples\n",
            "17166 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6CXR4byZ-7M"
      },
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is1m7Au7YywA"
      },
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128, 256, 512]))\n",
        "HP_NUM_UNITS1 = hp.HParam('num_units1', hp.Discrete([128, 256, 512]))\n",
        "HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([128, 256, 512]))\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_NUM_UNITS1, HP_NUM_UNITS2],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWoLXV1Qr4E9"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
        "def our_model(hparams):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 512, input_length = padding_length - 1))\n",
        "  model.add(Bidirectional(LSTM(hparams[HP_NUM_UNITS],return_sequences = True)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Bidirectional(LSTM(hparams[HP_NUM_UNITS1], return_sequences=True)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Bidirectional(LSTM(hparams[HP_NUM_UNITS2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(vocab_size, activation = 'softmax'))\n",
        "\n",
        "  model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "  \n",
        "  model.fit(x_train, y_train,\n",
        "            validation_data = (x_val, y_val),\n",
        "            epochs=20, batch_size = 256)\n",
        "  _, accuracy = model.evaluate(x_val, y_val)\n",
        "  return accuracy\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQXUtCQBnob0"
      },
      "source": [
        "\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = our_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2i3meGmph4U"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for num_units1 in HP_NUM_UNITS1.domain.values:\n",
        "   for num_units2 in HP_NUM_UNITS2.domain.values:\n",
        "\n",
        "    hparams = {\n",
        "      HP_NUM_UNITS: num_units,\n",
        "      HP_NUM_UNITS1: num_units1,\n",
        "      HP_NUM_UNITS2: num_units2,\n",
        "    }  \n",
        "    run_name = \"run-%d\" % session_num\n",
        "    print('--- Starting trial: %s' % run_name)\n",
        "    print({h.name: hparams[h] for h in hparams})\n",
        "    run('logs/hparam_tuning_3/' + run_name, hparams)\n",
        "    session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onFpiKnbSEEa"
      },
      "source": [
        "%tensorboard --logdir logs/hparam_tuning_bi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrSCdGazUSr2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
