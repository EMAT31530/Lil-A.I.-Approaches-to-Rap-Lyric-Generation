{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SyllNNModel3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsnsZh72-hH0"
      },
      "source": [
        "# Markov with Syllable Neural Network\n",
        "# Same rhyme ranker as MikesVersion1\n",
        "# Changelog:\n",
        "# Uses early stopping to stop training when accuracy stops improving"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQph78-g-hIE",
        "outputId": "7971e7e4-ae66-4238-b721-70d033031844"
      },
      "source": [
        "#@title Import Statements\n",
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pprint import pprint # For pretty printing\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n",
        "\n",
        "# For the Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# For importing the training, testing and validation data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyGithub\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/44/df78514f2b5f5abaec330596e0fa3273824238399a964d1a7e82fd39990d/PyGithub-1.54.1-py3-none-any.whl (289kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 23.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 17.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102kB 12.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174kB 12.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 194kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 245kB 12.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 286kB 12.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 12.3MB/s \n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Collecting pyjwt<2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Installing collected packages: deprecated, pyjwt, PyGithub\n",
            "Successfully installed PyGithub-1.54.1 deprecated-1.2.12 pyjwt-1.7.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N624BVtBsmy"
      },
      "source": [
        "#@title Function Definitions\n",
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"ye\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "    print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Committing files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5On3o1fS-hIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c55603-b6e4-4f5c-b854-599c6d71b32b"
      },
      "source": [
        "# Import all of Mike's lyrics.\n",
        "import_github()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key ··········\n",
            "Main Branch: Yes or No? y\n",
            "Importing Github cleaned text files...\n",
            "AllLyrics.txt is now up to date!\n",
            "Writing file 0 capitals.csv\n",
            "Writing file 1 censors.csv\n",
            "Writing file 2 censors2.csv\n",
            "All files now up to date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WahREYjCV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e11c969-c96d-4a47-dc8b-40da899699db"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/training_data.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/testing_data.csv')\n",
        "validation = pd.read_csv('/content/drive/MyDrive/validation_data.csv')\n",
        "\n",
        "train_in = []\n",
        "test_in = []\n",
        "train_out = []\n",
        "test_out = []\n",
        "validation_in = []\n",
        "validation_out = []\n",
        "\n",
        "for row in train.itertuples():\n",
        "    # train_in.append(row.Word)\n",
        "    train_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':  # an empty word was getting in for some reason\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        train_in.append(temp)\n",
        "\n",
        "for row in test.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    test_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        test_in.append(temp)\n",
        "\n",
        "for row in validation.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    validation_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        validation_in.append(temp)\n",
        "\n",
        "test_in = np.array(test_in)\n",
        "test_out = np.array(test_out)\n",
        "train_in = np.array(train_in)\n",
        "train_out = np.array(train_out)\n",
        "validation_in = np.array(validation_in)\n",
        "validation_out = np.array(validation_out)\n",
        "\n",
        "max_word = 143\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(max_word, 100))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(100, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(50, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(8, activation=tf.nn.softmax))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.2, use_locking=False),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_in, train_out, epochs=1000, batch_size=1000, validation_data=(test_in, test_out), verbose=2, callbacks=[callback])\n",
        "\n",
        "results = model.evaluate(validation_in, validation_out)\n",
        "print('Accuracy is', results[1])\n",
        "print(len(history.history['loss']))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, None, 100)         14300     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_7 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 8)                 408       \n",
            "=================================================================\n",
            "Total params: 29,858\n",
            "Trainable params: 29,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "26/26 - 1s - loss: 1.6242 - accuracy: 0.3577 - val_loss: 1.4075 - val_accuracy: 0.3748\n",
            "Epoch 2/1000\n",
            "26/26 - 0s - loss: 1.3740 - accuracy: 0.3851 - val_loss: 1.3623 - val_accuracy: 0.3749\n",
            "Epoch 3/1000\n",
            "26/26 - 0s - loss: 1.3425 - accuracy: 0.3872 - val_loss: 1.3336 - val_accuracy: 0.3860\n",
            "Epoch 4/1000\n",
            "26/26 - 0s - loss: 1.3041 - accuracy: 0.4493 - val_loss: 1.2854 - val_accuracy: 0.3986\n",
            "Epoch 5/1000\n",
            "26/26 - 0s - loss: 1.2286 - accuracy: 0.5096 - val_loss: 1.1840 - val_accuracy: 0.5030\n",
            "Epoch 6/1000\n",
            "26/26 - 0s - loss: 1.1169 - accuracy: 0.5298 - val_loss: 1.1067 - val_accuracy: 0.4973\n",
            "Epoch 7/1000\n",
            "26/26 - 0s - loss: 1.1049 - accuracy: 0.4882 - val_loss: 1.0299 - val_accuracy: 0.5537\n",
            "Epoch 8/1000\n",
            "26/26 - 0s - loss: 1.0433 - accuracy: 0.5267 - val_loss: 1.0228 - val_accuracy: 0.5602\n",
            "Epoch 9/1000\n",
            "26/26 - 0s - loss: 1.0020 - accuracy: 0.5576 - val_loss: 0.9753 - val_accuracy: 0.5737\n",
            "Epoch 10/1000\n",
            "26/26 - 0s - loss: 0.9480 - accuracy: 0.5817 - val_loss: 1.0548 - val_accuracy: 0.5407\n",
            "Epoch 11/1000\n",
            "26/26 - 0s - loss: 0.9693 - accuracy: 0.5645 - val_loss: 0.9405 - val_accuracy: 0.5753\n",
            "Epoch 12/1000\n",
            "26/26 - 0s - loss: 0.9219 - accuracy: 0.5855 - val_loss: 0.9174 - val_accuracy: 0.5863\n",
            "Epoch 13/1000\n",
            "26/26 - 0s - loss: 0.9280 - accuracy: 0.5787 - val_loss: 0.8626 - val_accuracy: 0.6308\n",
            "Epoch 14/1000\n",
            "26/26 - 0s - loss: 0.8679 - accuracy: 0.6125 - val_loss: 1.0615 - val_accuracy: 0.4929\n",
            "Epoch 15/1000\n",
            "26/26 - 0s - loss: 0.9143 - accuracy: 0.5913 - val_loss: 0.8737 - val_accuracy: 0.6099\n",
            "Epoch 16/1000\n",
            "26/26 - 0s - loss: 0.8783 - accuracy: 0.6030 - val_loss: 0.8680 - val_accuracy: 0.6172\n",
            "Epoch 17/1000\n",
            "26/26 - 0s - loss: 0.8516 - accuracy: 0.6210 - val_loss: 0.8532 - val_accuracy: 0.6153\n",
            "Epoch 18/1000\n",
            "26/26 - 0s - loss: 0.8400 - accuracy: 0.6234 - val_loss: 0.8772 - val_accuracy: 0.6022\n",
            "Epoch 19/1000\n",
            "26/26 - 0s - loss: 0.8710 - accuracy: 0.6029 - val_loss: 0.8350 - val_accuracy: 0.6269\n",
            "Epoch 20/1000\n",
            "26/26 - 0s - loss: 0.8339 - accuracy: 0.6262 - val_loss: 0.8389 - val_accuracy: 0.6279\n",
            "Epoch 21/1000\n",
            "26/26 - 0s - loss: 0.8100 - accuracy: 0.6392 - val_loss: 0.8426 - val_accuracy: 0.6156\n",
            "Epoch 22/1000\n",
            "26/26 - 0s - loss: 0.8275 - accuracy: 0.6224 - val_loss: 0.7978 - val_accuracy: 0.6438\n",
            "Epoch 23/1000\n",
            "26/26 - 0s - loss: 0.8201 - accuracy: 0.6303 - val_loss: 0.8542 - val_accuracy: 0.6172\n",
            "Epoch 24/1000\n",
            "26/26 - 0s - loss: 0.8150 - accuracy: 0.6319 - val_loss: 0.8525 - val_accuracy: 0.6076\n",
            "Epoch 25/1000\n",
            "26/26 - 0s - loss: 0.7789 - accuracy: 0.6478 - val_loss: 0.8571 - val_accuracy: 0.6132\n",
            "Epoch 26/1000\n",
            "26/26 - 0s - loss: 0.7969 - accuracy: 0.6368 - val_loss: 0.7623 - val_accuracy: 0.6637\n",
            "Epoch 27/1000\n",
            "26/26 - 0s - loss: 0.7999 - accuracy: 0.6375 - val_loss: 0.7749 - val_accuracy: 0.6558\n",
            "Epoch 28/1000\n",
            "26/26 - 0s - loss: 0.7754 - accuracy: 0.6536 - val_loss: 0.7442 - val_accuracy: 0.6726\n",
            "Epoch 29/1000\n",
            "26/26 - 0s - loss: 0.7540 - accuracy: 0.6629 - val_loss: 0.8247 - val_accuracy: 0.6286\n",
            "Epoch 30/1000\n",
            "26/26 - 0s - loss: 0.7854 - accuracy: 0.6421 - val_loss: 0.7378 - val_accuracy: 0.6759\n",
            "Epoch 31/1000\n",
            "26/26 - 0s - loss: 0.7377 - accuracy: 0.6699 - val_loss: 0.7703 - val_accuracy: 0.6535\n",
            "Epoch 32/1000\n",
            "26/26 - 0s - loss: 0.7603 - accuracy: 0.6568 - val_loss: 0.7669 - val_accuracy: 0.6545\n",
            "Epoch 33/1000\n",
            "26/26 - 0s - loss: 0.7324 - accuracy: 0.6721 - val_loss: 0.8053 - val_accuracy: 0.6376\n",
            "Epoch 34/1000\n",
            "26/26 - 0s - loss: 0.7787 - accuracy: 0.6493 - val_loss: 0.7425 - val_accuracy: 0.6703\n",
            "Epoch 35/1000\n",
            "26/26 - 0s - loss: 0.7577 - accuracy: 0.6578 - val_loss: 0.7392 - val_accuracy: 0.6712\n",
            "Epoch 36/1000\n",
            "26/26 - 0s - loss: 0.7327 - accuracy: 0.6737 - val_loss: 0.6887 - val_accuracy: 0.6994\n",
            "Epoch 37/1000\n",
            "26/26 - 0s - loss: 0.7413 - accuracy: 0.6680 - val_loss: 0.6977 - val_accuracy: 0.6916\n",
            "Epoch 38/1000\n",
            "26/26 - 0s - loss: 0.7347 - accuracy: 0.6726 - val_loss: 0.7807 - val_accuracy: 0.6483\n",
            "Epoch 39/1000\n",
            "26/26 - 0s - loss: 0.7264 - accuracy: 0.6759 - val_loss: 0.7371 - val_accuracy: 0.6696\n",
            "Epoch 40/1000\n",
            "26/26 - 0s - loss: 0.7137 - accuracy: 0.6811 - val_loss: 0.7449 - val_accuracy: 0.6679\n",
            "Epoch 41/1000\n",
            "26/26 - 0s - loss: 0.7054 - accuracy: 0.6845 - val_loss: 0.7828 - val_accuracy: 0.6464\n",
            "Epoch 42/1000\n",
            "26/26 - 0s - loss: 0.7250 - accuracy: 0.6740 - val_loss: 0.7185 - val_accuracy: 0.6815\n",
            "Epoch 43/1000\n",
            "26/26 - 0s - loss: 0.7076 - accuracy: 0.6844 - val_loss: 0.6690 - val_accuracy: 0.7039\n",
            "Epoch 44/1000\n",
            "26/26 - 0s - loss: 0.6786 - accuracy: 0.6976 - val_loss: 0.7676 - val_accuracy: 0.6555\n",
            "Epoch 45/1000\n",
            "26/26 - 0s - loss: 0.7113 - accuracy: 0.6805 - val_loss: 0.6966 - val_accuracy: 0.6895\n",
            "Epoch 46/1000\n",
            "26/26 - 0s - loss: 0.7135 - accuracy: 0.6812 - val_loss: 0.6812 - val_accuracy: 0.6977\n",
            "Epoch 47/1000\n",
            "26/26 - 0s - loss: 0.6636 - accuracy: 0.7052 - val_loss: 0.6762 - val_accuracy: 0.6986\n",
            "Epoch 48/1000\n",
            "26/26 - 0s - loss: 0.7114 - accuracy: 0.6795 - val_loss: 0.6729 - val_accuracy: 0.6996\n",
            "Epoch 49/1000\n",
            "26/26 - 0s - loss: 0.6570 - accuracy: 0.7108 - val_loss: 0.6497 - val_accuracy: 0.7120\n",
            "Epoch 50/1000\n",
            "26/26 - 0s - loss: 0.7111 - accuracy: 0.6813 - val_loss: 0.6542 - val_accuracy: 0.7123\n",
            "Epoch 51/1000\n",
            "26/26 - 0s - loss: 0.6740 - accuracy: 0.7043 - val_loss: 0.7346 - val_accuracy: 0.6714\n",
            "Epoch 52/1000\n",
            "26/26 - 0s - loss: 0.6887 - accuracy: 0.6894 - val_loss: 0.6373 - val_accuracy: 0.7245\n",
            "Epoch 53/1000\n",
            "26/26 - 0s - loss: 0.6892 - accuracy: 0.6883 - val_loss: 0.6803 - val_accuracy: 0.7067\n",
            "Epoch 54/1000\n",
            "26/26 - 0s - loss: 0.6586 - accuracy: 0.7094 - val_loss: 0.8403 - val_accuracy: 0.6273\n",
            "Epoch 55/1000\n",
            "26/26 - 0s - loss: 0.6758 - accuracy: 0.6996 - val_loss: 0.6582 - val_accuracy: 0.7155\n",
            "Epoch 56/1000\n",
            "26/26 - 0s - loss: 0.6872 - accuracy: 0.6859 - val_loss: 0.6714 - val_accuracy: 0.7061\n",
            "Epoch 57/1000\n",
            "26/26 - 0s - loss: 0.6547 - accuracy: 0.7070 - val_loss: 0.7680 - val_accuracy: 0.6590\n",
            "Epoch 58/1000\n",
            "26/26 - 0s - loss: 0.6498 - accuracy: 0.7130 - val_loss: 0.6326 - val_accuracy: 0.7273\n",
            "Epoch 59/1000\n",
            "26/26 - 0s - loss: 0.7067 - accuracy: 0.6809 - val_loss: 0.6506 - val_accuracy: 0.7208\n",
            "Epoch 60/1000\n",
            "26/26 - 0s - loss: 0.6347 - accuracy: 0.7191 - val_loss: 0.7855 - val_accuracy: 0.6438\n",
            "Epoch 61/1000\n",
            "26/26 - 0s - loss: 0.6636 - accuracy: 0.7051 - val_loss: 0.6593 - val_accuracy: 0.7060\n",
            "Epoch 62/1000\n",
            "26/26 - 0s - loss: 0.6406 - accuracy: 0.7173 - val_loss: 0.6918 - val_accuracy: 0.6877\n",
            "Epoch 63/1000\n",
            "26/26 - 0s - loss: 0.6870 - accuracy: 0.6896 - val_loss: 0.6250 - val_accuracy: 0.7261\n",
            "Epoch 64/1000\n",
            "26/26 - 0s - loss: 0.6690 - accuracy: 0.7004 - val_loss: 0.6878 - val_accuracy: 0.6903\n",
            "Epoch 65/1000\n",
            "26/26 - 0s - loss: 0.6687 - accuracy: 0.6996 - val_loss: 0.6254 - val_accuracy: 0.7277\n",
            "Epoch 66/1000\n",
            "26/26 - 0s - loss: 0.6177 - accuracy: 0.7281 - val_loss: 0.6248 - val_accuracy: 0.7298\n",
            "Epoch 67/1000\n",
            "26/26 - 0s - loss: 0.6459 - accuracy: 0.7120 - val_loss: 0.6745 - val_accuracy: 0.7020\n",
            "Epoch 68/1000\n",
            "26/26 - 0s - loss: 0.6637 - accuracy: 0.7007 - val_loss: 0.6203 - val_accuracy: 0.7268\n",
            "Epoch 69/1000\n",
            "26/26 - 0s - loss: 0.6650 - accuracy: 0.7001 - val_loss: 0.6530 - val_accuracy: 0.7131\n",
            "Epoch 70/1000\n",
            "26/26 - 0s - loss: 0.6632 - accuracy: 0.6979 - val_loss: 0.6264 - val_accuracy: 0.7295\n",
            "Epoch 71/1000\n",
            "26/26 - 0s - loss: 0.6425 - accuracy: 0.7124 - val_loss: 0.7344 - val_accuracy: 0.6658\n",
            "Epoch 72/1000\n",
            "26/26 - 0s - loss: 0.6562 - accuracy: 0.7048 - val_loss: 0.6247 - val_accuracy: 0.7247\n",
            "Epoch 73/1000\n",
            "26/26 - 0s - loss: 0.6617 - accuracy: 0.6996 - val_loss: 0.6488 - val_accuracy: 0.7045\n",
            "Epoch 74/1000\n",
            "26/26 - 0s - loss: 0.6326 - accuracy: 0.7185 - val_loss: 0.6185 - val_accuracy: 0.7267\n",
            "Epoch 75/1000\n",
            "26/26 - 0s - loss: 0.6531 - accuracy: 0.7066 - val_loss: 0.6641 - val_accuracy: 0.6987\n",
            "Epoch 76/1000\n",
            "26/26 - 0s - loss: 0.6348 - accuracy: 0.7153 - val_loss: 0.6149 - val_accuracy: 0.7277\n",
            "Epoch 77/1000\n",
            "26/26 - 0s - loss: 0.6295 - accuracy: 0.7177 - val_loss: 0.6277 - val_accuracy: 0.7258\n",
            "Epoch 78/1000\n",
            "26/26 - 0s - loss: 0.6743 - accuracy: 0.6936 - val_loss: 0.6171 - val_accuracy: 0.7292\n",
            "Epoch 79/1000\n",
            "26/26 - 0s - loss: 0.6059 - accuracy: 0.7313 - val_loss: 0.6106 - val_accuracy: 0.7313\n",
            "Epoch 80/1000\n",
            "26/26 - 0s - loss: 0.6474 - accuracy: 0.7087 - val_loss: 0.6722 - val_accuracy: 0.6974\n",
            "Epoch 81/1000\n",
            "26/26 - 0s - loss: 0.6276 - accuracy: 0.7218 - val_loss: 0.6540 - val_accuracy: 0.7022\n",
            "Epoch 82/1000\n",
            "26/26 - 0s - loss: 0.6650 - accuracy: 0.6971 - val_loss: 0.6261 - val_accuracy: 0.7189\n",
            "Epoch 83/1000\n",
            "26/26 - 0s - loss: 0.6057 - accuracy: 0.7299 - val_loss: 0.6128 - val_accuracy: 0.7281\n",
            "Epoch 84/1000\n",
            "26/26 - 0s - loss: 0.6636 - accuracy: 0.6983 - val_loss: 0.6672 - val_accuracy: 0.6944\n",
            "Epoch 85/1000\n",
            "26/26 - 0s - loss: 0.6143 - accuracy: 0.7236 - val_loss: 0.6219 - val_accuracy: 0.7224\n",
            "Epoch 86/1000\n",
            "26/26 - 0s - loss: 0.6017 - accuracy: 0.7313 - val_loss: 0.6062 - val_accuracy: 0.7304\n",
            "Epoch 87/1000\n",
            "26/26 - 0s - loss: 0.6044 - accuracy: 0.7307 - val_loss: 0.6113 - val_accuracy: 0.7276\n",
            "Epoch 88/1000\n",
            "26/26 - 0s - loss: 0.6047 - accuracy: 0.7291 - val_loss: 0.6412 - val_accuracy: 0.7139\n",
            "Epoch 89/1000\n",
            "26/26 - 0s - loss: 0.6604 - accuracy: 0.6973 - val_loss: 0.6288 - val_accuracy: 0.7162\n",
            "Epoch 90/1000\n",
            "26/26 - 0s - loss: 0.6351 - accuracy: 0.7137 - val_loss: 0.6274 - val_accuracy: 0.7180\n",
            "Epoch 91/1000\n",
            "26/26 - 0s - loss: 0.6361 - accuracy: 0.7112 - val_loss: 0.6118 - val_accuracy: 0.7249\n",
            "Epoch 92/1000\n",
            "26/26 - 0s - loss: 0.6296 - accuracy: 0.7163 - val_loss: 0.6135 - val_accuracy: 0.7254\n",
            "Epoch 93/1000\n",
            "26/26 - 0s - loss: 0.6191 - accuracy: 0.7205 - val_loss: 0.6489 - val_accuracy: 0.7062\n",
            "Epoch 94/1000\n",
            "26/26 - 0s - loss: 0.6108 - accuracy: 0.7262 - val_loss: 0.6085 - val_accuracy: 0.7287\n",
            "Epoch 95/1000\n",
            "26/26 - 0s - loss: 0.6174 - accuracy: 0.7205 - val_loss: 0.6351 - val_accuracy: 0.7151\n",
            "Epoch 96/1000\n",
            "26/26 - 0s - loss: 0.6356 - accuracy: 0.7084 - val_loss: 0.6683 - val_accuracy: 0.6970\n",
            "Epoch 97/1000\n",
            "26/26 - 0s - loss: 0.6171 - accuracy: 0.7216 - val_loss: 0.6075 - val_accuracy: 0.7230\n",
            "Epoch 98/1000\n",
            "26/26 - 0s - loss: 0.6074 - accuracy: 0.7232 - val_loss: 0.6009 - val_accuracy: 0.7297\n",
            "Epoch 99/1000\n",
            "26/26 - 0s - loss: 0.6019 - accuracy: 0.7288 - val_loss: 0.6408 - val_accuracy: 0.7103\n",
            "Epoch 100/1000\n",
            "26/26 - 0s - loss: 0.6398 - accuracy: 0.7070 - val_loss: 0.6971 - val_accuracy: 0.6816\n",
            "Epoch 101/1000\n",
            "26/26 - 0s - loss: 0.6049 - accuracy: 0.7264 - val_loss: 0.6802 - val_accuracy: 0.6915\n",
            "Epoch 102/1000\n",
            "26/26 - 0s - loss: 0.6124 - accuracy: 0.7222 - val_loss: 0.6303 - val_accuracy: 0.7146\n",
            "Epoch 103/1000\n",
            "26/26 - 0s - loss: 0.6309 - accuracy: 0.7114 - val_loss: 0.6210 - val_accuracy: 0.7198\n",
            "Epoch 104/1000\n",
            "26/26 - 0s - loss: 0.5925 - accuracy: 0.7326 - val_loss: 0.5982 - val_accuracy: 0.7310\n",
            "Epoch 105/1000\n",
            "26/26 - 0s - loss: 0.6226 - accuracy: 0.7162 - val_loss: 0.6850 - val_accuracy: 0.6811\n",
            "Epoch 106/1000\n",
            "26/26 - 0s - loss: 0.6241 - accuracy: 0.7147 - val_loss: 0.6660 - val_accuracy: 0.6951\n",
            "Epoch 107/1000\n",
            "26/26 - 0s - loss: 0.5926 - accuracy: 0.7349 - val_loss: 0.5944 - val_accuracy: 0.7330\n",
            "Epoch 108/1000\n",
            "26/26 - 0s - loss: 0.5910 - accuracy: 0.7341 - val_loss: 0.6097 - val_accuracy: 0.7194\n",
            "Epoch 109/1000\n",
            "26/26 - 0s - loss: 0.6138 - accuracy: 0.7165 - val_loss: 0.6118 - val_accuracy: 0.7205\n",
            "Epoch 110/1000\n",
            "26/26 - 0s - loss: 0.6115 - accuracy: 0.7197 - val_loss: 0.6221 - val_accuracy: 0.7165\n",
            "Epoch 111/1000\n",
            "26/26 - 0s - loss: 0.6010 - accuracy: 0.7258 - val_loss: 0.6019 - val_accuracy: 0.7246\n",
            "Epoch 112/1000\n",
            "26/26 - 0s - loss: 0.6086 - accuracy: 0.7217 - val_loss: 0.6087 - val_accuracy: 0.7245\n",
            "Epoch 113/1000\n",
            "26/26 - 0s - loss: 0.5982 - accuracy: 0.7272 - val_loss: 0.6298 - val_accuracy: 0.7118\n",
            "Epoch 114/1000\n",
            "26/26 - 0s - loss: 0.6106 - accuracy: 0.7173 - val_loss: 0.6054 - val_accuracy: 0.7244\n",
            "Epoch 115/1000\n",
            "26/26 - 0s - loss: 0.6065 - accuracy: 0.7216 - val_loss: 0.6054 - val_accuracy: 0.7277\n",
            "Epoch 116/1000\n",
            "26/26 - 0s - loss: 0.5900 - accuracy: 0.7323 - val_loss: 0.6284 - val_accuracy: 0.7100\n",
            "Epoch 117/1000\n",
            "26/26 - 0s - loss: 0.5950 - accuracy: 0.7290 - val_loss: 0.5883 - val_accuracy: 0.7325\n",
            "Epoch 118/1000\n",
            "26/26 - 0s - loss: 0.5937 - accuracy: 0.7293 - val_loss: 0.6332 - val_accuracy: 0.7053\n",
            "Epoch 119/1000\n",
            "26/26 - 0s - loss: 0.5932 - accuracy: 0.7287 - val_loss: 0.5899 - val_accuracy: 0.7280\n",
            "Epoch 120/1000\n",
            "26/26 - 0s - loss: 0.6029 - accuracy: 0.7231 - val_loss: 0.6408 - val_accuracy: 0.7069\n",
            "Epoch 121/1000\n",
            "26/26 - 0s - loss: 0.5922 - accuracy: 0.7294 - val_loss: 0.6056 - val_accuracy: 0.7265\n",
            "Epoch 122/1000\n",
            "26/26 - 0s - loss: 0.6008 - accuracy: 0.7236 - val_loss: 0.5899 - val_accuracy: 0.7270\n",
            "Epoch 123/1000\n",
            "26/26 - 0s - loss: 0.5878 - accuracy: 0.7312 - val_loss: 0.5884 - val_accuracy: 0.7317\n",
            "Epoch 124/1000\n",
            "26/26 - 0s - loss: 0.5977 - accuracy: 0.7253 - val_loss: 0.5918 - val_accuracy: 0.7276\n",
            "Epoch 125/1000\n",
            "26/26 - 0s - loss: 0.5961 - accuracy: 0.7274 - val_loss: 0.5942 - val_accuracy: 0.7275\n",
            "Epoch 126/1000\n",
            "26/26 - 0s - loss: 0.5822 - accuracy: 0.7333 - val_loss: 0.5963 - val_accuracy: 0.7245\n",
            "Epoch 127/1000\n",
            "26/26 - 0s - loss: 0.5920 - accuracy: 0.7282 - val_loss: 0.6100 - val_accuracy: 0.7170\n",
            "Epoch 128/1000\n",
            "26/26 - 0s - loss: 0.5785 - accuracy: 0.7343 - val_loss: 0.5798 - val_accuracy: 0.7360\n",
            "Epoch 129/1000\n",
            "26/26 - 0s - loss: 0.5876 - accuracy: 0.7312 - val_loss: 0.6288 - val_accuracy: 0.7147\n",
            "Epoch 130/1000\n",
            "26/26 - 0s - loss: 0.5941 - accuracy: 0.7264 - val_loss: 0.5894 - val_accuracy: 0.7287\n",
            "Epoch 131/1000\n",
            "26/26 - 0s - loss: 0.5937 - accuracy: 0.7267 - val_loss: 0.5849 - val_accuracy: 0.7337\n",
            "Epoch 132/1000\n",
            "26/26 - 0s - loss: 0.5828 - accuracy: 0.7332 - val_loss: 0.5782 - val_accuracy: 0.7369\n",
            "Epoch 133/1000\n",
            "26/26 - 0s - loss: 0.5897 - accuracy: 0.7280 - val_loss: 0.6041 - val_accuracy: 0.7224\n",
            "Epoch 134/1000\n",
            "26/26 - 0s - loss: 0.5826 - accuracy: 0.7324 - val_loss: 0.6371 - val_accuracy: 0.7065\n",
            "Epoch 135/1000\n",
            "26/26 - 0s - loss: 0.5864 - accuracy: 0.7308 - val_loss: 0.5968 - val_accuracy: 0.7254\n",
            "Epoch 136/1000\n",
            "26/26 - 0s - loss: 0.5906 - accuracy: 0.7269 - val_loss: 0.5763 - val_accuracy: 0.7372\n",
            "Epoch 137/1000\n",
            "26/26 - 0s - loss: 0.5758 - accuracy: 0.7370 - val_loss: 0.5781 - val_accuracy: 0.7351\n",
            "Epoch 138/1000\n",
            "26/26 - 0s - loss: 0.5731 - accuracy: 0.7360 - val_loss: 0.5927 - val_accuracy: 0.7267\n",
            "Epoch 139/1000\n",
            "26/26 - 0s - loss: 0.5758 - accuracy: 0.7350 - val_loss: 0.6120 - val_accuracy: 0.7155\n",
            "Epoch 140/1000\n",
            "26/26 - 0s - loss: 0.5874 - accuracy: 0.7292 - val_loss: 0.5859 - val_accuracy: 0.7329\n",
            "Epoch 141/1000\n",
            "26/26 - 0s - loss: 0.5759 - accuracy: 0.7368 - val_loss: 0.5858 - val_accuracy: 0.7299\n",
            "Epoch 142/1000\n",
            "26/26 - 0s - loss: 0.5701 - accuracy: 0.7397 - val_loss: 0.5896 - val_accuracy: 0.7289\n",
            "Epoch 143/1000\n",
            "26/26 - 0s - loss: 0.5770 - accuracy: 0.7357 - val_loss: 0.5827 - val_accuracy: 0.7292\n",
            "Epoch 144/1000\n",
            "26/26 - 0s - loss: 0.5783 - accuracy: 0.7351 - val_loss: 0.5998 - val_accuracy: 0.7215\n",
            "Epoch 145/1000\n",
            "26/26 - 0s - loss: 0.5714 - accuracy: 0.7381 - val_loss: 0.5731 - val_accuracy: 0.7397\n",
            "Epoch 146/1000\n",
            "26/26 - 0s - loss: 0.5705 - accuracy: 0.7399 - val_loss: 0.5820 - val_accuracy: 0.7313\n",
            "Epoch 147/1000\n",
            "26/26 - 0s - loss: 0.5705 - accuracy: 0.7385 - val_loss: 0.5717 - val_accuracy: 0.7385\n",
            "Epoch 148/1000\n",
            "26/26 - 0s - loss: 0.5656 - accuracy: 0.7397 - val_loss: 0.5806 - val_accuracy: 0.7348\n",
            "Epoch 149/1000\n",
            "26/26 - 0s - loss: 0.5661 - accuracy: 0.7414 - val_loss: 0.5706 - val_accuracy: 0.7415\n",
            "Epoch 150/1000\n",
            "26/26 - 0s - loss: 0.5733 - accuracy: 0.7364 - val_loss: 0.5754 - val_accuracy: 0.7338\n",
            "Epoch 151/1000\n",
            "26/26 - 0s - loss: 0.5642 - accuracy: 0.7380 - val_loss: 0.5765 - val_accuracy: 0.7358\n",
            "Epoch 152/1000\n",
            "26/26 - 0s - loss: 0.5731 - accuracy: 0.7350 - val_loss: 0.5726 - val_accuracy: 0.7374\n",
            "Epoch 153/1000\n",
            "26/26 - 0s - loss: 0.5774 - accuracy: 0.7343 - val_loss: 0.5873 - val_accuracy: 0.7273\n",
            "Epoch 154/1000\n",
            "26/26 - 0s - loss: 0.5653 - accuracy: 0.7412 - val_loss: 0.5797 - val_accuracy: 0.7342\n",
            "Epoch 155/1000\n",
            "26/26 - 0s - loss: 0.5643 - accuracy: 0.7408 - val_loss: 0.6064 - val_accuracy: 0.7210\n",
            "Epoch 156/1000\n",
            "26/26 - 0s - loss: 0.5701 - accuracy: 0.7360 - val_loss: 0.5740 - val_accuracy: 0.7365\n",
            "Epoch 157/1000\n",
            "26/26 - 0s - loss: 0.5773 - accuracy: 0.7350 - val_loss: 0.5706 - val_accuracy: 0.7372\n",
            "Epoch 158/1000\n",
            "26/26 - 0s - loss: 0.5670 - accuracy: 0.7396 - val_loss: 0.5874 - val_accuracy: 0.7246\n",
            "Epoch 159/1000\n",
            "26/26 - 0s - loss: 0.5688 - accuracy: 0.7376 - val_loss: 0.5763 - val_accuracy: 0.7329\n",
            "Epoch 160/1000\n",
            "26/26 - 0s - loss: 0.5620 - accuracy: 0.7420 - val_loss: 0.5704 - val_accuracy: 0.7354\n",
            "Epoch 161/1000\n",
            "26/26 - 0s - loss: 0.5737 - accuracy: 0.7358 - val_loss: 0.5840 - val_accuracy: 0.7289\n",
            "Epoch 162/1000\n",
            "26/26 - 0s - loss: 0.5640 - accuracy: 0.7413 - val_loss: 0.6472 - val_accuracy: 0.7020\n",
            "Epoch 163/1000\n",
            "26/26 - 0s - loss: 0.5719 - accuracy: 0.7368 - val_loss: 0.5662 - val_accuracy: 0.7419\n",
            "Epoch 164/1000\n",
            "26/26 - 0s - loss: 0.5628 - accuracy: 0.7411 - val_loss: 0.5754 - val_accuracy: 0.7337\n",
            "Epoch 165/1000\n",
            "26/26 - 0s - loss: 0.5676 - accuracy: 0.7386 - val_loss: 0.5783 - val_accuracy: 0.7310\n",
            "Epoch 166/1000\n",
            "26/26 - 0s - loss: 0.5621 - accuracy: 0.7427 - val_loss: 0.6136 - val_accuracy: 0.7152\n",
            "Epoch 167/1000\n",
            "26/26 - 0s - loss: 0.5662 - accuracy: 0.7403 - val_loss: 0.5659 - val_accuracy: 0.7403\n",
            "Epoch 168/1000\n",
            "26/26 - 0s - loss: 0.5588 - accuracy: 0.7419 - val_loss: 0.6045 - val_accuracy: 0.7196\n",
            "Epoch 169/1000\n",
            "26/26 - 0s - loss: 0.5683 - accuracy: 0.7378 - val_loss: 0.5697 - val_accuracy: 0.7373\n",
            "Epoch 170/1000\n",
            "26/26 - 0s - loss: 0.5606 - accuracy: 0.7412 - val_loss: 0.5684 - val_accuracy: 0.7381\n",
            "Epoch 171/1000\n",
            "26/26 - 0s - loss: 0.5686 - accuracy: 0.7383 - val_loss: 0.5810 - val_accuracy: 0.7359\n",
            "Epoch 172/1000\n",
            "26/26 - 0s - loss: 0.5640 - accuracy: 0.7403 - val_loss: 0.5675 - val_accuracy: 0.7384\n",
            "Epoch 173/1000\n",
            "26/26 - 0s - loss: 0.5594 - accuracy: 0.7442 - val_loss: 0.5674 - val_accuracy: 0.7397\n",
            "Epoch 174/1000\n",
            "26/26 - 0s - loss: 0.5629 - accuracy: 0.7387 - val_loss: 0.5803 - val_accuracy: 0.7348\n",
            "Epoch 175/1000\n",
            "26/26 - 0s - loss: 0.5634 - accuracy: 0.7408 - val_loss: 0.5634 - val_accuracy: 0.7413\n",
            "Epoch 176/1000\n",
            "26/26 - 0s - loss: 0.5616 - accuracy: 0.7413 - val_loss: 0.5916 - val_accuracy: 0.7273\n",
            "Epoch 177/1000\n",
            "26/26 - 0s - loss: 0.5654 - accuracy: 0.7391 - val_loss: 0.5743 - val_accuracy: 0.7376\n",
            "Epoch 178/1000\n",
            "26/26 - 0s - loss: 0.5618 - accuracy: 0.7409 - val_loss: 0.5659 - val_accuracy: 0.7382\n",
            "Epoch 179/1000\n",
            "26/26 - 0s - loss: 0.5583 - accuracy: 0.7421 - val_loss: 0.5773 - val_accuracy: 0.7329\n",
            "Epoch 180/1000\n",
            "26/26 - 0s - loss: 0.5593 - accuracy: 0.7426 - val_loss: 0.5899 - val_accuracy: 0.7273\n",
            "Epoch 181/1000\n",
            "26/26 - 0s - loss: 0.5562 - accuracy: 0.7435 - val_loss: 0.5942 - val_accuracy: 0.7280\n",
            "Epoch 182/1000\n",
            "26/26 - 0s - loss: 0.5587 - accuracy: 0.7443 - val_loss: 0.5645 - val_accuracy: 0.7419\n",
            "Epoch 183/1000\n",
            "26/26 - 0s - loss: 0.5614 - accuracy: 0.7402 - val_loss: 0.5630 - val_accuracy: 0.7397\n",
            "Epoch 184/1000\n",
            "26/26 - 0s - loss: 0.5599 - accuracy: 0.7410 - val_loss: 0.5644 - val_accuracy: 0.7401\n",
            "Epoch 185/1000\n",
            "26/26 - 0s - loss: 0.5579 - accuracy: 0.7429 - val_loss: 0.5887 - val_accuracy: 0.7272\n",
            "Epoch 186/1000\n",
            "26/26 - 0s - loss: 0.5573 - accuracy: 0.7437 - val_loss: 0.5665 - val_accuracy: 0.7391\n",
            "Epoch 187/1000\n",
            "26/26 - 0s - loss: 0.5609 - accuracy: 0.7413 - val_loss: 0.5650 - val_accuracy: 0.7419\n",
            "Epoch 188/1000\n",
            "26/26 - 0s - loss: 0.5572 - accuracy: 0.7429 - val_loss: 0.5603 - val_accuracy: 0.7432\n",
            "Epoch 189/1000\n",
            "26/26 - 0s - loss: 0.5548 - accuracy: 0.7448 - val_loss: 0.5617 - val_accuracy: 0.7423\n",
            "Epoch 190/1000\n",
            "26/26 - 0s - loss: 0.5539 - accuracy: 0.7458 - val_loss: 0.5641 - val_accuracy: 0.7425\n",
            "Epoch 191/1000\n",
            "26/26 - 0s - loss: 0.5547 - accuracy: 0.7447 - val_loss: 0.5727 - val_accuracy: 0.7359\n",
            "Epoch 192/1000\n",
            "26/26 - 0s - loss: 0.5490 - accuracy: 0.7497 - val_loss: 0.5704 - val_accuracy: 0.7380\n",
            "Epoch 193/1000\n",
            "26/26 - 0s - loss: 0.5580 - accuracy: 0.7438 - val_loss: 0.5795 - val_accuracy: 0.7337\n",
            "Epoch 194/1000\n",
            "26/26 - 0s - loss: 0.5556 - accuracy: 0.7451 - val_loss: 0.5755 - val_accuracy: 0.7359\n",
            "Epoch 195/1000\n",
            "26/26 - 0s - loss: 0.5511 - accuracy: 0.7465 - val_loss: 0.5676 - val_accuracy: 0.7351\n",
            "Epoch 196/1000\n",
            "26/26 - 0s - loss: 0.5527 - accuracy: 0.7471 - val_loss: 0.5744 - val_accuracy: 0.7358\n",
            "Epoch 197/1000\n",
            "26/26 - 0s - loss: 0.5573 - accuracy: 0.7441 - val_loss: 0.5905 - val_accuracy: 0.7238\n",
            "Epoch 198/1000\n",
            "26/26 - 0s - loss: 0.5578 - accuracy: 0.7413 - val_loss: 0.5746 - val_accuracy: 0.7342\n",
            "Epoch 199/1000\n",
            "26/26 - 0s - loss: 0.5493 - accuracy: 0.7481 - val_loss: 0.5688 - val_accuracy: 0.7415\n",
            "Epoch 200/1000\n",
            "26/26 - 0s - loss: 0.5528 - accuracy: 0.7457 - val_loss: 0.5574 - val_accuracy: 0.7451\n",
            "Epoch 201/1000\n",
            "26/26 - 0s - loss: 0.5620 - accuracy: 0.7410 - val_loss: 0.5823 - val_accuracy: 0.7311\n",
            "Epoch 202/1000\n",
            "26/26 - 0s - loss: 0.5529 - accuracy: 0.7475 - val_loss: 0.5599 - val_accuracy: 0.7432\n",
            "Epoch 203/1000\n",
            "26/26 - 0s - loss: 0.5512 - accuracy: 0.7476 - val_loss: 0.5580 - val_accuracy: 0.7442\n",
            "Epoch 204/1000\n",
            "26/26 - 0s - loss: 0.5502 - accuracy: 0.7485 - val_loss: 0.5706 - val_accuracy: 0.7363\n",
            "Epoch 205/1000\n",
            "26/26 - 0s - loss: 0.5533 - accuracy: 0.7453 - val_loss: 0.5702 - val_accuracy: 0.7346\n",
            "Epoch 206/1000\n",
            "26/26 - 0s - loss: 0.5575 - accuracy: 0.7465 - val_loss: 0.5970 - val_accuracy: 0.7262\n",
            "Epoch 207/1000\n",
            "26/26 - 0s - loss: 0.5535 - accuracy: 0.7458 - val_loss: 0.5604 - val_accuracy: 0.7433\n",
            "Epoch 208/1000\n",
            "26/26 - 0s - loss: 0.5561 - accuracy: 0.7456 - val_loss: 0.5723 - val_accuracy: 0.7369\n",
            "Epoch 209/1000\n",
            "26/26 - 0s - loss: 0.5557 - accuracy: 0.7431 - val_loss: 0.5585 - val_accuracy: 0.7430\n",
            "Epoch 210/1000\n",
            "26/26 - 0s - loss: 0.5582 - accuracy: 0.7421 - val_loss: 0.5705 - val_accuracy: 0.7384\n",
            "Epoch 211/1000\n",
            "26/26 - 0s - loss: 0.5525 - accuracy: 0.7471 - val_loss: 0.5758 - val_accuracy: 0.7331\n",
            "Epoch 212/1000\n",
            "26/26 - 0s - loss: 0.5499 - accuracy: 0.7477 - val_loss: 0.5610 - val_accuracy: 0.7426\n",
            "Epoch 213/1000\n",
            "26/26 - 0s - loss: 0.5489 - accuracy: 0.7473 - val_loss: 0.5594 - val_accuracy: 0.7445\n",
            "Epoch 214/1000\n",
            "26/26 - 0s - loss: 0.5544 - accuracy: 0.7452 - val_loss: 0.5742 - val_accuracy: 0.7355\n",
            "Epoch 215/1000\n",
            "26/26 - 0s - loss: 0.5562 - accuracy: 0.7450 - val_loss: 0.5750 - val_accuracy: 0.7366\n",
            "Epoch 216/1000\n",
            "26/26 - 0s - loss: 0.5485 - accuracy: 0.7487 - val_loss: 0.5575 - val_accuracy: 0.7419\n",
            "Epoch 217/1000\n",
            "26/26 - 0s - loss: 0.5522 - accuracy: 0.7461 - val_loss: 0.5678 - val_accuracy: 0.7388\n",
            "Epoch 218/1000\n",
            "26/26 - 0s - loss: 0.5515 - accuracy: 0.7455 - val_loss: 0.5557 - val_accuracy: 0.7447\n",
            "Epoch 219/1000\n",
            "26/26 - 0s - loss: 0.5511 - accuracy: 0.7473 - val_loss: 0.5628 - val_accuracy: 0.7410\n",
            "Epoch 220/1000\n",
            "26/26 - 0s - loss: 0.5512 - accuracy: 0.7464 - val_loss: 0.5605 - val_accuracy: 0.7420\n",
            "Epoch 221/1000\n",
            "26/26 - 0s - loss: 0.5476 - accuracy: 0.7494 - val_loss: 0.5612 - val_accuracy: 0.7416\n",
            "Epoch 222/1000\n",
            "26/26 - 0s - loss: 0.5496 - accuracy: 0.7477 - val_loss: 0.5604 - val_accuracy: 0.7416\n",
            "Epoch 223/1000\n",
            "26/26 - 0s - loss: 0.5516 - accuracy: 0.7449 - val_loss: 0.5544 - val_accuracy: 0.7468\n",
            "Epoch 224/1000\n",
            "26/26 - 0s - loss: 0.5549 - accuracy: 0.7460 - val_loss: 0.5737 - val_accuracy: 0.7342\n",
            "Epoch 225/1000\n",
            "26/26 - 0s - loss: 0.5529 - accuracy: 0.7454 - val_loss: 0.5646 - val_accuracy: 0.7404\n",
            "Epoch 226/1000\n",
            "26/26 - 0s - loss: 0.5506 - accuracy: 0.7480 - val_loss: 0.5620 - val_accuracy: 0.7410\n",
            "Epoch 227/1000\n",
            "26/26 - 0s - loss: 0.5486 - accuracy: 0.7466 - val_loss: 0.5552 - val_accuracy: 0.7448\n",
            "Epoch 228/1000\n",
            "26/26 - 0s - loss: 0.5441 - accuracy: 0.7492 - val_loss: 0.5581 - val_accuracy: 0.7424\n",
            "Epoch 229/1000\n",
            "26/26 - 0s - loss: 0.5472 - accuracy: 0.7465 - val_loss: 0.5632 - val_accuracy: 0.7379\n",
            "Epoch 230/1000\n",
            "26/26 - 0s - loss: 0.5441 - accuracy: 0.7519 - val_loss: 0.5581 - val_accuracy: 0.7434\n",
            "Epoch 231/1000\n",
            "26/26 - 0s - loss: 0.5602 - accuracy: 0.7429 - val_loss: 0.5563 - val_accuracy: 0.7469\n",
            "Epoch 232/1000\n",
            "26/26 - 0s - loss: 0.5462 - accuracy: 0.7498 - val_loss: 0.5675 - val_accuracy: 0.7404\n",
            "Epoch 233/1000\n",
            "26/26 - 0s - loss: 0.5466 - accuracy: 0.7470 - val_loss: 0.5554 - val_accuracy: 0.7447\n",
            "Epoch 234/1000\n",
            "26/26 - 0s - loss: 0.5500 - accuracy: 0.7475 - val_loss: 0.5566 - val_accuracy: 0.7442\n",
            "Epoch 235/1000\n",
            "26/26 - 0s - loss: 0.5531 - accuracy: 0.7460 - val_loss: 0.5555 - val_accuracy: 0.7427\n",
            "Epoch 236/1000\n",
            "26/26 - 0s - loss: 0.5505 - accuracy: 0.7461 - val_loss: 0.5585 - val_accuracy: 0.7405\n",
            "Epoch 237/1000\n",
            "26/26 - 0s - loss: 0.5479 - accuracy: 0.7473 - val_loss: 0.5532 - val_accuracy: 0.7456\n",
            "Epoch 238/1000\n",
            "26/26 - 0s - loss: 0.5443 - accuracy: 0.7501 - val_loss: 0.5572 - val_accuracy: 0.7427\n",
            "Epoch 239/1000\n",
            "26/26 - 0s - loss: 0.5488 - accuracy: 0.7460 - val_loss: 0.5566 - val_accuracy: 0.7441\n",
            "Epoch 240/1000\n",
            "26/26 - 0s - loss: 0.5455 - accuracy: 0.7504 - val_loss: 0.5561 - val_accuracy: 0.7445\n",
            "Epoch 241/1000\n",
            "26/26 - 0s - loss: 0.5493 - accuracy: 0.7477 - val_loss: 0.5574 - val_accuracy: 0.7415\n",
            "Epoch 242/1000\n",
            "26/26 - 0s - loss: 0.5499 - accuracy: 0.7449 - val_loss: 0.5700 - val_accuracy: 0.7359\n",
            "Epoch 243/1000\n",
            "26/26 - 0s - loss: 0.5445 - accuracy: 0.7511 - val_loss: 0.5588 - val_accuracy: 0.7451\n",
            "Epoch 244/1000\n",
            "26/26 - 0s - loss: 0.5455 - accuracy: 0.7489 - val_loss: 0.5575 - val_accuracy: 0.7434\n",
            "Epoch 245/1000\n",
            "26/26 - 0s - loss: 0.5470 - accuracy: 0.7497 - val_loss: 0.5577 - val_accuracy: 0.7442\n",
            "Epoch 246/1000\n",
            "26/26 - 0s - loss: 0.5487 - accuracy: 0.7473 - val_loss: 0.5601 - val_accuracy: 0.7396\n",
            "Epoch 247/1000\n",
            "26/26 - 0s - loss: 0.5425 - accuracy: 0.7502 - val_loss: 0.5538 - val_accuracy: 0.7459\n",
            "Epoch 248/1000\n",
            "26/26 - 0s - loss: 0.5489 - accuracy: 0.7505 - val_loss: 0.5630 - val_accuracy: 0.7431\n",
            "Epoch 249/1000\n",
            "26/26 - 0s - loss: 0.5472 - accuracy: 0.7481 - val_loss: 0.5619 - val_accuracy: 0.7383\n",
            "Epoch 250/1000\n",
            "26/26 - 0s - loss: 0.5433 - accuracy: 0.7489 - val_loss: 0.5576 - val_accuracy: 0.7430\n",
            "Epoch 251/1000\n",
            "26/26 - 0s - loss: 0.5471 - accuracy: 0.7512 - val_loss: 0.5600 - val_accuracy: 0.7423\n",
            "Epoch 252/1000\n",
            "26/26 - 0s - loss: 0.5506 - accuracy: 0.7464 - val_loss: 0.5642 - val_accuracy: 0.7399\n",
            "Epoch 253/1000\n",
            "26/26 - 0s - loss: 0.5484 - accuracy: 0.7470 - val_loss: 0.5530 - val_accuracy: 0.7448\n",
            "Epoch 254/1000\n",
            "26/26 - 0s - loss: 0.5443 - accuracy: 0.7509 - val_loss: 0.5562 - val_accuracy: 0.7410\n",
            "Epoch 255/1000\n",
            "26/26 - 0s - loss: 0.5458 - accuracy: 0.7493 - val_loss: 0.5575 - val_accuracy: 0.7422\n",
            "Epoch 256/1000\n",
            "26/26 - 0s - loss: 0.5457 - accuracy: 0.7486 - val_loss: 0.5532 - val_accuracy: 0.7470\n",
            "Epoch 257/1000\n",
            "26/26 - 0s - loss: 0.5469 - accuracy: 0.7505 - val_loss: 0.5525 - val_accuracy: 0.7433\n",
            "Epoch 258/1000\n",
            "26/26 - 0s - loss: 0.5476 - accuracy: 0.7487 - val_loss: 0.5598 - val_accuracy: 0.7458\n",
            "Epoch 259/1000\n",
            "26/26 - 0s - loss: 0.5490 - accuracy: 0.7467 - val_loss: 0.5543 - val_accuracy: 0.7454\n",
            "Epoch 260/1000\n",
            "26/26 - 0s - loss: 0.5476 - accuracy: 0.7475 - val_loss: 0.5538 - val_accuracy: 0.7419\n",
            "Epoch 261/1000\n",
            "26/26 - 0s - loss: 0.5477 - accuracy: 0.7476 - val_loss: 0.5521 - val_accuracy: 0.7432\n",
            "Epoch 262/1000\n",
            "26/26 - 0s - loss: 0.5415 - accuracy: 0.7510 - val_loss: 0.5577 - val_accuracy: 0.7420\n",
            "Epoch 263/1000\n",
            "26/26 - 0s - loss: 0.5409 - accuracy: 0.7526 - val_loss: 0.5610 - val_accuracy: 0.7416\n",
            "Epoch 264/1000\n",
            "26/26 - 0s - loss: 0.5479 - accuracy: 0.7488 - val_loss: 0.5519 - val_accuracy: 0.7468\n",
            "Epoch 265/1000\n",
            "26/26 - 0s - loss: 0.5470 - accuracy: 0.7498 - val_loss: 0.5549 - val_accuracy: 0.7437\n",
            "Epoch 266/1000\n",
            "26/26 - 0s - loss: 0.5429 - accuracy: 0.7484 - val_loss: 0.5603 - val_accuracy: 0.7430\n",
            "Epoch 267/1000\n",
            "26/26 - 0s - loss: 0.5481 - accuracy: 0.7473 - val_loss: 0.5616 - val_accuracy: 0.7451\n",
            "Epoch 268/1000\n",
            "26/26 - 0s - loss: 0.5474 - accuracy: 0.7478 - val_loss: 0.5538 - val_accuracy: 0.7453\n",
            "Epoch 269/1000\n",
            "26/26 - 0s - loss: 0.5438 - accuracy: 0.7504 - val_loss: 0.5530 - val_accuracy: 0.7452\n",
            "Epoch 270/1000\n",
            "26/26 - 0s - loss: 0.5423 - accuracy: 0.7509 - val_loss: 0.5759 - val_accuracy: 0.7341\n",
            "Epoch 271/1000\n",
            "26/26 - 0s - loss: 0.5424 - accuracy: 0.7523 - val_loss: 0.5521 - val_accuracy: 0.7466\n",
            "Epoch 272/1000\n",
            "26/26 - 0s - loss: 0.5444 - accuracy: 0.7500 - val_loss: 0.5578 - val_accuracy: 0.7459\n",
            "Epoch 273/1000\n",
            "26/26 - 0s - loss: 0.5457 - accuracy: 0.7494 - val_loss: 0.5493 - val_accuracy: 0.7472\n",
            "Epoch 274/1000\n",
            "26/26 - 0s - loss: 0.5469 - accuracy: 0.7493 - val_loss: 0.5562 - val_accuracy: 0.7425\n",
            "Epoch 275/1000\n",
            "26/26 - 0s - loss: 0.5416 - accuracy: 0.7519 - val_loss: 0.5536 - val_accuracy: 0.7454\n",
            "Epoch 276/1000\n",
            "26/26 - 0s - loss: 0.5415 - accuracy: 0.7513 - val_loss: 0.5639 - val_accuracy: 0.7420\n",
            "Epoch 277/1000\n",
            "26/26 - 0s - loss: 0.5472 - accuracy: 0.7486 - val_loss: 0.5593 - val_accuracy: 0.7416\n",
            "Epoch 278/1000\n",
            "26/26 - 0s - loss: 0.5426 - accuracy: 0.7493 - val_loss: 0.5614 - val_accuracy: 0.7384\n",
            "Epoch 279/1000\n",
            "26/26 - 0s - loss: 0.5407 - accuracy: 0.7509 - val_loss: 0.5613 - val_accuracy: 0.7412\n",
            "Epoch 280/1000\n",
            "26/26 - 0s - loss: 0.5472 - accuracy: 0.7500 - val_loss: 0.5487 - val_accuracy: 0.7491\n",
            "Epoch 281/1000\n",
            "26/26 - 0s - loss: 0.5475 - accuracy: 0.7482 - val_loss: 0.5492 - val_accuracy: 0.7458\n",
            "Epoch 282/1000\n",
            "26/26 - 0s - loss: 0.5422 - accuracy: 0.7515 - val_loss: 0.5720 - val_accuracy: 0.7419\n",
            "Epoch 283/1000\n",
            "26/26 - 0s - loss: 0.5414 - accuracy: 0.7501 - val_loss: 0.5534 - val_accuracy: 0.7444\n",
            "Epoch 284/1000\n",
            "26/26 - 0s - loss: 0.5392 - accuracy: 0.7520 - val_loss: 0.5600 - val_accuracy: 0.7398\n",
            "Epoch 285/1000\n",
            "26/26 - 0s - loss: 0.5443 - accuracy: 0.7488 - val_loss: 0.5484 - val_accuracy: 0.7502\n",
            "Epoch 286/1000\n",
            "26/26 - 0s - loss: 0.5373 - accuracy: 0.7552 - val_loss: 0.5491 - val_accuracy: 0.7481\n",
            "Epoch 287/1000\n",
            "26/26 - 0s - loss: 0.5416 - accuracy: 0.7521 - val_loss: 0.5542 - val_accuracy: 0.7459\n",
            "Epoch 288/1000\n",
            "26/26 - 0s - loss: 0.5410 - accuracy: 0.7521 - val_loss: 0.5488 - val_accuracy: 0.7487\n",
            "Epoch 289/1000\n",
            "26/26 - 0s - loss: 0.5396 - accuracy: 0.7528 - val_loss: 0.5637 - val_accuracy: 0.7419\n",
            "Epoch 290/1000\n",
            "26/26 - 0s - loss: 0.5459 - accuracy: 0.7490 - val_loss: 0.5782 - val_accuracy: 0.7358\n",
            "Epoch 291/1000\n",
            "26/26 - 0s - loss: 0.5394 - accuracy: 0.7517 - val_loss: 0.5500 - val_accuracy: 0.7480\n",
            "Epoch 292/1000\n",
            "26/26 - 0s - loss: 0.5405 - accuracy: 0.7504 - val_loss: 0.5501 - val_accuracy: 0.7483\n",
            "Epoch 293/1000\n",
            "26/26 - 0s - loss: 0.5425 - accuracy: 0.7491 - val_loss: 0.5803 - val_accuracy: 0.7332\n",
            "Epoch 294/1000\n",
            "26/26 - 0s - loss: 0.5474 - accuracy: 0.7476 - val_loss: 0.5481 - val_accuracy: 0.7473\n",
            "Epoch 295/1000\n",
            "26/26 - 0s - loss: 0.5370 - accuracy: 0.7522 - val_loss: 0.5494 - val_accuracy: 0.7458\n",
            "Epoch 296/1000\n",
            "26/26 - 0s - loss: 0.5349 - accuracy: 0.7567 - val_loss: 0.5472 - val_accuracy: 0.7495\n",
            "Epoch 297/1000\n",
            "26/26 - 0s - loss: 0.5399 - accuracy: 0.7528 - val_loss: 0.5534 - val_accuracy: 0.7446\n",
            "Epoch 298/1000\n",
            "26/26 - 0s - loss: 0.5424 - accuracy: 0.7492 - val_loss: 0.5493 - val_accuracy: 0.7483\n",
            "Epoch 299/1000\n",
            "26/26 - 0s - loss: 0.5374 - accuracy: 0.7550 - val_loss: 0.5637 - val_accuracy: 0.7419\n",
            "Epoch 300/1000\n",
            "26/26 - 0s - loss: 0.5449 - accuracy: 0.7493 - val_loss: 0.5472 - val_accuracy: 0.7502\n",
            "Epoch 301/1000\n",
            "26/26 - 0s - loss: 0.5385 - accuracy: 0.7547 - val_loss: 0.5479 - val_accuracy: 0.7494\n",
            "Epoch 302/1000\n",
            "26/26 - 0s - loss: 0.5404 - accuracy: 0.7520 - val_loss: 0.5588 - val_accuracy: 0.7454\n",
            "Epoch 303/1000\n",
            "26/26 - 0s - loss: 0.5427 - accuracy: 0.7499 - val_loss: 0.5487 - val_accuracy: 0.7487\n",
            "Epoch 304/1000\n",
            "26/26 - 0s - loss: 0.5371 - accuracy: 0.7521 - val_loss: 0.5544 - val_accuracy: 0.7460\n",
            "Epoch 305/1000\n",
            "26/26 - 0s - loss: 0.5419 - accuracy: 0.7522 - val_loss: 0.5622 - val_accuracy: 0.7358\n",
            "Epoch 306/1000\n",
            "26/26 - 0s - loss: 0.5412 - accuracy: 0.7518 - val_loss: 0.5595 - val_accuracy: 0.7392\n",
            "Epoch 307/1000\n",
            "26/26 - 0s - loss: 0.5386 - accuracy: 0.7524 - val_loss: 0.5569 - val_accuracy: 0.7467\n",
            "Epoch 308/1000\n",
            "26/26 - 0s - loss: 0.5393 - accuracy: 0.7552 - val_loss: 0.5504 - val_accuracy: 0.7477\n",
            "Epoch 309/1000\n",
            "26/26 - 0s - loss: 0.5405 - accuracy: 0.7509 - val_loss: 0.5537 - val_accuracy: 0.7459\n",
            "Epoch 310/1000\n",
            "26/26 - 0s - loss: 0.5384 - accuracy: 0.7541 - val_loss: 0.5469 - val_accuracy: 0.7498\n",
            "Epoch 311/1000\n",
            "26/26 - 0s - loss: 0.5367 - accuracy: 0.7548 - val_loss: 0.5531 - val_accuracy: 0.7448\n",
            "Epoch 312/1000\n",
            "26/26 - 0s - loss: 0.5399 - accuracy: 0.7537 - val_loss: 0.5561 - val_accuracy: 0.7439\n",
            "Epoch 313/1000\n",
            "26/26 - 0s - loss: 0.5369 - accuracy: 0.7544 - val_loss: 0.5474 - val_accuracy: 0.7475\n",
            "Epoch 314/1000\n",
            "26/26 - 0s - loss: 0.5354 - accuracy: 0.7559 - val_loss: 0.5542 - val_accuracy: 0.7433\n",
            "Epoch 315/1000\n",
            "26/26 - 0s - loss: 0.5362 - accuracy: 0.7553 - val_loss: 0.5587 - val_accuracy: 0.7449\n",
            "Epoch 316/1000\n",
            "26/26 - 0s - loss: 0.5385 - accuracy: 0.7513 - val_loss: 0.5478 - val_accuracy: 0.7482\n",
            "Epoch 317/1000\n",
            "26/26 - 0s - loss: 0.5351 - accuracy: 0.7566 - val_loss: 0.5490 - val_accuracy: 0.7460\n",
            "Epoch 318/1000\n",
            "26/26 - 0s - loss: 0.5371 - accuracy: 0.7532 - val_loss: 0.5524 - val_accuracy: 0.7451\n",
            "Epoch 319/1000\n",
            "26/26 - 0s - loss: 0.5409 - accuracy: 0.7523 - val_loss: 0.5484 - val_accuracy: 0.7523\n",
            "Epoch 320/1000\n",
            "26/26 - 0s - loss: 0.5356 - accuracy: 0.7531 - val_loss: 0.5545 - val_accuracy: 0.7418\n",
            "Epoch 321/1000\n",
            "26/26 - 0s - loss: 0.5370 - accuracy: 0.7546 - val_loss: 0.5698 - val_accuracy: 0.7395\n",
            "Epoch 322/1000\n",
            "26/26 - 0s - loss: 0.5405 - accuracy: 0.7522 - val_loss: 0.5537 - val_accuracy: 0.7441\n",
            "Epoch 323/1000\n",
            "26/26 - 0s - loss: 0.5407 - accuracy: 0.7519 - val_loss: 0.5590 - val_accuracy: 0.7455\n",
            "Epoch 324/1000\n",
            "26/26 - 0s - loss: 0.5384 - accuracy: 0.7536 - val_loss: 0.5453 - val_accuracy: 0.7498\n",
            "Epoch 325/1000\n",
            "26/26 - 0s - loss: 0.5382 - accuracy: 0.7539 - val_loss: 0.5437 - val_accuracy: 0.7518\n",
            "Epoch 326/1000\n",
            "26/26 - 0s - loss: 0.5372 - accuracy: 0.7532 - val_loss: 0.5487 - val_accuracy: 0.7474\n",
            "Epoch 327/1000\n",
            "26/26 - 0s - loss: 0.5380 - accuracy: 0.7542 - val_loss: 0.5462 - val_accuracy: 0.7518\n",
            "Epoch 328/1000\n",
            "26/26 - 0s - loss: 0.5367 - accuracy: 0.7536 - val_loss: 0.5527 - val_accuracy: 0.7447\n",
            "Epoch 329/1000\n",
            "26/26 - 0s - loss: 0.5342 - accuracy: 0.7546 - val_loss: 0.5432 - val_accuracy: 0.7495\n",
            "Epoch 330/1000\n",
            "26/26 - 0s - loss: 0.5379 - accuracy: 0.7535 - val_loss: 0.5468 - val_accuracy: 0.7518\n",
            "Epoch 331/1000\n",
            "26/26 - 0s - loss: 0.5405 - accuracy: 0.7522 - val_loss: 0.5496 - val_accuracy: 0.7469\n",
            "Epoch 332/1000\n",
            "26/26 - 0s - loss: 0.5431 - accuracy: 0.7496 - val_loss: 0.5441 - val_accuracy: 0.7503\n",
            "Epoch 333/1000\n",
            "26/26 - 0s - loss: 0.5333 - accuracy: 0.7570 - val_loss: 0.5450 - val_accuracy: 0.7501\n",
            "Epoch 334/1000\n",
            "26/26 - 0s - loss: 0.5356 - accuracy: 0.7554 - val_loss: 0.5435 - val_accuracy: 0.7510\n",
            "Epoch 335/1000\n",
            "26/26 - 0s - loss: 0.5355 - accuracy: 0.7556 - val_loss: 0.5436 - val_accuracy: 0.7517\n",
            "Epoch 336/1000\n",
            "26/26 - 0s - loss: 0.5346 - accuracy: 0.7563 - val_loss: 0.5438 - val_accuracy: 0.7518\n",
            "Epoch 337/1000\n",
            "26/26 - 0s - loss: 0.5349 - accuracy: 0.7543 - val_loss: 0.5447 - val_accuracy: 0.7516\n",
            "Epoch 338/1000\n",
            "26/26 - 0s - loss: 0.5349 - accuracy: 0.7551 - val_loss: 0.5472 - val_accuracy: 0.7490\n",
            "Epoch 339/1000\n",
            "26/26 - 0s - loss: 0.5404 - accuracy: 0.7521 - val_loss: 0.5509 - val_accuracy: 0.7462\n",
            "Epoch 340/1000\n",
            "26/26 - 0s - loss: 0.5387 - accuracy: 0.7520 - val_loss: 0.5625 - val_accuracy: 0.7432\n",
            "Epoch 341/1000\n",
            "26/26 - 0s - loss: 0.5389 - accuracy: 0.7532 - val_loss: 0.5455 - val_accuracy: 0.7501\n",
            "Epoch 342/1000\n",
            "26/26 - 0s - loss: 0.5357 - accuracy: 0.7531 - val_loss: 0.5690 - val_accuracy: 0.7359\n",
            "Epoch 343/1000\n",
            "26/26 - 0s - loss: 0.5385 - accuracy: 0.7522 - val_loss: 0.5830 - val_accuracy: 0.7368\n",
            "Epoch 344/1000\n",
            "26/26 - 0s - loss: 0.5368 - accuracy: 0.7552 - val_loss: 0.5488 - val_accuracy: 0.7474\n",
            "Epoch 345/1000\n",
            "26/26 - 0s - loss: 0.5351 - accuracy: 0.7562 - val_loss: 0.5453 - val_accuracy: 0.7502\n",
            "Epoch 346/1000\n",
            "26/26 - 0s - loss: 0.5348 - accuracy: 0.7548 - val_loss: 0.5457 - val_accuracy: 0.7492\n",
            "Epoch 347/1000\n",
            "26/26 - 0s - loss: 0.5340 - accuracy: 0.7559 - val_loss: 0.5479 - val_accuracy: 0.7485\n",
            "Epoch 348/1000\n",
            "26/26 - 0s - loss: 0.5335 - accuracy: 0.7556 - val_loss: 0.5713 - val_accuracy: 0.7373\n",
            "Epoch 349/1000\n",
            "26/26 - 0s - loss: 0.5356 - accuracy: 0.7549 - val_loss: 0.5729 - val_accuracy: 0.7304\n",
            "269/269 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.7260\n",
            "Accuracy is 0.7259621024131775\n",
            "349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-llET58GAB"
      },
      "source": [
        "def rap():\n",
        "    num_lines = int(input('How many lines would you like the rap to be? '))\n",
        "    num_generated_lines = int(input('How many lines should be generated to choose from? '))\n",
        "    count = int(input(\"How many syllables per line? \"))\n",
        "\n",
        "    # Extract all of Mike's lyrics.\n",
        "    text = open(\"/content/drive/MyDrive/AllLyrics_uncleanOLD.txt\", \"r\").read()\n",
        "    vocabulary = ''.join([i for i in text if not i.isdigit()]).replace(\"\\n\", \" \").split(' ')\n",
        "\n",
        "    # Generate text\n",
        "    def line_generator(vocab):\n",
        "        index = 1\n",
        "        chain = {}\n",
        "        # count = 16 # https://colemizestudios.com/rap-lyrics-syllables/, apparently rappers usually use semiquavers\n",
        "        line_count = 0\n",
        "        number_of_tries = 0\n",
        "\n",
        "        for word in vocab[index:]:\n",
        "            key = vocab[index - 1]\n",
        "            if key in chain:\n",
        "                chain[key].append(word)\n",
        "            else:\n",
        "                chain[key] = [word]\n",
        "            index += 1\n",
        "\n",
        "        word1 = random.choice(list(chain.keys()))\n",
        "        line = word1.capitalize()\n",
        "        word1_with_spaces = word1\n",
        "        while len(word1_with_spaces) < 16:\n",
        "            word1_with_spaces += ' '\n",
        "        temp_word = list(word1_with_spaces)\n",
        "        for i in range(len(temp_word)):\n",
        "            temp_word[i] = ord(temp_word[i])\n",
        "        word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "        word_count = word_syllables\n",
        "        line_count += word_count\n",
        "\n",
        "        while line_count < count:\n",
        "            number_of_tries += 1\n",
        "            word2 = random.choice(chain[word1])\n",
        "            word2_with_spaces = word2\n",
        "            while len(word2_with_spaces) < 16:\n",
        "                word2_with_spaces += ' '\n",
        "            temp_word = list(word2_with_spaces)\n",
        "            for i in range(len(temp_word)):\n",
        "                temp_word[i] = ord(temp_word[i])\n",
        "            word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "            word_count = word_syllables\n",
        "            line_count += word_count\n",
        "            # print(n)\n",
        "            if line_count > count:  # don't include word if it makes line go over syllable count\n",
        "                line_count -= word_count\n",
        "            else:\n",
        "                word1 = word2\n",
        "                line += ' ' + word2.lower()\n",
        "            if number_of_tries > 99:  # if not finding a word with right number of syllables, stop trying\n",
        "                line += ' ERROR FINDING CORRECT SYLLABLE WORD'\n",
        "                line_count = count\n",
        "        return line\n",
        "\n",
        "    # Rhyme Functions\n",
        "    def reverse_syllable_extract(text):\n",
        "        sy_form = []\n",
        "        characters = [char for char in text]\n",
        "        sylls = ['a', 'e', 'i', 'o', 'u', 'y']\n",
        "        for x in characters:\n",
        "            if x in sylls:\n",
        "                sy_form.append(x)\n",
        "        sy_form.reverse()\n",
        "        return sy_form\n",
        "\n",
        "    def rev_syllable_stop_count(text1, text2):\n",
        "        counter = True\n",
        "        i = 0\n",
        "        counter = 0\n",
        "        syll1 = reverse_syllable_extract(text1)\n",
        "        syll2 = reverse_syllable_extract(text2)\n",
        "        while counter:\n",
        "            if i < min(len(syll1), len(syll2)) and syll1[i] == syll2[i]:\n",
        "                counter += 1\n",
        "                i += 1\n",
        "            else:\n",
        "                counter = False\n",
        "        return counter\n",
        "\n",
        "    def next_line_stop_count(start_line, lines):\n",
        "        sy_lines = []\n",
        "        for i in lines:\n",
        "            sy_lines.append(rev_syllable_stop_count(start_line, i))\n",
        "        choice = sy_lines[0]\n",
        "        count = 0\n",
        "        for i in range(len(sy_lines)):\n",
        "            if sy_lines[i] > choice:\n",
        "                choice = sy_lines[i]\n",
        "        return lines[sy_lines.index(choice)]\n",
        "\n",
        "    start_line = line_generator(vocabulary)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if 'ERROR FINDING CORRECT SYLLABLE WORD' in start_line:\n",
        "            start_line = line_generator(vocabulary)\n",
        "        else:\n",
        "            done = True\n",
        "\n",
        "    all_other_lines = []\n",
        "    for i in range(num_generated_lines - 1):\n",
        "      all_other_lines.append(line_generator(vocabulary))\n",
        "    rap = [start_line]\n",
        "\n",
        "    for n, line in enumerate(all_other_lines):\n",
        "        done = False\n",
        "        while not done:\n",
        "            if 'ERROR FINDING CORRECT SYLLABLE WORD' in line:\n",
        "                line = line_generator(vocabulary)\n",
        "                all_other_lines[n] = line\n",
        "            else:\n",
        "                done = True\n",
        "\n",
        "    for i in range(num_lines):\n",
        "        if i % 2 == 1:\n",
        "            next_line = next_line_stop_count(rap[len(rap) - 1], all_other_lines)\n",
        "        else:\n",
        "            next_line = random.choice(all_other_lines)\n",
        "        all_other_lines.remove(next_line)\n",
        "        rap.append(next_line)\n",
        "    censors = pd.read_csv('censors2.csv')\n",
        "    for i, line in enumerate(rap):\n",
        "      for j, word in enumerate(line):\n",
        "        for row in censors.itertuples():\n",
        "          if word == row.word:\n",
        "            line[j] = row.replacement\n",
        "      rap[i] = line\n",
        "    \n",
        "    return rap"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0WbObRPyyIy",
        "outputId": "7b560438-0131-4d3e-9689-01453361d85e"
      },
      "source": [
        "rap()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How many lines would you like the rap to be? 10\n",
            "How many lines should be generated to choose from? 500\n",
            "How many syllables per line? 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Heineken  dress up the blunts was the high now im paid uh cmon now',\n",
              " 'Bon appetit  asked my wall and recognize the head right one',\n",
              " 'Welcome to choose the hun chorusrepeat  went to die ',\n",
              " 'Highness  had to speak to packin gats and taking her nose even',\n",
              " 'In the ladies tonight that i get they money baby damn a',\n",
              " 'Shootin  then im  im fuckin an  people at the third',\n",
              " 'Changed but attack  and dangerous we buckin at my rings',\n",
              " 'Burgular alarm systems  flippin on the sl the rim with',\n",
              " 'From gym class to the goldie sound  hey ill die  to breathe',\n",
              " 'Brat  i get suspicious  run up like his name to old',\n",
              " 'Dot on  aint an itchy for my man n tip  see me ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9SRl8X-hIQ"
      },
      "source": [
        "# Takes longer to load due to iterating through words until finding word with the right number of syllables\n",
        "# Also takes longer due to replacing lines which contain errors\n",
        "# Takes much much longer due to use of the neural network to predict syllables"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}