{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SyllNNModel1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsnsZh72-hH0"
      },
      "source": [
        "# Markov with Syllable Neural Network\n",
        "# Same rhyme ranker as MikesVersion1\n",
        "# Changelog:\n",
        "# 'All files now up to date' now only displays once\n",
        "# - Uses neural network to count syllables"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQph78-g-hIE",
        "outputId": "953eec80-a213-4861-8adb-4e6ddf206078"
      },
      "source": [
        "#@title Import Statements\n",
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pprint import pprint # For pretty printing\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n",
        "\n",
        "# For the Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# For importing the training, testing and validation data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.7/dist-packages (1.54.1)\n",
            "Requirement already satisfied: pyjwt<2.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.7.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.2.12)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N624BVtBsmy"
      },
      "source": [
        "#@title Function Definitions\n",
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "    print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Committing files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5On3o1fS-hIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cc387b-f7d7-4ceb-85ed-b47b3fc030a7"
      },
      "source": [
        "# Import all of Mike's lyrics.\n",
        "import_github()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key ··········\n",
            "Main Branch: Yes or No? y\n",
            "Importing Github cleaned text files...\n",
            "AllLyrics.txt is already up to date!\n",
            "Writing file 0 capitals.csv\n",
            "Writing file 1 censors.csv\n",
            "Writing file 2 censors2.csv\n",
            "All files now up to date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WahREYjCV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10645fce-e038-4950-c695-f29b098c1852"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/training_data.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/testing_data.csv')\n",
        "validation = pd.read_csv('/content/drive/MyDrive/validation_data.csv')\n",
        "\n",
        "train_in = []\n",
        "test_in = []\n",
        "train_out = []\n",
        "test_out = []\n",
        "validation_in = []\n",
        "validation_out = []\n",
        "\n",
        "for row in train.itertuples():\n",
        "    # train_in.append(row.Word)\n",
        "    train_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':  # an empty word was getting in for some reason\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        train_in.append(temp)\n",
        "\n",
        "for row in test.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    test_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        test_in.append(temp)\n",
        "\n",
        "for row in validation.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    validation_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        validation_in.append(temp)\n",
        "\n",
        "test_in = np.array(test_in)\n",
        "test_out = np.array(test_out)\n",
        "train_in = np.array(train_in)\n",
        "train_out = np.array(train_out)\n",
        "validation_in = np.array(validation_in)\n",
        "validation_out = np.array(validation_out)\n",
        "\n",
        "max_word = 143\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(max_word, 100))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(100, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(50, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(8, activation=tf.nn.softmax))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.2, use_locking=False),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_in, train_out, epochs=100, batch_size=100, validation_data=(test_in, test_out), verbose=2)\n",
        "\n",
        "results = model.evaluate(validation_in, validation_out)\n",
        "print('Accuracy is', results[1])"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 100)         14300     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 8)                 408       \n",
            "=================================================================\n",
            "Total params: 29,858\n",
            "Trainable params: 29,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "259/259 - 1s - loss: 1.2112 - accuracy: 0.4828 - val_loss: 1.8470 - val_accuracy: 0.1617\n",
            "Epoch 2/100\n",
            "259/259 - 1s - loss: 0.9058 - accuracy: 0.5988 - val_loss: 2.2588 - val_accuracy: 0.3955\n",
            "Epoch 3/100\n",
            "259/259 - 1s - loss: 0.8299 - accuracy: 0.6350 - val_loss: 0.7690 - val_accuracy: 0.6480\n",
            "Epoch 4/100\n",
            "259/259 - 1s - loss: 0.7624 - accuracy: 0.6618 - val_loss: 1.5958 - val_accuracy: 0.3048\n",
            "Epoch 5/100\n",
            "259/259 - 1s - loss: 0.7391 - accuracy: 0.6723 - val_loss: 2.0604 - val_accuracy: 0.3381\n",
            "Epoch 6/100\n",
            "259/259 - 1s - loss: 0.6982 - accuracy: 0.6915 - val_loss: 0.8335 - val_accuracy: 0.6412\n",
            "Epoch 7/100\n",
            "259/259 - 1s - loss: 0.6867 - accuracy: 0.6940 - val_loss: 0.8893 - val_accuracy: 0.6333\n",
            "Epoch 8/100\n",
            "259/259 - 1s - loss: 0.6624 - accuracy: 0.7073 - val_loss: 1.3610 - val_accuracy: 0.4471\n",
            "Epoch 9/100\n",
            "259/259 - 1s - loss: 0.6588 - accuracy: 0.7039 - val_loss: 0.6763 - val_accuracy: 0.6990\n",
            "Epoch 10/100\n",
            "259/259 - 1s - loss: 0.6457 - accuracy: 0.7094 - val_loss: 2.2299 - val_accuracy: 0.1724\n",
            "Epoch 11/100\n",
            "259/259 - 1s - loss: 0.6360 - accuracy: 0.7115 - val_loss: 0.6039 - val_accuracy: 0.7299\n",
            "Epoch 12/100\n",
            "259/259 - 1s - loss: 0.6234 - accuracy: 0.7136 - val_loss: 1.6211 - val_accuracy: 0.3059\n",
            "Epoch 13/100\n",
            "259/259 - 1s - loss: 0.6205 - accuracy: 0.7146 - val_loss: 1.1433 - val_accuracy: 0.5535\n",
            "Epoch 14/100\n",
            "259/259 - 1s - loss: 0.6107 - accuracy: 0.7207 - val_loss: 0.6975 - val_accuracy: 0.6486\n",
            "Epoch 15/100\n",
            "259/259 - 1s - loss: 0.5999 - accuracy: 0.7253 - val_loss: 0.5962 - val_accuracy: 0.7253\n",
            "Epoch 16/100\n",
            "259/259 - 1s - loss: 0.5968 - accuracy: 0.7238 - val_loss: 1.4216 - val_accuracy: 0.4766\n",
            "Epoch 17/100\n",
            "259/259 - 1s - loss: 0.5995 - accuracy: 0.7242 - val_loss: 0.6532 - val_accuracy: 0.7045\n",
            "Epoch 18/100\n",
            "259/259 - 1s - loss: 0.5851 - accuracy: 0.7304 - val_loss: 3.9288 - val_accuracy: 0.0231\n",
            "Epoch 19/100\n",
            "259/259 - 1s - loss: 0.6773 - accuracy: 0.7017 - val_loss: 2.1949 - val_accuracy: 0.3109\n",
            "Epoch 20/100\n",
            "259/259 - 1s - loss: 0.6225 - accuracy: 0.7169 - val_loss: 0.7959 - val_accuracy: 0.6443\n",
            "Epoch 21/100\n",
            "259/259 - 1s - loss: 0.5983 - accuracy: 0.7250 - val_loss: 1.0620 - val_accuracy: 0.5660\n",
            "Epoch 22/100\n",
            "259/259 - 1s - loss: 0.5895 - accuracy: 0.7281 - val_loss: 1.5985 - val_accuracy: 0.4468\n",
            "Epoch 23/100\n",
            "259/259 - 1s - loss: 0.5914 - accuracy: 0.7306 - val_loss: 0.6744 - val_accuracy: 0.6948\n",
            "Epoch 24/100\n",
            "259/259 - 1s - loss: 0.5763 - accuracy: 0.7328 - val_loss: 0.7669 - val_accuracy: 0.6830\n",
            "Epoch 25/100\n",
            "259/259 - 1s - loss: 0.5742 - accuracy: 0.7373 - val_loss: 0.7606 - val_accuracy: 0.7017\n",
            "Epoch 26/100\n",
            "259/259 - 1s - loss: 0.5726 - accuracy: 0.7353 - val_loss: 0.7471 - val_accuracy: 0.6238\n",
            "Epoch 27/100\n",
            "259/259 - 1s - loss: 0.5720 - accuracy: 0.7360 - val_loss: 0.7203 - val_accuracy: 0.6963\n",
            "Epoch 28/100\n",
            "259/259 - 1s - loss: 0.5698 - accuracy: 0.7372 - val_loss: 1.4464 - val_accuracy: 0.5127\n",
            "Epoch 29/100\n",
            "259/259 - 1s - loss: 0.5719 - accuracy: 0.7356 - val_loss: 3.7288 - val_accuracy: 0.2523\n",
            "Epoch 30/100\n",
            "259/259 - 1s - loss: 0.6081 - accuracy: 0.7285 - val_loss: 1.6052 - val_accuracy: 0.3933\n",
            "Epoch 31/100\n",
            "259/259 - 1s - loss: 0.5723 - accuracy: 0.7387 - val_loss: 0.7312 - val_accuracy: 0.6783\n",
            "Epoch 32/100\n",
            "259/259 - 1s - loss: 0.5638 - accuracy: 0.7396 - val_loss: 0.7472 - val_accuracy: 0.6440\n",
            "Epoch 33/100\n",
            "259/259 - 1s - loss: 0.5625 - accuracy: 0.7400 - val_loss: 0.6054 - val_accuracy: 0.7163\n",
            "Epoch 34/100\n",
            "259/259 - 1s - loss: 0.5596 - accuracy: 0.7431 - val_loss: 1.6389 - val_accuracy: 0.3755\n",
            "Epoch 35/100\n",
            "259/259 - 1s - loss: 0.5827 - accuracy: 0.7341 - val_loss: 1.0904 - val_accuracy: 0.6222\n",
            "Epoch 36/100\n",
            "259/259 - 1s - loss: 0.5625 - accuracy: 0.7446 - val_loss: 1.5251 - val_accuracy: 0.4247\n",
            "Epoch 37/100\n",
            "259/259 - 1s - loss: 0.5625 - accuracy: 0.7422 - val_loss: 0.7597 - val_accuracy: 0.6400\n",
            "Epoch 38/100\n",
            "259/259 - 1s - loss: 0.5577 - accuracy: 0.7417 - val_loss: 0.5615 - val_accuracy: 0.7382\n",
            "Epoch 39/100\n",
            "259/259 - 1s - loss: 0.5573 - accuracy: 0.7446 - val_loss: 0.6143 - val_accuracy: 0.7165\n",
            "Epoch 40/100\n",
            "259/259 - 1s - loss: 0.5580 - accuracy: 0.7419 - val_loss: 0.5874 - val_accuracy: 0.7272\n",
            "Epoch 41/100\n",
            "259/259 - 1s - loss: 0.5556 - accuracy: 0.7434 - val_loss: 3.7811 - val_accuracy: 0.2082\n",
            "Epoch 42/100\n",
            "259/259 - 1s - loss: 0.5879 - accuracy: 0.7372 - val_loss: 0.6433 - val_accuracy: 0.7082\n",
            "Epoch 43/100\n",
            "259/259 - 1s - loss: 0.5552 - accuracy: 0.7470 - val_loss: 2.1709 - val_accuracy: 0.1359\n",
            "Epoch 44/100\n",
            "259/259 - 1s - loss: 0.5685 - accuracy: 0.7396 - val_loss: 0.6977 - val_accuracy: 0.6812\n",
            "Epoch 45/100\n",
            "259/259 - 1s - loss: 0.5537 - accuracy: 0.7462 - val_loss: 0.7092 - val_accuracy: 0.6654\n",
            "Epoch 46/100\n",
            "259/259 - 1s - loss: 0.5529 - accuracy: 0.7463 - val_loss: 0.6731 - val_accuracy: 0.7068\n",
            "Epoch 47/100\n",
            "259/259 - 1s - loss: 0.5529 - accuracy: 0.7463 - val_loss: 0.7784 - val_accuracy: 0.6304\n",
            "Epoch 48/100\n",
            "259/259 - 1s - loss: 0.5502 - accuracy: 0.7479 - val_loss: 1.1229 - val_accuracy: 0.5791\n",
            "Epoch 49/100\n",
            "259/259 - 1s - loss: 0.5529 - accuracy: 0.7462 - val_loss: 0.8682 - val_accuracy: 0.5960\n",
            "Epoch 50/100\n",
            "259/259 - 1s - loss: 0.5502 - accuracy: 0.7485 - val_loss: 0.7384 - val_accuracy: 0.6705\n",
            "Epoch 51/100\n",
            "259/259 - 1s - loss: 0.5476 - accuracy: 0.7475 - val_loss: 0.6046 - val_accuracy: 0.7236\n",
            "Epoch 52/100\n",
            "259/259 - 1s - loss: 0.5506 - accuracy: 0.7461 - val_loss: 0.5892 - val_accuracy: 0.7317\n",
            "Epoch 53/100\n",
            "259/259 - 1s - loss: 0.5466 - accuracy: 0.7487 - val_loss: 0.8596 - val_accuracy: 0.6645\n",
            "Epoch 54/100\n",
            "259/259 - 1s - loss: 0.5482 - accuracy: 0.7489 - val_loss: 0.7451 - val_accuracy: 0.6704\n",
            "Epoch 55/100\n",
            "259/259 - 1s - loss: 0.5468 - accuracy: 0.7500 - val_loss: 0.5519 - val_accuracy: 0.7409\n",
            "Epoch 56/100\n",
            "259/259 - 1s - loss: 0.5441 - accuracy: 0.7495 - val_loss: 1.2120 - val_accuracy: 0.5202\n",
            "Epoch 57/100\n",
            "259/259 - 1s - loss: 0.5485 - accuracy: 0.7488 - val_loss: 0.8159 - val_accuracy: 0.5915\n",
            "Epoch 58/100\n",
            "259/259 - 1s - loss: 0.5438 - accuracy: 0.7491 - val_loss: 0.9155 - val_accuracy: 0.6438\n",
            "Epoch 59/100\n",
            "259/259 - 1s - loss: 0.5441 - accuracy: 0.7503 - val_loss: 1.1351 - val_accuracy: 0.5180\n",
            "Epoch 60/100\n",
            "259/259 - 1s - loss: 0.5454 - accuracy: 0.7508 - val_loss: 1.1629 - val_accuracy: 0.6058\n",
            "Epoch 61/100\n",
            "259/259 - 1s - loss: 0.5473 - accuracy: 0.7466 - val_loss: 0.5938 - val_accuracy: 0.7284\n",
            "Epoch 62/100\n",
            "259/259 - 1s - loss: 0.5396 - accuracy: 0.7515 - val_loss: 0.6348 - val_accuracy: 0.7112\n",
            "Epoch 63/100\n",
            "259/259 - 1s - loss: 0.5400 - accuracy: 0.7520 - val_loss: 0.5929 - val_accuracy: 0.7086\n",
            "Epoch 64/100\n",
            "259/259 - 1s - loss: 0.5381 - accuracy: 0.7539 - val_loss: 0.7059 - val_accuracy: 0.6862\n",
            "Epoch 65/100\n",
            "259/259 - 1s - loss: 0.5403 - accuracy: 0.7512 - val_loss: 1.2031 - val_accuracy: 0.5987\n",
            "Epoch 66/100\n",
            "259/259 - 1s - loss: 0.5410 - accuracy: 0.7537 - val_loss: 0.9031 - val_accuracy: 0.5916\n",
            "Epoch 67/100\n",
            "259/259 - 1s - loss: 0.5367 - accuracy: 0.7531 - val_loss: 0.7030 - val_accuracy: 0.7039\n",
            "Epoch 68/100\n",
            "259/259 - 1s - loss: 0.5356 - accuracy: 0.7548 - val_loss: 0.7941 - val_accuracy: 0.6660\n",
            "Epoch 69/100\n",
            "259/259 - 1s - loss: 0.5349 - accuracy: 0.7540 - val_loss: 0.6604 - val_accuracy: 0.6818\n",
            "Epoch 70/100\n",
            "259/259 - 1s - loss: 0.5341 - accuracy: 0.7556 - val_loss: 0.6752 - val_accuracy: 0.7067\n",
            "Epoch 71/100\n",
            "259/259 - 1s - loss: 0.5332 - accuracy: 0.7572 - val_loss: 1.7651 - val_accuracy: 0.3656\n",
            "Epoch 72/100\n",
            "259/259 - 1s - loss: 0.5388 - accuracy: 0.7528 - val_loss: 0.9849 - val_accuracy: 0.6479\n",
            "Epoch 73/100\n",
            "259/259 - 1s - loss: 0.5361 - accuracy: 0.7555 - val_loss: 1.3928 - val_accuracy: 0.4807\n",
            "Epoch 74/100\n",
            "259/259 - 1s - loss: 0.5346 - accuracy: 0.7557 - val_loss: 0.6121 - val_accuracy: 0.7193\n",
            "Epoch 75/100\n",
            "259/259 - 1s - loss: 0.5288 - accuracy: 0.7590 - val_loss: 1.8053 - val_accuracy: 0.5177\n",
            "Epoch 76/100\n",
            "259/259 - 1s - loss: 0.5721 - accuracy: 0.7432 - val_loss: 3.8257 - val_accuracy: 0.4135\n",
            "Epoch 77/100\n",
            "259/259 - 1s - loss: 0.6133 - accuracy: 0.7315 - val_loss: 2.2999 - val_accuracy: 0.3604\n",
            "Epoch 78/100\n",
            "259/259 - 1s - loss: 0.5616 - accuracy: 0.7438 - val_loss: 0.9571 - val_accuracy: 0.6237\n",
            "Epoch 79/100\n",
            "259/259 - 1s - loss: 0.5447 - accuracy: 0.7526 - val_loss: 1.4349 - val_accuracy: 0.4933\n",
            "Epoch 80/100\n",
            "259/259 - 1s - loss: 0.5466 - accuracy: 0.7516 - val_loss: 0.7956 - val_accuracy: 0.6357\n",
            "Epoch 81/100\n",
            "259/259 - 1s - loss: 0.5365 - accuracy: 0.7543 - val_loss: 0.7998 - val_accuracy: 0.6372\n",
            "Epoch 82/100\n",
            "259/259 - 1s - loss: 0.5345 - accuracy: 0.7577 - val_loss: 1.7103 - val_accuracy: 0.3536\n",
            "Epoch 83/100\n",
            "259/259 - 1s - loss: 0.5492 - accuracy: 0.7498 - val_loss: 1.6697 - val_accuracy: 0.5214\n",
            "Epoch 84/100\n",
            "259/259 - 1s - loss: 0.5356 - accuracy: 0.7575 - val_loss: 1.0431 - val_accuracy: 0.5688\n",
            "Epoch 85/100\n",
            "259/259 - 2s - loss: 0.5348 - accuracy: 0.7558 - val_loss: 0.5962 - val_accuracy: 0.7176\n",
            "Epoch 86/100\n",
            "259/259 - 2s - loss: 0.5282 - accuracy: 0.7586 - val_loss: 0.6909 - val_accuracy: 0.6850\n",
            "Epoch 87/100\n",
            "259/259 - 2s - loss: 0.5269 - accuracy: 0.7590 - val_loss: 1.3962 - val_accuracy: 0.3684\n",
            "Epoch 88/100\n",
            "259/259 - 2s - loss: 0.5448 - accuracy: 0.7534 - val_loss: 0.8207 - val_accuracy: 0.6712\n",
            "Epoch 89/100\n",
            "259/259 - 2s - loss: 0.5286 - accuracy: 0.7581 - val_loss: 0.5879 - val_accuracy: 0.7382\n",
            "Epoch 90/100\n",
            "259/259 - 2s - loss: 0.5249 - accuracy: 0.7618 - val_loss: 0.5445 - val_accuracy: 0.7516\n",
            "Epoch 91/100\n",
            "259/259 - 2s - loss: 0.5232 - accuracy: 0.7627 - val_loss: 0.6013 - val_accuracy: 0.7159\n",
            "Epoch 92/100\n",
            "259/259 - 2s - loss: 0.5217 - accuracy: 0.7617 - val_loss: 0.5934 - val_accuracy: 0.7351\n",
            "Epoch 93/100\n",
            "259/259 - 2s - loss: 0.5232 - accuracy: 0.7622 - val_loss: 0.6724 - val_accuracy: 0.7046\n",
            "Epoch 94/100\n",
            "259/259 - 2s - loss: 0.5232 - accuracy: 0.7625 - val_loss: 0.6340 - val_accuracy: 0.7074\n",
            "Epoch 95/100\n",
            "259/259 - 2s - loss: 0.5199 - accuracy: 0.7600 - val_loss: 0.5621 - val_accuracy: 0.7455\n",
            "Epoch 96/100\n",
            "259/259 - 2s - loss: 0.5192 - accuracy: 0.7648 - val_loss: 1.0318 - val_accuracy: 0.6781\n",
            "Epoch 97/100\n",
            "259/259 - 2s - loss: 0.5228 - accuracy: 0.7633 - val_loss: 0.7221 - val_accuracy: 0.7067\n",
            "Epoch 98/100\n",
            "259/259 - 2s - loss: 0.5249 - accuracy: 0.7605 - val_loss: 0.5402 - val_accuracy: 0.7533\n",
            "Epoch 99/100\n",
            "259/259 - 2s - loss: 0.5187 - accuracy: 0.7622 - val_loss: 0.5858 - val_accuracy: 0.7337\n",
            "Epoch 100/100\n",
            "259/259 - 2s - loss: 0.5168 - accuracy: 0.7664 - val_loss: 0.5308 - val_accuracy: 0.7595\n",
            "269/269 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.7717\n",
            "Accuracy is 0.771681010723114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRMrTtWJ5D2",
        "outputId": "a385ea1e-e7b4-46d2-8082-6d2d292a1f8b"
      },
      "source": [
        "word = 'away'\n",
        "\n",
        "while len(word) < 16:\n",
        "  word += ' '\n",
        "\n",
        "temp = list(word)\n",
        "for i in range(len(temp)):\n",
        "  temp[i] = ord(temp[i])\n",
        "\n",
        "sylls = np.argmax(model.predict(temp), axis=-1)\n",
        "print(sylls)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 2 6 6 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "379qpcQOX16x",
        "outputId": "e70291c2-f786-426e-d2d1-2f046a05465b"
      },
      "source": [
        "word = 'a'\n",
        "\n",
        "while len(word) < 16:\n",
        "  word += ' '\n",
        "\n",
        "temp = list(word)\n",
        "for i in range(len(temp)):\n",
        "  temp[i] = ord(temp[i])\n",
        "\n",
        "prediction = model.predict([temp])\n",
        "print(np.argmax(prediction))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "cfZacD8RUSsA",
        "outputId": "23e15c29-d6e3-468c-db07-1c519eaa1ad9"
      },
      "source": [
        "validation_data = pd.read_csv('/content/drive/MyDrive/validation_data.csv')\n",
        "success = 0\n",
        "total = 0\n",
        "\n",
        "for row in validation_data.itertuples():\n",
        "  word = row.Word\n",
        "  while len(word) < 16:\n",
        "    word+= ' '\n",
        "  temp = list(word)\n",
        "  for i in range(len(temp)):\n",
        "    temp[i] = ord(temp[i])\n",
        "  sylls = np.argmax(model.predict(temp), axis=-1)\n",
        "  if sylls[0] == row.Number_of_Syllables:\n",
        "    success += 1\n",
        "    total += 1\n",
        "  else:\n",
        "    total += 1\n",
        "\n",
        "print('accuracy is', success/total)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-d44d8ca3f671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0msylls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msylls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber_of_Syllables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_recreate_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m           \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2972\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-llET58GAB"
      },
      "source": [
        "def rap():\n",
        "    num_lines = int(input('How many lines would you like the rap to be? '))\n",
        "    num_generated_lines = int(input('How many lines should be generated to choose from? '))\n",
        "    count = int(input(\"How many syllables per line? \"))\n",
        "\n",
        "    # Extract all of Mike's lyrics.\n",
        "    text = open(\"AllLyrics.txt\", \"r\").read()\n",
        "    vocabulary = ''.join([i for i in text if not i.isdigit()]).replace(\"\\n\", \" \").split(' ')\n",
        "\n",
        "    # Generate text\n",
        "    def line_generator(vocab):\n",
        "        index = 1\n",
        "        chain = {}\n",
        "        # count = 16 # https://colemizestudios.com/rap-lyrics-syllables/, apparently rappers usually use semiquavers\n",
        "        line_count = 0\n",
        "        number_of_tries = 0\n",
        "\n",
        "        for word in vocab[index:]:\n",
        "            key = vocab[index - 1]\n",
        "            if key in chain:\n",
        "                chain[key].append(word)\n",
        "            else:\n",
        "                chain[key] = [word]\n",
        "            index += 1\n",
        "\n",
        "        word1 = random.choice(list(chain.keys()))\n",
        "        line = word1.capitalize()\n",
        "        word1_with_spaces = word1\n",
        "        while len(word1_with_spaces) < 16:\n",
        "            word1_with_spaces += ' '\n",
        "        temp_word = list(word1_with_spaces)\n",
        "        for i in range(len(temp_word)):\n",
        "            temp_word[i] = ord(temp_word[i])\n",
        "        word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "        word_count = word_syllables\n",
        "        line_count += word_count\n",
        "\n",
        "        while line_count < count:\n",
        "            number_of_tries += 1\n",
        "            word2 = random.choice(chain[word1])\n",
        "            word2_with_spaces = word2\n",
        "            while len(word2_with_spaces) < 16:\n",
        "                word2_with_spaces += ' '\n",
        "            temp_word = list(word2_with_spaces)\n",
        "            for i in range(len(temp_word)):\n",
        "                temp_word[i] = ord(temp_word[i])\n",
        "            word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "            word_count = word_syllables\n",
        "            line_count += word_count\n",
        "            # print(n)\n",
        "            if line_count > count:  # don't include word if it makes line go over syllable count\n",
        "                line_count -= word_count\n",
        "            else:\n",
        "                word1 = word2\n",
        "                line += ' ' + word2.lower()\n",
        "            if number_of_tries > 99:  # if not finding a word with right number of syllables, stop trying\n",
        "                line += ' ERROR FINDING CORRECT SYLLABLE WORD'\n",
        "                line_count = count\n",
        "        return line\n",
        "\n",
        "    # Rhyme Functions\n",
        "    def reverse_syllable_extract(text):\n",
        "        sy_form = []\n",
        "        characters = [char for char in text]\n",
        "        sylls = ['a', 'e', 'i', 'o', 'u', 'y']\n",
        "        for x in characters:\n",
        "            if x in sylls:\n",
        "                sy_form.append(x)\n",
        "        sy_form.reverse()\n",
        "        return sy_form\n",
        "\n",
        "    def rev_syllable_stop_count(text1, text2):\n",
        "        counter = True\n",
        "        i = 0\n",
        "        counter = 0\n",
        "        syll1 = reverse_syllable_extract(text1)\n",
        "        syll2 = reverse_syllable_extract(text2)\n",
        "        while counter:\n",
        "            if i < min(len(syll1), len(syll2)) and syll1[i] == syll2[i]:\n",
        "                counter += 1\n",
        "                i += 1\n",
        "            else:\n",
        "                counter = False\n",
        "        return counter\n",
        "\n",
        "    def next_line_stop_count(start_line, lines):\n",
        "        sy_lines = []\n",
        "        for i in lines:\n",
        "            sy_lines.append(rev_syllable_stop_count(start_line, i))\n",
        "        choice = sy_lines[0]\n",
        "        count = 0\n",
        "        for i in range(len(sy_lines)):\n",
        "            if sy_lines[i] > choice:\n",
        "                choice = sy_lines[i]\n",
        "        return lines[sy_lines.index(choice)]\n",
        "\n",
        "    start_line = line_generator(vocabulary)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if 'ERROR FINDING CORRECT SYLLABLE WORD' in start_line:\n",
        "            start_line = line_generator(vocabulary)\n",
        "        else:\n",
        "            done = True\n",
        "\n",
        "    all_other_lines = []\n",
        "    for i in range(num_generated_lines - 1):\n",
        "      all_other_lines.append(line_generator(vocabulary))\n",
        "    rap = [start_line]\n",
        "\n",
        "    for n, line in enumerate(all_other_lines):\n",
        "        done = False\n",
        "        while not done:\n",
        "            if 'ERROR FINDING CORRECT SYLLABLE WORD' in line:\n",
        "                line = line_generator(vocabulary)\n",
        "                all_other_lines[n] = line\n",
        "            else:\n",
        "                done = True\n",
        "\n",
        "    for i in range(num_lines):\n",
        "        if i % 2 == 1:\n",
        "            next_line = next_line_stop_count(rap[len(rap) - 1], all_other_lines)\n",
        "        else:\n",
        "            next_line = random.choice(all_other_lines)\n",
        "        all_other_lines.remove(next_line)\n",
        "        rap.append(next_line)\n",
        "    return rap"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0WbObRPyyIy",
        "outputId": "487c2070-cc71-4452-87bd-b9e7a7e8cc7d"
      },
      "source": [
        "rap()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How many lines would you like the rap to be? 10\n",
            "How many lines should be generated to choose from? 500\n",
            "How many syllables per line? 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Bulletproof off so son while i'm straight to play-hate cos,\",\n",
              " 'Kurizzle was just kick off friendly fricking rap but they do',\n",
              " 'Intent to the way  close your whole lot of kids, so hurt, she',\n",
              " \"Funky-ass rhythm  rollin down let's all the stakes was when it's\",\n",
              " 'Approved of relief and all for help,you help we leave',\n",
              " \"Sprinkle of in the game i love em, b*****es i check 'em doggy\",\n",
              " \"Fountain that's why you can't wait to  , if i hate water, i\",\n",
              " \"She'll tell on boy you got this real just incase a motherfricker\",\n",
              " \"Bernard parks, i'm on if you it ain't even ready to hang with\",\n",
              " \"Hollow with major busta-ass ninjaz is hating hoes what's up\",\n",
              " 'Oh yah dem ninjas d*ck from the comb clipping the pound gang bang']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9SRl8X-hIQ"
      },
      "source": [
        "# Takes longer to load due to iterating through words until finding word with the right number of syllables\n",
        "# Also takes longer due to replacing lines which contain errors\n",
        "# Takes much much longer due to use of the neural network to predict syllables"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
