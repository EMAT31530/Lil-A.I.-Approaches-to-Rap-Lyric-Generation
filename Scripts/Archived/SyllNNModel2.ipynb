{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SyllNNModel2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsnsZh72-hH0"
      },
      "source": [
        "# Markov with Syllable Neural Network\n",
        "# Same rhyme ranker as MikesVersion1\n",
        "# Changelog:\n",
        "# Uses correct lyrics to generate rap, so no punctuation gets into the rap\n",
        "# Censors rap after generation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQph78-g-hIE",
        "outputId": "40d70f9f-7f1d-47ae-de14-c1771fbceb98"
      },
      "source": [
        "#@title Import Statements\n",
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pprint import pprint # For pretty printing\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n",
        "\n",
        "# For the Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# For importing the training, testing and validation data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyGithub\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/44/df78514f2b5f5abaec330596e0fa3273824238399a964d1a7e82fd39990d/PyGithub-1.54.1-py3-none-any.whl (289kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 194kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 245kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 286kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 6.8MB/s \n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Collecting pyjwt<2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Installing collected packages: deprecated, pyjwt, PyGithub\n",
            "Successfully installed PyGithub-1.54.1 deprecated-1.2.12 pyjwt-1.7.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N624BVtBsmy"
      },
      "source": [
        "#@title Function Definitions\n",
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"ye\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "    print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Committing files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5On3o1fS-hIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f2cee-2ff2-4491-f207-b156bf49d1d8"
      },
      "source": [
        "# Import all of Mike's lyrics.\n",
        "import_github()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key ··········\n",
            "Main Branch: Yes or No? ye\n",
            "Importing Github cleaned text files...\n",
            "AllLyrics.txt is now up to date!\n",
            "Writing file 0 capitals.csv\n",
            "Writing file 1 censors.csv\n",
            "Writing file 2 censors2.csv\n",
            "All files now up to date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3WahREYjCV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a3c1b8-190e-4318-bf39-f203e0a3854c"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/training_data.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/testing_data.csv')\n",
        "validation = pd.read_csv('/content/drive/MyDrive/validation_data.csv')\n",
        "\n",
        "train_in = []\n",
        "test_in = []\n",
        "train_out = []\n",
        "test_out = []\n",
        "validation_in = []\n",
        "validation_out = []\n",
        "\n",
        "for row in train.itertuples():\n",
        "    # train_in.append(row.Word)\n",
        "    train_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':  # an empty word was getting in for some reason\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        train_in.append(temp)\n",
        "\n",
        "for row in test.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    test_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        test_in.append(temp)\n",
        "\n",
        "for row in validation.itertuples():\n",
        "    # test_in.append(row.Word)\n",
        "    validation_out.append(row.Number_of_Syllables)\n",
        "    if row.Word == '                ':\n",
        "        pass\n",
        "    else:\n",
        "        temp = list(row.Word)\n",
        "        for i in range(len(temp)):\n",
        "            temp[i] = ord(temp[i])\n",
        "        validation_in.append(temp)\n",
        "\n",
        "test_in = np.array(test_in)\n",
        "test_out = np.array(test_out)\n",
        "train_in = np.array(train_in)\n",
        "train_out = np.array(train_out)\n",
        "validation_in = np.array(validation_in)\n",
        "validation_out = np.array(validation_out)\n",
        "\n",
        "max_word = 143\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(max_word, 100))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(100, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(50, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(8, activation=tf.nn.softmax))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.2, use_locking=False),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_in, train_out, epochs=100, batch_size=100, validation_data=(test_in, test_out), verbose=2)\n",
        "\n",
        "results = model.evaluate(validation_in, validation_out)\n",
        "print('Accuracy is', results[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 100)         14300     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 8)                 408       \n",
            "=================================================================\n",
            "Total params: 29,858\n",
            "Trainable params: 29,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "259/259 - 1s - loss: 1.2498 - accuracy: 0.4553 - val_loss: 1.1060 - val_accuracy: 0.4820\n",
            "Epoch 2/100\n",
            "259/259 - 1s - loss: 0.9183 - accuracy: 0.5925 - val_loss: 1.3498 - val_accuracy: 0.3864\n",
            "Epoch 3/100\n",
            "259/259 - 1s - loss: 0.8363 - accuracy: 0.6279 - val_loss: 4.1719 - val_accuracy: 0.2108\n",
            "Epoch 4/100\n",
            "259/259 - 1s - loss: 0.7852 - accuracy: 0.6557 - val_loss: 2.3766 - val_accuracy: 0.3856\n",
            "Epoch 5/100\n",
            "259/259 - 1s - loss: 0.7311 - accuracy: 0.6779 - val_loss: 1.0330 - val_accuracy: 0.6624\n",
            "Epoch 6/100\n",
            "259/259 - 1s - loss: 0.7094 - accuracy: 0.6876 - val_loss: 1.2852 - val_accuracy: 0.4893\n",
            "Epoch 7/100\n",
            "259/259 - 1s - loss: 0.6828 - accuracy: 0.6965 - val_loss: 1.4639 - val_accuracy: 0.6543\n",
            "Epoch 8/100\n",
            "259/259 - 1s - loss: 0.6722 - accuracy: 0.7027 - val_loss: 2.9473 - val_accuracy: 0.1648\n",
            "Epoch 9/100\n",
            "259/259 - 1s - loss: 0.6597 - accuracy: 0.7047 - val_loss: 0.8652 - val_accuracy: 0.6330\n",
            "Epoch 10/100\n",
            "259/259 - 1s - loss: 0.6464 - accuracy: 0.7088 - val_loss: 0.6401 - val_accuracy: 0.7140\n",
            "Epoch 11/100\n",
            "259/259 - 1s - loss: 0.6318 - accuracy: 0.7108 - val_loss: 1.3329 - val_accuracy: 0.5658\n",
            "Epoch 12/100\n",
            "259/259 - 1s - loss: 0.6246 - accuracy: 0.7165 - val_loss: 1.4818 - val_accuracy: 0.5207\n",
            "Epoch 13/100\n",
            "259/259 - 1s - loss: 0.6250 - accuracy: 0.7158 - val_loss: 1.6627 - val_accuracy: 0.2847\n",
            "Epoch 14/100\n",
            "259/259 - 1s - loss: 0.6116 - accuracy: 0.7181 - val_loss: 0.8257 - val_accuracy: 0.6469\n",
            "Epoch 15/100\n",
            "259/259 - 1s - loss: 0.6065 - accuracy: 0.7214 - val_loss: 0.7135 - val_accuracy: 0.6905\n",
            "Epoch 16/100\n",
            "259/259 - 1s - loss: 0.6021 - accuracy: 0.7235 - val_loss: 0.8902 - val_accuracy: 0.5516\n",
            "Epoch 17/100\n",
            "259/259 - 1s - loss: 0.5895 - accuracy: 0.7272 - val_loss: 0.6501 - val_accuracy: 0.6817\n",
            "Epoch 18/100\n",
            "259/259 - 1s - loss: 0.5881 - accuracy: 0.7277 - val_loss: 0.8539 - val_accuracy: 0.6817\n",
            "Epoch 19/100\n",
            "259/259 - 1s - loss: 0.5868 - accuracy: 0.7279 - val_loss: 1.0364 - val_accuracy: 0.5817\n",
            "Epoch 20/100\n",
            "259/259 - 1s - loss: 0.5793 - accuracy: 0.7354 - val_loss: 0.9723 - val_accuracy: 0.5887\n",
            "Epoch 21/100\n",
            "259/259 - 1s - loss: 0.5808 - accuracy: 0.7349 - val_loss: 1.8009 - val_accuracy: 0.4014\n",
            "Epoch 22/100\n",
            "259/259 - 1s - loss: 0.5797 - accuracy: 0.7309 - val_loss: 0.9168 - val_accuracy: 0.6409\n",
            "Epoch 23/100\n",
            "259/259 - 1s - loss: 0.5741 - accuracy: 0.7329 - val_loss: 0.6108 - val_accuracy: 0.7191\n",
            "Epoch 24/100\n",
            "259/259 - 1s - loss: 0.5726 - accuracy: 0.7334 - val_loss: 0.5936 - val_accuracy: 0.7233\n",
            "Epoch 25/100\n",
            "259/259 - 1s - loss: 0.5719 - accuracy: 0.7343 - val_loss: 0.7528 - val_accuracy: 0.6840\n",
            "Epoch 26/100\n",
            "259/259 - 1s - loss: 0.5667 - accuracy: 0.7383 - val_loss: 1.6386 - val_accuracy: 0.3520\n",
            "Epoch 27/100\n",
            "259/259 - 1s - loss: 0.5703 - accuracy: 0.7382 - val_loss: 0.8945 - val_accuracy: 0.5942\n",
            "Epoch 28/100\n",
            "259/259 - 1s - loss: 0.5649 - accuracy: 0.7371 - val_loss: 0.5807 - val_accuracy: 0.7273\n",
            "Epoch 29/100\n",
            "259/259 - 1s - loss: 0.5615 - accuracy: 0.7407 - val_loss: 0.5996 - val_accuracy: 0.7187\n",
            "Epoch 30/100\n",
            "259/259 - 1s - loss: 0.5639 - accuracy: 0.7387 - val_loss: 0.8488 - val_accuracy: 0.6604\n",
            "Epoch 31/100\n",
            "259/259 - 1s - loss: 0.5617 - accuracy: 0.7407 - val_loss: 1.2899 - val_accuracy: 0.5294\n",
            "Epoch 32/100\n",
            "259/259 - 1s - loss: 0.5644 - accuracy: 0.7401 - val_loss: 0.8127 - val_accuracy: 0.6750\n",
            "Epoch 33/100\n",
            "259/259 - 1s - loss: 0.5623 - accuracy: 0.7410 - val_loss: 0.5660 - val_accuracy: 0.7420\n",
            "Epoch 34/100\n",
            "259/259 - 1s - loss: 0.5581 - accuracy: 0.7428 - val_loss: 1.2329 - val_accuracy: 0.5870\n",
            "Epoch 35/100\n",
            "259/259 - 1s - loss: 0.5611 - accuracy: 0.7410 - val_loss: 0.8159 - val_accuracy: 0.6123\n",
            "Epoch 36/100\n",
            "259/259 - 1s - loss: 0.5574 - accuracy: 0.7415 - val_loss: 1.0846 - val_accuracy: 0.5731\n",
            "Epoch 37/100\n",
            "259/259 - 1s - loss: 0.6276 - accuracy: 0.7195 - val_loss: 1.5119 - val_accuracy: 0.3455\n",
            "Epoch 38/100\n",
            "259/259 - 1s - loss: 0.5964 - accuracy: 0.7265 - val_loss: 1.1454 - val_accuracy: 0.6447\n",
            "Epoch 39/100\n",
            "259/259 - 1s - loss: 0.5793 - accuracy: 0.7359 - val_loss: 2.0414 - val_accuracy: 0.3524\n",
            "Epoch 40/100\n",
            "259/259 - 1s - loss: 0.5785 - accuracy: 0.7351 - val_loss: 0.7789 - val_accuracy: 0.6768\n",
            "Epoch 41/100\n",
            "259/259 - 1s - loss: 0.5698 - accuracy: 0.7368 - val_loss: 1.3302 - val_accuracy: 0.6078\n",
            "Epoch 42/100\n",
            "259/259 - 1s - loss: 0.5681 - accuracy: 0.7374 - val_loss: 0.7251 - val_accuracy: 0.6759\n",
            "Epoch 43/100\n",
            "259/259 - 1s - loss: 0.5628 - accuracy: 0.7391 - val_loss: 0.6936 - val_accuracy: 0.6757\n",
            "Epoch 44/100\n",
            "259/259 - 1s - loss: 0.5610 - accuracy: 0.7417 - val_loss: 0.8208 - val_accuracy: 0.5807\n",
            "Epoch 45/100\n",
            "259/259 - 1s - loss: 0.5590 - accuracy: 0.7425 - val_loss: 0.7548 - val_accuracy: 0.6438\n",
            "Epoch 46/100\n",
            "259/259 - 1s - loss: 0.5593 - accuracy: 0.7421 - val_loss: 0.5739 - val_accuracy: 0.7377\n",
            "Epoch 47/100\n",
            "259/259 - 1s - loss: 0.5532 - accuracy: 0.7439 - val_loss: 0.8294 - val_accuracy: 0.6684\n",
            "Epoch 48/100\n",
            "259/259 - 1s - loss: 0.5585 - accuracy: 0.7417 - val_loss: 0.6107 - val_accuracy: 0.7110\n",
            "Epoch 49/100\n",
            "259/259 - 1s - loss: 0.5528 - accuracy: 0.7455 - val_loss: 1.2382 - val_accuracy: 0.4308\n",
            "Epoch 50/100\n",
            "259/259 - 1s - loss: 0.5547 - accuracy: 0.7443 - val_loss: 0.9816 - val_accuracy: 0.6666\n",
            "Epoch 51/100\n",
            "259/259 - 1s - loss: 0.5525 - accuracy: 0.7449 - val_loss: 0.7322 - val_accuracy: 0.6857\n",
            "Epoch 52/100\n",
            "259/259 - 1s - loss: 0.5551 - accuracy: 0.7434 - val_loss: 0.6741 - val_accuracy: 0.6751\n",
            "Epoch 53/100\n",
            "259/259 - 1s - loss: 0.5514 - accuracy: 0.7468 - val_loss: 1.0000 - val_accuracy: 0.6233\n",
            "Epoch 54/100\n",
            "259/259 - 1s - loss: 0.5508 - accuracy: 0.7462 - val_loss: 2.3380 - val_accuracy: 0.4405\n",
            "Epoch 55/100\n",
            "259/259 - 1s - loss: 0.5713 - accuracy: 0.7422 - val_loss: 0.6723 - val_accuracy: 0.7156\n",
            "Epoch 56/100\n",
            "259/259 - 1s - loss: 0.5496 - accuracy: 0.7464 - val_loss: 2.2280 - val_accuracy: 0.3199\n",
            "Epoch 57/100\n",
            "259/259 - 1s - loss: 0.5700 - accuracy: 0.7404 - val_loss: 0.8123 - val_accuracy: 0.6500\n",
            "Epoch 58/100\n",
            "259/259 - 1s - loss: 0.5489 - accuracy: 0.7455 - val_loss: 0.6682 - val_accuracy: 0.6946\n",
            "Epoch 59/100\n",
            "259/259 - 1s - loss: 0.5454 - accuracy: 0.7479 - val_loss: 0.7519 - val_accuracy: 0.6619\n",
            "Epoch 60/100\n",
            "259/259 - 1s - loss: 0.5454 - accuracy: 0.7483 - val_loss: 0.6724 - val_accuracy: 0.7183\n",
            "Epoch 61/100\n",
            "259/259 - 1s - loss: 0.5447 - accuracy: 0.7496 - val_loss: 0.8220 - val_accuracy: 0.6089\n",
            "Epoch 62/100\n",
            "259/259 - 1s - loss: 0.5451 - accuracy: 0.7514 - val_loss: 0.6012 - val_accuracy: 0.7068\n",
            "Epoch 63/100\n",
            "259/259 - 1s - loss: 0.5420 - accuracy: 0.7524 - val_loss: 0.6336 - val_accuracy: 0.6998\n",
            "Epoch 64/100\n",
            "259/259 - 1s - loss: 0.5403 - accuracy: 0.7508 - val_loss: 3.0810 - val_accuracy: 0.4420\n",
            "Epoch 65/100\n",
            "259/259 - 1s - loss: 0.5768 - accuracy: 0.7418 - val_loss: 0.5515 - val_accuracy: 0.7472\n",
            "Epoch 66/100\n",
            "259/259 - 1s - loss: 0.5404 - accuracy: 0.7510 - val_loss: 0.6656 - val_accuracy: 0.7070\n",
            "Epoch 67/100\n",
            "259/259 - 1s - loss: 0.5416 - accuracy: 0.7522 - val_loss: 0.7446 - val_accuracy: 0.6865\n",
            "Epoch 68/100\n",
            "259/259 - 1s - loss: 0.5376 - accuracy: 0.7539 - val_loss: 0.6212 - val_accuracy: 0.7031\n",
            "Epoch 69/100\n",
            "259/259 - 1s - loss: 0.5375 - accuracy: 0.7544 - val_loss: 0.6263 - val_accuracy: 0.7174\n",
            "Epoch 70/100\n",
            "259/259 - 1s - loss: 0.5366 - accuracy: 0.7555 - val_loss: 0.7108 - val_accuracy: 0.6815\n",
            "Epoch 71/100\n",
            "259/259 - 1s - loss: 0.5385 - accuracy: 0.7540 - val_loss: 1.9764 - val_accuracy: 0.5152\n",
            "Epoch 72/100\n",
            "259/259 - 1s - loss: 0.5476 - accuracy: 0.7508 - val_loss: 1.5880 - val_accuracy: 0.5787\n",
            "Epoch 73/100\n",
            "259/259 - 1s - loss: 0.5447 - accuracy: 0.7493 - val_loss: 0.5477 - val_accuracy: 0.7509\n",
            "Epoch 74/100\n",
            "259/259 - 1s - loss: 0.5340 - accuracy: 0.7549 - val_loss: 0.6347 - val_accuracy: 0.7063\n",
            "Epoch 75/100\n",
            "259/259 - 1s - loss: 0.5338 - accuracy: 0.7530 - val_loss: 1.0802 - val_accuracy: 0.6537\n",
            "Epoch 76/100\n",
            "259/259 - 1s - loss: 0.5377 - accuracy: 0.7529 - val_loss: 0.6929 - val_accuracy: 0.6891\n",
            "Epoch 77/100\n",
            "259/259 - 1s - loss: 0.5349 - accuracy: 0.7536 - val_loss: 0.5942 - val_accuracy: 0.7258\n",
            "Epoch 78/100\n",
            "259/259 - 1s - loss: 0.5321 - accuracy: 0.7562 - val_loss: 0.5458 - val_accuracy: 0.7548\n",
            "Epoch 79/100\n",
            "259/259 - 1s - loss: 0.5300 - accuracy: 0.7558 - val_loss: 0.6229 - val_accuracy: 0.7389\n",
            "Epoch 80/100\n",
            "259/259 - 1s - loss: 0.5307 - accuracy: 0.7569 - val_loss: 0.6230 - val_accuracy: 0.7298\n",
            "Epoch 81/100\n",
            "259/259 - 1s - loss: 0.5308 - accuracy: 0.7553 - val_loss: 0.8712 - val_accuracy: 0.5695\n",
            "Epoch 82/100\n",
            "259/259 - 1s - loss: 0.5324 - accuracy: 0.7565 - val_loss: 0.6982 - val_accuracy: 0.7113\n",
            "Epoch 83/100\n",
            "259/259 - 1s - loss: 0.5284 - accuracy: 0.7599 - val_loss: 1.5142 - val_accuracy: 0.5156\n",
            "Epoch 84/100\n",
            "259/259 - 1s - loss: 0.5386 - accuracy: 0.7565 - val_loss: 0.6743 - val_accuracy: 0.7029\n",
            "Epoch 85/100\n",
            "259/259 - 1s - loss: 0.5275 - accuracy: 0.7587 - val_loss: 0.5970 - val_accuracy: 0.7179\n",
            "Epoch 86/100\n",
            "259/259 - 1s - loss: 0.5247 - accuracy: 0.7606 - val_loss: 0.6000 - val_accuracy: 0.7127\n",
            "Epoch 87/100\n",
            "259/259 - 1s - loss: 0.5243 - accuracy: 0.7598 - val_loss: 0.6262 - val_accuracy: 0.7377\n",
            "Epoch 88/100\n",
            "259/259 - 1s - loss: 0.5264 - accuracy: 0.7585 - val_loss: 0.6316 - val_accuracy: 0.7136\n",
            "Epoch 89/100\n",
            "259/259 - 1s - loss: 0.5249 - accuracy: 0.7589 - val_loss: 1.1201 - val_accuracy: 0.5543\n",
            "Epoch 90/100\n",
            "259/259 - 1s - loss: 0.5272 - accuracy: 0.7606 - val_loss: 0.6929 - val_accuracy: 0.6997\n",
            "Epoch 91/100\n",
            "259/259 - 1s - loss: 0.5255 - accuracy: 0.7589 - val_loss: 0.5724 - val_accuracy: 0.7372\n",
            "Epoch 92/100\n",
            "259/259 - 1s - loss: 0.5223 - accuracy: 0.7606 - val_loss: 0.8553 - val_accuracy: 0.6696\n",
            "Epoch 93/100\n",
            "259/259 - 1s - loss: 0.5253 - accuracy: 0.7591 - val_loss: 0.8748 - val_accuracy: 0.6329\n",
            "Epoch 94/100\n",
            "259/259 - 1s - loss: 0.5234 - accuracy: 0.7611 - val_loss: 0.8237 - val_accuracy: 0.6530\n",
            "Epoch 95/100\n",
            "259/259 - 1s - loss: 0.5228 - accuracy: 0.7626 - val_loss: 0.6597 - val_accuracy: 0.7050\n",
            "Epoch 96/100\n",
            "259/259 - 1s - loss: 0.5199 - accuracy: 0.7643 - val_loss: 0.7939 - val_accuracy: 0.6815\n",
            "Epoch 97/100\n",
            "259/259 - 1s - loss: 0.5214 - accuracy: 0.7616 - val_loss: 2.7302 - val_accuracy: 0.3469\n",
            "Epoch 98/100\n",
            "259/259 - 1s - loss: 0.5732 - accuracy: 0.7417 - val_loss: 0.9913 - val_accuracy: 0.6201\n",
            "Epoch 99/100\n",
            "259/259 - 1s - loss: 0.5371 - accuracy: 0.7552 - val_loss: 0.5900 - val_accuracy: 0.7341\n",
            "Epoch 100/100\n",
            "259/259 - 1s - loss: 0.5230 - accuracy: 0.7606 - val_loss: 0.8152 - val_accuracy: 0.6703\n",
            "269/269 [==============================] - 0s 1ms/step - loss: 0.8050 - accuracy: 0.6772\n",
            "Accuracy is 0.6772468090057373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-llET58GAB"
      },
      "source": [
        "def rap():\n",
        "    num_lines = int(input('How many lines would you like the rap to be? '))\n",
        "    num_generated_lines = int(input('How many lines should be generated to choose from? '))\n",
        "    count = int(input(\"How many syllables per line? \"))\n",
        "\n",
        "    # Extract all of Mike's lyrics.\n",
        "    text = open(\"/content/drive/MyDrive/AllLyrics_uncleanOLD.txt\", \"r\").read()\n",
        "    vocabulary = ''.join([i for i in text if not i.isdigit()]).replace(\"\\n\", \" \").split(' ')\n",
        "\n",
        "    # Generate text\n",
        "    def line_generator(vocab):\n",
        "        index = 1\n",
        "        chain = {}\n",
        "        # count = 16 # https://colemizestudios.com/rap-lyrics-syllables/, apparently rappers usually use semiquavers\n",
        "        line_count = 0\n",
        "        number_of_tries = 0\n",
        "\n",
        "        for word in vocab[index:]:\n",
        "            key = vocab[index - 1]\n",
        "            if key in chain:\n",
        "                chain[key].append(word)\n",
        "            else:\n",
        "                chain[key] = [word]\n",
        "            index += 1\n",
        "\n",
        "        word1 = random.choice(list(chain.keys()))\n",
        "        line = word1.capitalize()\n",
        "        word1_with_spaces = word1\n",
        "        while len(word1_with_spaces) < 16:\n",
        "            word1_with_spaces += ' '\n",
        "        temp_word = list(word1_with_spaces)\n",
        "        for i in range(len(temp_word)):\n",
        "            temp_word[i] = ord(temp_word[i])\n",
        "        word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "        word_count = word_syllables\n",
        "        line_count += word_count\n",
        "\n",
        "        while line_count < count:\n",
        "            number_of_tries += 1\n",
        "            word2 = random.choice(chain[word1])\n",
        "            word2_with_spaces = word2\n",
        "            while len(word2_with_spaces) < 16:\n",
        "                word2_with_spaces += ' '\n",
        "            temp_word = list(word2_with_spaces)\n",
        "            for i in range(len(temp_word)):\n",
        "                temp_word[i] = ord(temp_word[i])\n",
        "            word_syllables = np.argmax(model.predict([temp_word]), axis=-1)\n",
        "            word_count = word_syllables\n",
        "            line_count += word_count\n",
        "            # print(n)\n",
        "            if line_count > count:  # don't include word if it makes line go over syllable count\n",
        "                line_count -= word_count\n",
        "            else:\n",
        "                word1 = word2\n",
        "                line += ' ' + word2.lower()\n",
        "            if number_of_tries > 99:  # if not finding a word with right number of syllables, stop trying\n",
        "                line += ' ERROR FINDING CORRECT SYLLABLE WORD'\n",
        "                line_count = count\n",
        "        return line\n",
        "\n",
        "    # Rhyme Functions\n",
        "    def reverse_syllable_extract(text):\n",
        "        sy_form = []\n",
        "        characters = [char for char in text]\n",
        "        sylls = ['a', 'e', 'i', 'o', 'u', 'y']\n",
        "        for x in characters:\n",
        "            if x in sylls:\n",
        "                sy_form.append(x)\n",
        "        sy_form.reverse()\n",
        "        return sy_form\n",
        "\n",
        "    def rev_syllable_stop_count(text1, text2):\n",
        "        counter = True\n",
        "        i = 0\n",
        "        counter = 0\n",
        "        syll1 = reverse_syllable_extract(text1)\n",
        "        syll2 = reverse_syllable_extract(text2)\n",
        "        while counter:\n",
        "            if i < min(len(syll1), len(syll2)) and syll1[i] == syll2[i]:\n",
        "                counter += 1\n",
        "                i += 1\n",
        "            else:\n",
        "                counter = False\n",
        "        return counter\n",
        "\n",
        "    def next_line_stop_count(start_line, lines):\n",
        "        sy_lines = []\n",
        "        for i in lines:\n",
        "            sy_lines.append(rev_syllable_stop_count(start_line, i))\n",
        "        choice = sy_lines[0]\n",
        "        count = 0\n",
        "        for i in range(len(sy_lines)):\n",
        "            if sy_lines[i] > choice:\n",
        "                choice = sy_lines[i]\n",
        "        return lines[sy_lines.index(choice)]\n",
        "\n",
        "    start_line = line_generator(vocabulary)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if 'ERROR FINDING CORRECT SYLLABLE WORD' in start_line:\n",
        "            start_line = line_generator(vocabulary)\n",
        "        else:\n",
        "            done = True\n",
        "\n",
        "    all_other_lines = []\n",
        "    for i in range(num_generated_lines - 1):\n",
        "      all_other_lines.append(line_generator(vocabulary))\n",
        "    rap = [start_line]\n",
        "\n",
        "    for n, line in enumerate(all_other_lines):\n",
        "        done = False\n",
        "        while not done:\n",
        "            if 'ERROR FINDING CORRECT SYLLABLE WORD' in line:\n",
        "                line = line_generator(vocabulary)\n",
        "                all_other_lines[n] = line\n",
        "            else:\n",
        "                done = True\n",
        "\n",
        "    for i in range(num_lines):\n",
        "        if i % 2 == 1:\n",
        "            next_line = next_line_stop_count(rap[len(rap) - 1], all_other_lines)\n",
        "        else:\n",
        "            next_line = random.choice(all_other_lines)\n",
        "        all_other_lines.remove(next_line)\n",
        "        rap.append(next_line)\n",
        "    censors = pd.read_csv('censors2.csv')\n",
        "    for i, line in enumerate(rap):\n",
        "      for j, word in enumerate(line):\n",
        "        for row in censors.itertuples():\n",
        "          if word == row.word:\n",
        "            line[j] = row.replacement\n",
        "      rap[i] = line\n",
        "    \n",
        "    return rap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0WbObRPyyIy",
        "outputId": "b8a5ceca-714e-4e56-ca2d-9b4ec08a8935"
      },
      "source": [
        "rap()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How many lines would you like the rap to be? 10\n",
            "How many lines should be generated to choose from? 500\n",
            "How many syllables per line? 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cashflow  bathroom started up the loot gimme one for my dick',\n",
              " 'Cab to encounter wit big  fulfillin fantasies without',\n",
              " 'Killa nixga  so i gotta be friends with the beach  ron',\n",
              " 'Chance  biggie biggie biggie  got the bad boy i know',\n",
              " 'Housing  simple and find me  start stackin  tie you know',\n",
              " 'Caper  doin sign language is real nixga yes my own porno',\n",
              " 'Blinkers  i got enough heart i learn though situation',\n",
              " 'Matter  with a sudden oh thought i hear death knockin at you',\n",
              " 'Galore  n now tell you lose me in the car keys  for a',\n",
              " 'Christening  biggie took me shit so paid uh sometimes i',\n",
              " 'Whack  from gym class to slow as a true fuckin but the pounds you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9SRl8X-hIQ"
      },
      "source": [
        "# Takes longer to load due to iterating through words until finding word with the right number of syllables\n",
        "# Also takes longer due to replacing lines which contain errors\n",
        "# Takes much much longer due to use of the neural network to predict syllables"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
