{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MegYdBK5tI8b"
      },
      "source": [
        "# Classifier 1\n",
        "# Here I have started work on a classifier using multinomial bayes - it seems like a good start \n",
        "# This is all done without the data we need so have just worked with the generators vocabulary - next step is to get our training/test data\n",
        "# All my notes on classifiers is first included"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ow82KcZtIOF"
      },
      "source": [
        "Classifier to test our rap\n",
        "\n",
        "We need to test our rap in some form and one way could be by creating a tester of some form that will classify lyrics into a genre.  If we create working and properly tested classifier(s) and then one of our raps is classified as a country song - something has gone wrong.\n",
        "\n",
        "Initial ideas to consider \n",
        "- KNN of some form,\n",
        "- Deep neural networks,\n",
        "- Keras has a IMDB review set we can look at https://keras.io/api/datasets/imdb/\n",
        "- https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2728368.pdf\n",
        "- http://cs229.stanford.edu/proj2017/final-reports/5242682.pdf\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "\n",
        "What will our classifier use as it's data? We need lyrics and a classified genre with each set to test and train.  With lyrics we can classify by the vocabulary of the lyrics (as our vocab is all from rap we should do well here), we could also look to see if there is a way once we have built our models that we look at the structure of songs and see if this has an effect on the genre.\n",
        "\n",
        "As far as getting training and test data to build a classifier we can see if we can use the work we have for building the vocabulary of the generators.  We don't need to censor lyrics like with our generator as the output will be a genre name not lyrics.\n",
        "\n",
        "We do need to make our lyrics uniform in that no punctuation and no capitals, same as vocabulary generated for our rapper.\n",
        "\n",
        "Ideas of genres\n",
        "- rap\n",
        "- rock (include all types)\n",
        "- pop (this is very vague)\n",
        "- soul (include R&B)\n",
        "- country\n",
        "- reggae \n",
        "\n",
        "We should aim to have our training and test data have roughly equal proportions of each genre so as to avoid skew."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM0LUW_50vtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66ec30e-ddcb-4776-bce9-db2540b8ef32"
      },
      "source": [
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.6/dist-packages (1.54.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.6/dist-packages (from PyGithub) (1.2.11)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.6/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: pyjwt<2.0 in /usr/local/lib/python3.6/dist-packages (from PyGithub) (1.7.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated->PyGithub) (1.12.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZn5pCk_cjpT"
      },
      "source": [
        "# Need our training/test data, some is already being purposed on wordcloud.py for now just using generators lyrics "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcn3cchkiacY"
      },
      "source": [
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "        print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Commiting files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8LNWFpficsY",
        "outputId": "c91b6b8f-7167-43d7-863b-9be8f29b1501"
      },
      "source": [
        "# Import vocabulary. PATKEY: 5ae2446bd5828c9e27deb3865118d9e783aa6e15\n",
        "import_github()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key ··········\n",
            "Main Branch: Yes or No? yes\n",
            "Importing Github cleaned text files...\n",
            "AllLyrics.txt is now up to date!\n",
            "Writing file 0 capitals.csv\n",
            "All files now up to date!\n",
            "Writing file 1 censors.csv\n",
            "All files now up to date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6V3U8zkidaM"
      },
      "source": [
        "# Read lyrics\n",
        "Text = open(\"AllLyrics.txt\", \"r\").read()\n",
        "Vocabulary = ''.join([i for i in Text if not i.isdigit()]).replace(\"\\n\", \" \").split(' ')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiHYaTQOi80z",
        "outputId": "6b0ed8d6-7d91-4f84-9eac-22b4e575bee6"
      },
      "source": [
        "len(Vocabulary)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfJSMfexrfMe"
      },
      "source": [
        "Class =['rap']*46236"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAtOCGE8eMDi"
      },
      "source": [
        "genres = ['rap', 'rock', 'soul', 'country'] # pop too broad possibly - reggae less common for finding lyrics possibly too\n",
        "# will have to make csv files containing the lyrics and the accompanying genre - wont need more data than that"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJDuimPqpSnj"
      },
      "source": [
        "# for this first play we are using all data as training data then just test on the Markov6 generated lyrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cXdqjXqhSUG",
        "outputId": "90cdcfb0-516f-4502-c31b-88506ccbfab6"
      },
      "source": [
        "# just word frequency\n",
        "# countvectorizer uses simple word counts\n",
        "classifier = Pipeline([('vector', CountVectorizer()), ('classifier', MultinomialNB(alpha=0))]) \n",
        "# can figure out what alpha is best - info on one of the scikit links above\n",
        "\n",
        "# training - obviously need different data to train\n",
        "classifier.fit(Vocabulary, Class)  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vector',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 MultinomialNB(alpha=0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWyIgC7-kA9U",
        "outputId": "da35b64e-6fb0-4fc3-9808-07ae5a697c95"
      },
      "source": [
        "# how does the classifier classify genre of each line (the lyrics were generated by function on Markov_6)\n",
        "classifier.predict(['Bumpin i meant for you call my ninja like',\n",
        " 'Biz dont take their baby mommas ninja frick you nasty boy you',\n",
        " 'Shifty sticks and pray and flee the frick all of you',\n",
        " 'Glocks but all ill die slow',\n",
        " 'Wondering if im askin blunt sip champagne range rover been outside for',\n",
        " 'And youre so take that crown two pounds you know',\n",
        " 'Publishing i thought i get witcha can i could cop',\n",
        " 'Miss the more cause you in the right one',\n",
        " 'Onyx and them hoes i love',\n",
        " 'Gat call me puff daddy biggie gots ta like',\n",
        " 'Everything around me shit b***** in ya imma stay yappin when',\n",
        " 'Hum all about fingers in the loot im',\n",
        " 'Rollem up heard whos this yeah keep on top sky is',\n",
        " 'Drunk of ninjaz from now drop to',\n",
        " 'Declinin windin like flypaper neighbor slow down',\n",
        " 'Expensive cars i tote my crew i only got enough heart',\n",
        " 'Lame dudes whos next move but the drugs to spit phrases thatll',\n",
        " 'Guy well its cool and your poop so hard to',\n",
        " 'Clap wit my life in ma little nasty boy',\n",
        " 'Dial you should too much better man played',\n",
        " 'Lali like that you frick doin all mcs have'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap',\n",
              "       'rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap', 'rap',\n",
              "       'rap', 'rap', 'rap'], dtype='<U3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qYzmcvKsGkP"
      },
      "source": [
        "# it has predicted it all as rap but then it only knows rap too"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbvrzimcsWtE",
        "outputId": "87cab1f1-bac6-420c-9a52-f3a9a57978af"
      },
      "source": [
        "# to see how our model predicts the lyrics mostly \n",
        "classified = classifier.predict(Vocabulary) # here vocabulary would have to be the test vocabulary\n",
        "np.mean(classified == Class) # here class would have to be the test genre classes\n",
        "# it should be 1.0 or something went badly wrong"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLTEP2qchnLz"
      },
      "source": [
        "# term frequency-inverse document frequency - look at to extend but should get our data before"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}