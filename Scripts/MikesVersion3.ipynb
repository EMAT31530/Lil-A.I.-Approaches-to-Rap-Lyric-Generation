{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MikesVersion3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsnsZh72-hH0"
      },
      "source": [
        "# Markov Attempt 4 \r\n",
        "# All lyrics - way more lines made - same rhyme ranker as in attempt 1\r\n",
        "# Changelog: \r\n",
        "# - Uploaded the fuction for updating the bad words. \r\n",
        "# - Uploaded a new function for recursively importing the cleaned lyrics. This way it's always updated. \r\n",
        "# - Changed the way which words are capitalized with a capitals.csv which can be changed easily\r\n",
        "# - Made the imports faster by changing the folder hierarchy \r\n",
        "# - Added the ability to change main branch, not advised to use tho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQph78-g-hIE",
        "cellView": "form",
        "outputId": "88ef3345-c510-4cf2-bf88-efdb1d5a5ec4"
      },
      "source": [
        "#@title Import Statements\r\n",
        "!pip install PyGithub\r\n",
        "\r\n",
        "# Package Imports\r\n",
        "import random\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from urllib.request import urlopen # The default requests package\r\n",
        "import requests # For making GitHub requests\r\n",
        "from pprint import pprint # For pretty printing\r\n",
        "from pathlib import Path # The Path class\r\n",
        "import re  # For multiple delimiters seperating strings\r\n",
        "\r\n",
        "# System Packages \r\n",
        "import base64\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import time\r\n",
        "\r\n",
        "# Tensorflow \r\n",
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "\r\n",
        "# For the more advanced requests\r\n",
        "sys.path.append(\"./PyGithub\");\r\n",
        "from github import Github\r\n",
        "from getpass import getpass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.6/dist-packages (1.54.1)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.6/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.6/dist-packages (from PyGithub) (1.2.11)\n",
            "Requirement already satisfied: pyjwt<2.0 in /usr/local/lib/python3.6/dist-packages (from PyGithub) (1.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated->PyGithub) (1.12.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N624BVtBsmy",
        "cellView": "form"
      },
      "source": [
        "#@title Function Definitions 1\r\n",
        "# Recursively Import the Data (AUTOMATIC)\r\n",
        "\r\n",
        "def _decode_and_write(file__, path_):\r\n",
        "    data = file__.decoded_content\r\n",
        "    data = data.decode('utf-8')[1:]\r\n",
        "    with open(path_, 'w') as writefile:\r\n",
        "        writefile.write(data) \r\n",
        "    data = data.splitlines()\r\n",
        "    data_rows = []\r\n",
        "    for count, word in enumerate(data):\r\n",
        "        if count>0:\r\n",
        "            data_rows.append(word.split(','))\r\n",
        "    data = pd.DataFrame(data_rows)\r\n",
        "    data = data.to_numpy()\r\n",
        "    return data\r\n",
        "\r\n",
        "\r\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\r\n",
        "    \"\"\"\r\n",
        "    Function for importing the github file\r\n",
        "    path_name: str\r\n",
        "    output: None\r\n",
        "    \"\"\"\r\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\r\n",
        "    username = \"MikeMNelhams\"\r\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\r\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\r\n",
        "    if main_branch_bool.lower() in yes_synonyms: \r\n",
        "        branch = \"master\" \r\n",
        "    else: \r\n",
        "        branch = \"PROTOTYPE\"\r\n",
        "\r\n",
        "    user = g.get_user(username)\r\n",
        "    r_proj_clone = 0\r\n",
        "    for repo in g.get_user().get_repos():\r\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\r\n",
        "            r_proj_clone = repo\r\n",
        "            break\r\n",
        "        # To see all the available attributes and methods\r\n",
        "        print(dir(repo))\r\n",
        "    if type(r_proj_clone) == int:\r\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\r\n",
        "        sys.exit()\r\n",
        "    print(\"Importing Github cleaned text files...\")\r\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\r\n",
        "    RAP_DATA = []\r\n",
        "    for file_ in contents:\r\n",
        "        path = file_.path\r\n",
        "        path = str(path) \r\n",
        "        # Only choose the .txt files\r\n",
        "        if path[-4:] == '.txt':\r\n",
        "            # Append the Lyrics\r\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \r\n",
        "    \r\n",
        "    temp_path = Path(path_name)\r\n",
        "    if temp_path.is_file(): \r\n",
        "        if os.stat(path_name).st_size == 0:\r\n",
        "            write_bool2 = True\r\n",
        "        else: \r\n",
        "            write_bool2 = False\r\n",
        "    else: \r\n",
        "        write_bool2 = True\r\n",
        "    \r\n",
        "    if write_bool2: \r\n",
        "        for lyric in RAP_DATA: \r\n",
        "            try:\r\n",
        "                with open(path_name, 'w') as writefile: \r\n",
        "                    writefile.write(lyric)\r\n",
        "            except: \r\n",
        "                print(\"Error, file moved/deleted during write\")\r\n",
        "        print(\"{} is now up to date!\".format(path_name))\r\n",
        "    else: \r\n",
        "        print(\"{} is already up to date!\".format(path_name))\r\n",
        "\r\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\r\n",
        "    for counter, file_ in enumerate(contents):\r\n",
        "        path = file_.path\r\n",
        "        path = str(path) \r\n",
        "\r\n",
        "        title_start = path.find('Other')\r\n",
        "        title_len = path[title_start:].find('.')\r\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\r\n",
        "        \r\n",
        "        print(\"Writing file {} {}\".format(counter, path))\r\n",
        "        temp_path = Path(path)\r\n",
        "        if temp_path.is_file():\r\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\r\n",
        "        \r\n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \r\n",
        "        data = file_.decoded_content\r\n",
        "        data = data.decode('utf-8')[1:]\r\n",
        "\r\n",
        "        with open(path, 'w') as writefile:\r\n",
        "            writefile.write(data) \r\n",
        "    print(\"All files now up to date!\")\r\n",
        "\r\n",
        "\r\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\r\n",
        "    \"\"\"\r\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \r\n",
        "    write_bool: bool\r\n",
        "    path_name: str\r\n",
        "    output: None\r\n",
        "    \"\"\"\r\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\r\n",
        "    username = \"MikeMNelhams\"\r\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\r\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\r\n",
        "    if main_branch_bool.lower() in yes_synonyms: \r\n",
        "        branch = \"master\" \r\n",
        "    else: \r\n",
        "        branch = \"PROTOTYPE\"\r\n",
        "\r\n",
        "    user = g.get_user(username)\r\n",
        "    r_proj_clone = 0\r\n",
        "    for repo in g.get_user().get_repos():\r\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\r\n",
        "            r_proj_clone = repo\r\n",
        "            break\r\n",
        "        # To see all the available attributes and methods\r\n",
        "        print(dir(repo))\r\n",
        "    \r\n",
        "    if not r_proj_clone:\r\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\r\n",
        "        sys.exit()\r\n",
        "\r\n",
        "    print(\"Importing editing csv files...\")\r\n",
        "\r\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\r\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\r\n",
        "    for counter, file_ in enumerate(contents):\r\n",
        "        path = file_.path \r\n",
        "        path = str(path)\r\n",
        "        title_start = path.find('Other')\r\n",
        "        title_len = path[title_start:].find('.')\r\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\r\n",
        "        print(\"Writing file {} {}\".format(counter, name))\r\n",
        "        if name.lower() == \"censors.csv\":\r\n",
        "            censors = _decode_and_write(file_, name)\r\n",
        "        elif name.lower() == \"capitals.csv\":\r\n",
        "            capitals = _decode_and_write(file_, name)\r\n",
        "        else: \r\n",
        "            _decode_and_write(file_, name)\r\n",
        "    print(\"All editing csv files are up to date!\")\r\n",
        "\r\n",
        "    print(\"Importing Github uncleaned text files...\")\r\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\r\n",
        "\r\n",
        "    RAP_DATA = []\r\n",
        "    rap_lyric_names = []\r\n",
        "\r\n",
        "    for file_ in contents:\r\n",
        "        path = file_.path\r\n",
        "        path = str(path) \r\n",
        "        # Only choose the .txt files\r\n",
        "        if path[-4:] == '.txt':\r\n",
        "            # Append the name\r\n",
        "            title_start = path.find('UNCLEAN')\r\n",
        "            title_len = path[title_start:].find('.')\r\n",
        "            name = path[title_start + 8:title_start + title_len]\r\n",
        "            if name[-2:] == 'UC':\r\n",
        "                name = name[:-2]\r\n",
        "            rap_lyric_names.append(name) \r\n",
        "\r\n",
        "        # Append the Lyrics\r\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \r\n",
        "        \r\n",
        "    # Remove the \\ufeff at the beginning O(n)\r\n",
        "    for count, lyric in enumerate(RAP_DATA): \r\n",
        "        RAP_DATA[count] = lyric[1:]\r\n",
        "\r\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\r\n",
        "    for count in range(len(RAP_DATA)): \r\n",
        "        for i in range(len(censors[0:])):\r\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\r\n",
        "        for i in range(len(capitals[0:])):\r\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\r\n",
        "\r\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\r\n",
        "    cleaned_names = []\r\n",
        "    for counter, file_ in enumerate(contents):\r\n",
        "        path = file_.path\r\n",
        "        path = str(path) \r\n",
        "        print(\"File {} \".format(counter + 1) + path)\r\n",
        "        # Only choose the .txt files\r\n",
        "        if path[-4:] == '.txt':\r\n",
        "            # Append the name\r\n",
        "            title_start = path.find('CLEAN')\r\n",
        "            title_len = path[title_start:].find('.')\r\n",
        "        name = path[title_start + 6:title_start + title_len]\r\n",
        "        if name[-2:] == 'CL':\r\n",
        "            name = name[:-2]\r\n",
        "        cleaned_names.append(name) \r\n",
        "\r\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\r\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \r\n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\r\n",
        "    # print(rap_lyric_names)\r\n",
        "    # print(cleaned_names)\r\n",
        "    print(\"Commiting files to github...\")\r\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \r\n",
        "        if new_name in cleaned_names: \r\n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\r\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\r\n",
        "        else:\r\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\r\n",
        "\r\n",
        "    if write_bool: \r\n",
        "        print(\"Writing text file to: {}\".format(path_name))\r\n",
        "        with open(path_name, 'w') as writefile:\r\n",
        "            for lyric in RAP_DATA:\r\n",
        "                writefile.write(lyric)\r\n",
        "\r\n",
        "\r\n",
        "def generate_rap(Vocabulary):\r\n",
        "    start_line = line_generator(Vocabulary)\r\n",
        "    all_other_lines = [line_generator(Vocabulary) for i in range(999)]\r\n",
        "    rap = [start_line]\r\n",
        "    \r\n",
        "    for i in range (19):\r\n",
        "        next_line = next_line_stop_count(rap[len(rap) - 1], all_other_lines)\r\n",
        "        all_other_lines.remove(next_line)\r\n",
        "        rap.append(next_line)\r\n",
        "    return rap\r\n",
        "\r\n",
        "\r\n",
        "# Generate text\r\n",
        "def line_generator(Vocabulary):\r\n",
        "    index = 1\r\n",
        "    chain = {}\r\n",
        "    count = random.randint(6, 12) # Why 6 and 12?\r\n",
        "    \r\n",
        "    for word in Vocabulary[index:]:\r\n",
        "        key = Vocabulary[index-1]\r\n",
        "        if key in chain:\r\n",
        "            chain[key].append(word)\r\n",
        "        else:\r\n",
        "            chain[key] = [word]\r\n",
        "        index += 1\r\n",
        "        \r\n",
        "    word1 = random.choice(list(chain.keys()))\r\n",
        "    line = word1.capitalize()\r\n",
        "\r\n",
        "    while len(line.split(' ')) < count:\r\n",
        "        word2 = random.choice(chain[word1])\r\n",
        "        word1 = word2\r\n",
        "        line += ' ' + word2.lower()\r\n",
        "    return line\r\n",
        "\r\n",
        "\r\n",
        "# Rhyme Functions\r\n",
        "def reverse_syllable_extract(text):\r\n",
        "    sy_form = []\r\n",
        "    characters = [char for char in text]\r\n",
        "    sylls = ['a', 'e', 'i', 'o', 'u']\r\n",
        "    for x in characters:\r\n",
        "        if x in sylls:\r\n",
        "            sy_form.append(x)\r\n",
        "    sy_form.reverse()\r\n",
        "    return sy_form\r\n",
        "\r\n",
        "\r\n",
        "def rev_syllable_stop_count(text1, text2):\r\n",
        "    count = True \r\n",
        "    i = 0\r\n",
        "    counter = 0\r\n",
        "    syll1 = reverse_syllable_extract(text1)\r\n",
        "    syll2 = reverse_syllable_extract(text2)\r\n",
        "    while count == True:\r\n",
        "        if i < min(len(syll1), len(syll2)) and syll1[i] == syll2[i]:\r\n",
        "            counter += 1\r\n",
        "            i += 1\r\n",
        "        else:\r\n",
        "            count = False\r\n",
        "    return counter\r\n",
        "\r\n",
        "\r\n",
        "def next_line_stop_count(start_line, lines):\r\n",
        "    sy_lines = []\r\n",
        "    for i in lines:\r\n",
        "        sy_lines.append(rev_syllable_stop_count(start_line, i))\r\n",
        "    choice = sy_lines[0]\r\n",
        "    count = 0\r\n",
        "    for i in range(len(sy_lines)):\r\n",
        "        if sy_lines[i] > choice:\r\n",
        "            choice = sy_lines[i]\r\n",
        "    return lines[sy_lines.index(choice)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfpwMfaNR4vb",
        "cellView": "form"
      },
      "source": [
        "#@title Function Definitions RNN\r\n",
        "def _split_input_target(sequence):\r\n",
        "    input_text = sequence[:-1]\r\n",
        "    target_text = sequence[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "def train_RNN(sequence_length=100, batch_length=64, path_=\"AllLyrics.txt\", rnn_units_ = 1024, epochs=10):\r\n",
        "    # https://www.tensorflow.org/tutorials/text/text_generation\r\n",
        "    # Step 1\r\n",
        "    text = open(path_, 'rb').read().decode(encoding='utf-8')\r\n",
        "    vocab = sorted(set(text))  # List of vocab\r\n",
        "    chars = tf.strings.unicode_split(re.split('\\s\\n',text), input_encoding='UTF-8')  # Byte vector list object \r\n",
        "    ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))  # List of ID object \r\n",
        "\r\n",
        "    # Step 2\r\n",
        "    ids = ids_from_chars(chars)  # Convert from tokens + paddings\r\n",
        "    chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)  # Does something\r\n",
        "    chars = chars_from_ids(ids)\r\n",
        "\r\n",
        "    # Step 3\r\n",
        "    all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\r\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\r\n",
        "\r\n",
        "    def _text_from_ids(ids):\r\n",
        "        return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\r\n",
        "\r\n",
        "    # Test 1\r\n",
        "    # for ids in ids_dataset.take(10):\r\n",
        "    #    print(chars_from_ids(ids).numpy().decode('utf-8'))\r\n",
        "\r\n",
        "    # Step 4\r\n",
        "    seq_length = sequence_length\r\n",
        "    examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "    sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "    # Test 2 \r\n",
        "    # for seq in sequences.take(1):\r\n",
        "    #    print(chars_from_ids(seq))\r\n",
        "    # Test 3 - Back from tokens \r\n",
        "    # for seq in sequences.take(5):\r\n",
        "    #    print(_text_from_ids(seq).numpy())\r\n",
        "\r\n",
        "    # Step 5 - Create training examples and targets\r\n",
        "    dataset = sequences.map(_split_input_target)\r\n",
        "    # Test 3\r\n",
        "    # for input_example, target_example in  dataset.take(1):\r\n",
        "    #    print(\"Input :\", _text_from_ids(input_example).numpy())\r\n",
        "    #    print(\"Target:\", _text_from_ids(target_example).numpy())\r\n",
        "    \r\n",
        "    # Step 6 - Create training batches \r\n",
        "    # Batch size\r\n",
        "    BATCH_SIZE = batch_length\r\n",
        "\r\n",
        "    # Buffer size to shuffle the dataset\r\n",
        "    # (TF data is designed to work with possibly infinite sequences,\r\n",
        "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "    # it maintains a buffer in which it shuffles elements).\r\n",
        "    BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "    dataset = (\r\n",
        "        dataset\r\n",
        "        .shuffle(BUFFER_SIZE)\r\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
        "    \r\n",
        "    # Step 7 - Build the model\r\n",
        "    # Length of the vocabulary in chars\r\n",
        "    vocab_size = len(vocab)\r\n",
        "\r\n",
        "    # The embedding dimension\r\n",
        "    embedding_dim = 256\r\n",
        "\r\n",
        "    # Number of RNN units\r\n",
        "    rnn_units = rnn_units_\r\n",
        "\r\n",
        "    class MyModel(tf.keras.Model):\r\n",
        "        def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "            super().__init__(self)\r\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "            self.gru = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                        return_sequences=True, \r\n",
        "                                        return_state=True)\r\n",
        "            self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "        def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "            x = inputs\r\n",
        "            x = self.embedding(x, training=training)\r\n",
        "            if states is None:\r\n",
        "                states = self.gru.get_initial_state(x)\r\n",
        "            x, states = self.gru(x, initial_state=states, training=training)\r\n",
        "            x = self.dense(x, training=training)\r\n",
        "\r\n",
        "            if return_state:\r\n",
        "                return x, states\r\n",
        "            else: \r\n",
        "                return x\r\n",
        "\r\n",
        "    model = MyModel(\r\n",
        "        # Be sure the vocabulary size matches the `StringLookup` layers.\r\n",
        "        vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "        embedding_dim=embedding_dim,\r\n",
        "        rnn_units=rnn_units)\r\n",
        "    \r\n",
        "    for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "        example_batch_predictions = model(input_example_batch)\r\n",
        "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\r\n",
        "    \r\n",
        "    model.summary()\r\n",
        "\r\n",
        "    # Step 8 - Train the model\r\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "    mean_loss = example_batch_loss.numpy().mean()\r\n",
        "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "    print(\"Mean loss:        \", mean_loss)\r\n",
        "\r\n",
        "    model.compile(optimizer='adam', loss=loss)\r\n",
        "\r\n",
        "    # Directory where the checkpoints will be saved\r\n",
        "    checkpoint_dir = './training_checkpoints'\r\n",
        "    # Name of the checkpoint files\r\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "        filepath=checkpoint_prefix,\r\n",
        "        save_weights_only=True)\r\n",
        "    \r\n",
        "    EPOCHS = epochs  # Number of training steps \r\n",
        "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\r\n",
        "\r\n",
        "    class OneStep(tf.keras.Model):\r\n",
        "        def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\r\n",
        "            super().__init__()\r\n",
        "            self.temperature=temperature\r\n",
        "            self.model = model\r\n",
        "            self.chars_from_ids = chars_from_ids\r\n",
        "            self.ids_from_chars = ids_from_chars\r\n",
        "\r\n",
        "            # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "            skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\r\n",
        "            sparse_mask = tf.SparseTensor(\r\n",
        "                # Put a -inf at each bad index.\r\n",
        "                values=[-float('inf')]*len(skip_ids),\r\n",
        "                indices = skip_ids,\r\n",
        "                # Match the shape to the vocabulary\r\n",
        "                dense_shape=[len(ids_from_chars.get_vocabulary())]) \r\n",
        "            self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
        "\r\n",
        "    \r\n",
        "        @tf.function\r\n",
        "        def generate_one_step(self, inputs, states=None):\r\n",
        "            # Convert strings to token IDs.\r\n",
        "            input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
        "            input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
        "\r\n",
        "            # Run the model.\r\n",
        "            # predicted_logits.shape is [batch, char, next_char_logits] \r\n",
        "            predicted_logits, states =  self.model(inputs=input_ids, states=states, \r\n",
        "                                                return_state=True)\r\n",
        "            # Only use the last prediction.\r\n",
        "            predicted_logits = predicted_logits[:, -1, :]\r\n",
        "            predicted_logits = predicted_logits/self.temperature\r\n",
        "            # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "            predicted_logits = predicted_logits + self.prediction_mask\r\n",
        "\r\n",
        "            # Sample the output logits to generate token IDs.\r\n",
        "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
        "            predicted_ids = tf.squeeze(predicted_ids, axis=-1)\r\n",
        "\r\n",
        "            # Convert from token ids to characters\r\n",
        "            predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
        "\r\n",
        "            # Return the characters and model state.\r\n",
        "            return predicted_chars, states\r\n",
        "    \r\n",
        "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\r\n",
        "\r\n",
        "    start = time.time()\r\n",
        "    states = None\r\n",
        "    next_char = tf.constant([' ', ' ', ' ', ' ', ' '])\r\n",
        "    result = [next_char]\r\n",
        "\r\n",
        "    for n in range(1000):\r\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "        result.append(next_char)\r\n",
        "\r\n",
        "    result = tf.strings.join(result)\r\n",
        "    end = time.time()\r\n",
        "\r\n",
        "    print(result, '\\n\\n' + '_'*80)\r\n",
        "    print(f\"\\nRun time: {end - start}\")\r\n",
        "\r\n",
        "    tf.saved_model.save(one_step_model, 'one_step')\r\n",
        "\r\n",
        "    print(tf.strings.join(result).numpy().decode(\"utf-8\"))\r\n",
        "\r\n",
        "\r\n",
        "def generate_rap_RNN(line_start='It', num_chars=100):\r\n",
        "    # Currently does not work, since google colab is very finicky with saving big files...\r\n",
        "    one_step_reloaded = tf.saved_model.load('one_step')\r\n",
        "    states = None\r\n",
        "    next_char = tf.constant([line_start])\r\n",
        "    result = [next_char]\r\n",
        "\r\n",
        "    for _ in range(num_chars):\r\n",
        "        next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\r\n",
        "        result.append(next_char)\r\n",
        "\r\n",
        "    print(tf.strings.join(result).numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrB2lq589b9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9rQe8tjquMA",
        "cellView": "form",
        "outputId": "291bbb8f-6c35-4f9a-86d5-f6233650b01f"
      },
      "source": [
        "#@title Example model\r\n",
        "train_RNN(epochs=50)\r\n",
        "#generate_rap_RNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 109) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     multiple                  27904     \n",
            "_________________________________________________________________\n",
            "gru_12 (GRU)                 multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             multiple                  111725    \n",
            "=================================================================\n",
            "Total params: 4,077,933\n",
            "Trainable params: 4,077,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Prediction shape:  (64, 100, 109)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.6926003\n",
            "Epoch 1/50\n",
            "211/211 [==============================] - 13s 53ms/step - loss: 3.2603\n",
            "Epoch 2/50\n",
            "211/211 [==============================] - 12s 53ms/step - loss: 2.0553\n",
            "Epoch 3/50\n",
            "211/211 [==============================] - 12s 54ms/step - loss: 1.7632\n",
            "Epoch 4/50\n",
            "211/211 [==============================] - 13s 54ms/step - loss: 1.5943\n",
            "Epoch 5/50\n",
            "211/211 [==============================] - 13s 55ms/step - loss: 1.4751\n",
            "Epoch 6/50\n",
            "211/211 [==============================] - 13s 55ms/step - loss: 1.3852\n",
            "Epoch 7/50\n",
            "211/211 [==============================] - 13s 56ms/step - loss: 1.3080\n",
            "Epoch 8/50\n",
            "211/211 [==============================] - 13s 57ms/step - loss: 1.2274\n",
            "Epoch 9/50\n",
            "211/211 [==============================] - 13s 57ms/step - loss: 1.1575\n",
            "Epoch 10/50\n",
            "211/211 [==============================] - 13s 57ms/step - loss: 1.0805\n",
            "Epoch 11/50\n",
            "211/211 [==============================] - 13s 58ms/step - loss: 0.9978\n",
            "Epoch 12/50\n",
            "211/211 [==============================] - 13s 58ms/step - loss: 0.9301\n",
            "Epoch 13/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.8567\n",
            "Epoch 14/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.7889\n",
            "Epoch 15/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.7263\n",
            "Epoch 16/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.6726\n",
            "Epoch 17/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.6282\n",
            "Epoch 18/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.5959\n",
            "Epoch 19/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.5673\n",
            "Epoch 20/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.5385\n",
            "Epoch 21/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.5261\n",
            "Epoch 22/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.5096\n",
            "Epoch 23/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4992\n",
            "Epoch 24/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4875\n",
            "Epoch 25/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4808\n",
            "Epoch 26/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4707\n",
            "Epoch 27/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4712\n",
            "Epoch 28/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4614\n",
            "Epoch 29/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4621\n",
            "Epoch 30/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4590\n",
            "Epoch 31/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4554\n",
            "Epoch 32/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4588\n",
            "Epoch 33/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4553\n",
            "Epoch 34/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4566\n",
            "Epoch 35/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4515\n",
            "Epoch 36/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4527\n",
            "Epoch 37/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4534\n",
            "Epoch 38/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4559\n",
            "Epoch 39/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4571\n",
            "Epoch 40/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4681\n",
            "Epoch 41/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4626\n",
            "Epoch 42/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4567\n",
            "Epoch 43/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4600\n",
            "Epoch 44/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4645\n",
            "Epoch 45/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4688\n",
            "Epoch 46/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4671\n",
            "Epoch 47/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4802\n",
            "Epoch 48/50\n",
            "211/211 [==============================] - 14s 60ms/step - loss: 0.4754\n",
            "Epoch 49/50\n",
            "211/211 [==============================] - 13s 59ms/step - loss: 0.4763\n",
            "Epoch 50/50\n",
            "211/211 [==============================] - 14s 59ms/step - loss: 0.4837\n",
            "tf.Tensor(\n",
            "[b\" it's elloyour fate\\nIn a B Fly shut\\nIs this pic, long eight switch b*****\\nWanta suck my d*ck in the trailer valuman\\nTrynn's to watch my hustle with surprising, I ain't love me when I wait, we're in the Bee'?\\nBleed stop right we look released\\nBut yet that's why my breathed and attrestep with me like K either New York Knine He the NBB\\nThese creats suckers\\nTy life these b*****es\\nYeah yeah\\nJung Doneven\\nYall eat piranin and the album though My flashy sy\\nCant let it go it aint my fault\\nI wouldnt rap like a bum of your kids good\\nBut I can rending but I just been busy\\nThe sunsetter doors\\nYeah he in the vanson people better gree enough\\nThis very money I make you white bemome bring ball ninjas\\nWhat you wanna see\\nBaby the type to be fricked with\\nThey gone tome back this the way you lie\\nI got game spitta end\\nShouth hair to about honey Jam\\nAnd it was naked Lavor Keep em regal\\nIm maybe what she a standin to men\\nThank a death weeke club up when keep ya didnt firs What\\nYou know I scrutch for sex is lik\"\n",
            " b\" Jesus get with Aname it)\\nBut squeeze your hand, I realized say away from the women huh she tryin' to surp that\\nWhen I'm in frigged your counter called me Monical years old. And I wanna, Biggie Smalls the right b*ck got it\\nShe got a windffind the king of the tap take away from me, too fagua\\nIt's no way that you'll always lookin' for you\\nTo all the pot on the domn Michael Jackson's\\nMack a dream, dat b***** good things, don't frick with it\\nNow if not them little figures Ninjas be really broke blessed you\\nWhat happened Too smoked the door cryin\\nBut still ain't my fourcovaron\\nCause when they steamined them poops who's back in time\\nBefore I even got my ride from me in the fast last\\nSee your fittery this is the post raps my visions\\nWhen I played the black b***** parkia\\nGot liquer than the peopleeler was just comin up\\nGot a new blogges tells me then your ghost workin too kind of brotzess\\nAwards what you laudafrickin they bouncie\\nEverybody from the 313\\nPut your mother fricking downs\\nBe never go\"\n",
            " b\" not all, Bubblims\\nCall me Rappe ya\\nGirl, that money livel ninjai\\nIn the elvivorue with something man it\\nSeems like this Fuck the b***** up in crazy\\nI get blue Nah or G-I-mou at a toll on my Sky is missing\\nThe bullpoop we be dog wides til somebody killed the future ladies\\nI know Mack activite skeet restrictin and had a strap\\nAnd I know that Linda Hailie and I don't know\\nOh Biggie gimme one more time I see the lotto\\nRight in a can rush for me\\nSee everything you soft that Danging aint no stomach about her\\nNamed Damn this Puloponnin at the God amotins\\nAint got no problems we seen hits you next and bein broke as the hard way\\nFrank he telling me Im the cate emerice\\nIf yo I eater since I call out With House\\nTry to tempta throw up b*****\\nWe go always true I\\nI could make it round the leadest from nothin motherfricker reciress\\nNow you gons be on your whole life\\nWhats beef Beef is when you die\\nLast year I dont wanna stict my momma was shut up and verse\\nMy minds over and it got has poop like a pan\"\n",
            " b\" coming up for the titchen\\nOne more head thats a crash week we gon act me\\nIf you was choked reverse to broke or give ma them sheffed now I know shurt me in this world of\\nGot Biggie Smalls Julium real\\nGod Blunt Ye get that I think you feel like Jackin Cerena\\nPlease ask Ludacris renoin overal\\nEremes to get on the streets like I was dumb but my d*ck Auto\\nMake it hot\\nThe Remoon Malks\\nCause Rover Millin Went hove\\nThats he is not the CoPbaday\\nSometimes I wanna see in that Ye in the streets is my terrigler\\nHe used to eat about the contunects of hello while\\nBut we gon have you looking for you So slip Go me\\nThis spill if I was always send a milembern\\na hundred Jerry Jam Duechion on Taki My face\\nMy heart brought inter people reach as he\\nLike who you though high in the sky\\nWhen I repast my foul des rolle\\nWho you think you fricking with her man did he shoulda talk bout you Barked it from the ordinars\\nYou all prolly Hellow\\nSextermant girl so you checkin em I see\\nIs like the kind and just layin' on a\"\n",
            " b\" gone off the wall out\\nWhere the players but Ill creep What\\nNightmmanin Francl To Flow Ms Feelin From Young Moneys Let Chorus\\nI aint no good gonna do the whole neighborhood everyone in the blow and let me live around\\nOn the place we the price off\\nMy clique dama frontins that act\\nMy girl pape that confidence\\nis long everybody loves to root for a new wrist\\nShe said I woulda show it her me\\nI empays suemment wind ballin like a long time\\nIve been outta the crew\\nYou cum delovie that would eve used to hustle burs\\nThey claim my sex at least Hole in my enerica Is that THO No Favor\\nIt was just a crip to the I\\nUnforted cards of poop When you head profise\\nHarp women try to act a folk FDA\\nWhen I was driving around with a slim That He that Daddy Baby future into Remolacs and account than Peter Poppa tryin to provide Laney new tickets\\nSpell a chill like a planer\\nSo Imma redied this rap stains\\nDamn same this b***** anybody scrutble with really\\nFuckin Nickits swell to stop swerve off\\nIf I ain't getting \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.641752004623413\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.train_RNN.<locals>.OneStep object at 0x7fb80032c0b8>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.train_RNN.<locals>.OneStep object at 0x7fb80032c0b8>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_12_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_12_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " it's elloyour fate\n",
            "In a B Fly shut\n",
            "Is this pic, long eight switch b*****\n",
            "Wanta suck my d*ck in the trailer valuman\n",
            "Trynn's to watch my hustle with surprising, I ain't love me when I wait, we're in the Bee'?\n",
            "Bleed stop right we look released\n",
            "But yet that's why my breathed and attrestep with me like K either New York Knine He the NBB\n",
            "These creats suckers\n",
            "Ty life these b*****es\n",
            "Yeah yeah\n",
            "Jung Doneven\n",
            "Yall eat piranin and the album though My flashy sy\n",
            "Cant let it go it aint my fault\n",
            "I wouldnt rap like a bum of your kids good\n",
            "But I can rending but I just been busy\n",
            "The sunsetter doors\n",
            "Yeah he in the vanson people better gree enough\n",
            "This very money I make you white bemome bring ball ninjas\n",
            "What you wanna see\n",
            "Baby the type to be fricked with\n",
            "They gone tome back this the way you lie\n",
            "I got game spitta end\n",
            "Shouth hair to about honey Jam\n",
            "And it was naked Lavor Keep em regal\n",
            "Im maybe what she a standin to men\n",
            "Thank a death weeke club up when keep ya didnt firs What\n",
            "You know I scrutch for sex is lik Jesus get with Aname it)\n",
            "But squeeze your hand, I realized say away from the women huh she tryin' to surp that\n",
            "When I'm in frigged your counter called me Monical years old. And I wanna, Biggie Smalls the right b*ck got it\n",
            "She got a windffind the king of the tap take away from me, too fagua\n",
            "It's no way that you'll always lookin' for you\n",
            "To all the pot on the domn Michael Jackson's\n",
            "Mack a dream, dat b***** good things, don't frick with it\n",
            "Now if not them little figures Ninjas be really broke blessed you\n",
            "What happened Too smoked the door cryin\n",
            "But still ain't my fourcovaron\n",
            "Cause when they steamined them poops who's back in time\n",
            "Before I even got my ride from me in the fast last\n",
            "See your fittery this is the post raps my visions\n",
            "When I played the black b***** parkia\n",
            "Got liquer than the peopleeler was just comin up\n",
            "Got a new blogges tells me then your ghost workin too kind of brotzess\n",
            "Awards what you laudafrickin they bouncie\n",
            "Everybody from the 313\n",
            "Put your mother fricking downs\n",
            "Be never go not all, Bubblims\n",
            "Call me Rappe ya\n",
            "Girl, that money livel ninjai\n",
            "In the elvivorue with something man it\n",
            "Seems like this Fuck the b***** up in crazy\n",
            "I get blue Nah or G-I-mou at a toll on my Sky is missing\n",
            "The bullpoop we be dog wides til somebody killed the future ladies\n",
            "I know Mack activite skeet restrictin and had a strap\n",
            "And I know that Linda Hailie and I don't know\n",
            "Oh Biggie gimme one more time I see the lotto\n",
            "Right in a can rush for me\n",
            "See everything you soft that Danging aint no stomach about her\n",
            "Named Damn this Puloponnin at the God amotins\n",
            "Aint got no problems we seen hits you next and bein broke as the hard way\n",
            "Frank he telling me Im the cate emerice\n",
            "If yo I eater since I call out With House\n",
            "Try to tempta throw up b*****\n",
            "We go always true I\n",
            "I could make it round the leadest from nothin motherfricker reciress\n",
            "Now you gons be on your whole life\n",
            "Whats beef Beef is when you die\n",
            "Last year I dont wanna stict my momma was shut up and verse\n",
            "My minds over and it got has poop like a pan coming up for the titchen\n",
            "One more head thats a crash week we gon act me\n",
            "If you was choked reverse to broke or give ma them sheffed now I know shurt me in this world of\n",
            "Got Biggie Smalls Julium real\n",
            "God Blunt Ye get that I think you feel like Jackin Cerena\n",
            "Please ask Ludacris renoin overal\n",
            "Eremes to get on the streets like I was dumb but my d*ck Auto\n",
            "Make it hot\n",
            "The Remoon Malks\n",
            "Cause Rover Millin Went hove\n",
            "Thats he is not the CoPbaday\n",
            "Sometimes I wanna see in that Ye in the streets is my terrigler\n",
            "He used to eat about the contunects of hello while\n",
            "But we gon have you looking for you So slip Go me\n",
            "This spill if I was always send a milembern\n",
            "a hundred Jerry Jam Duechion on Taki My face\n",
            "My heart brought inter people reach as he\n",
            "Like who you though high in the sky\n",
            "When I repast my foul des rolle\n",
            "Who you think you fricking with her man did he shoulda talk bout you Barked it from the ordinars\n",
            "You all prolly Hellow\n",
            "Sextermant girl so you checkin em I see\n",
            "Is like the kind and just layin' on a gone off the wall out\n",
            "Where the players but Ill creep What\n",
            "Nightmmanin Francl To Flow Ms Feelin From Young Moneys Let Chorus\n",
            "I aint no good gonna do the whole neighborhood everyone in the blow and let me live around\n",
            "On the place we the price off\n",
            "My clique dama frontins that act\n",
            "My girl pape that confidence\n",
            "is long everybody loves to root for a new wrist\n",
            "She said I woulda show it her me\n",
            "I empays suemment wind ballin like a long time\n",
            "Ive been outta the crew\n",
            "You cum delovie that would eve used to hustle burs\n",
            "They claim my sex at least Hole in my enerica Is that THO No Favor\n",
            "It was just a crip to the I\n",
            "Unforted cards of poop When you head profise\n",
            "Harp women try to act a folk FDA\n",
            "When I was driving around with a slim That He that Daddy Baby future into Remolacs and account than Peter Poppa tryin to provide Laney new tickets\n",
            "Spell a chill like a planer\n",
            "So Imma redied this rap stains\n",
            "Damn same this b***** anybody scrutble with really\n",
            "Fuckin Nickits swell to stop swerve off\n",
            "If I ain't getting \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5On3o1fS-hIG",
        "outputId": "8a54b748-b06a-4e95-dc5a-fffc16b92de1"
      },
      "source": [
        "# Import all of Mike's lyrics. PATKEY: 5ae2446bd5828c9e27deb3865118d9e783aa6e15\n",
        "import_github()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key \n",
            "Main Branch: Yes or No? Yes\n",
            "Importing editing csv files...\n",
            "Writing file 0 capitals.csv\n",
            "Writing file 1 censors.csv\n",
            "All editing csv files are up to date!\n",
            "Importing Github uncleaned text files...\n",
            "File 1 RapLyrics/CLEAN/Kanye_WestCL.txt\n",
            "File 2 RapLyrics/CLEAN/Lil_WayneCL.txt\n",
            "File 3 RapLyrics/CLEAN/eminemCL.txt\n",
            "File 4 RapLyrics/CLEAN/ludacrisCL.txt\n",
            "File 5 RapLyrics/CLEAN/nicki-minajCL.txt\n",
            "File 6 RapLyrics/CLEAN/notorious-bigCL.txt\n",
            "Commiting files to github...\n",
            "Writing text file to: AllLyrics.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWGUsctgpGVp"
      },
      "source": [
        "# Extract all of Mike's lyrics. (Markov Model)\r\n",
        "Text = open(\"AllLyrics.txt\", \"r\").read()\r\n",
        "Vocabulary = ''.join([i for i in Text if not i.isdigit()]).replace(\"\\n\", \" \").split(' ')\r\n",
        "# print(Vocabulary)\r\n",
        "# line_generator(Vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0WbObRPyyIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80976617-3ce6-45eb-987e-f54aa6be090b"
      },
      "source": [
        "for line in generate_rap(Vocabulary):\r\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Violate me thats my decimals damn ma i reach\n",
            "Theyll laugh when i dare you can hook b that drive da\n",
            "Ills holdin it doesnt ran up what i mean\n",
            "Making money and it rub it doesnt it say i mean\n",
            "Peel more money much you love ma little nasty\n",
            "Lap chronic by tomorrow all i sing real\n",
            "Brains blown out my bag steamin girls steady\n",
            "Prey hey dad place when i b smoking mics like a\n",
            "Creepin two rides than biggie biggie smalls\n",
            "Lavender and on toes b***** in jay getting high reach\n",
            "Friend rog whats your clip hit me as\n",
            "Thou shalt not guilty richer than\n",
            "Happennin out here for the winter far\n",
            "Rosewood and wednesdays while we so why cause im dead\n",
            "Cop all day another struggle i yeah\n",
            "May be classified in the stash\n",
            "Comer then i rep smokin sacks up the funk ill be what\n",
            "Camera rollin blunts left but my nine at\n",
            "Dukes a place why it on sky is heat\n",
            "Bacon backspins to make the stars\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9SRl8X-hIQ"
      },
      "source": [
        "# Now move to generate a 20 line rap\r\n",
        "# Will generate 1000 lines using line_generator then from first generated line pick next best line by rhyme 19 times\r\n",
        "# I've left it as a list with strings as the lines, we can have fun improving things if we think this is a good start\r\n",
        "# The above has taken ages to load as has made 1000 lines then compared each one to try and find the best next!\r\n",
        "# Also the vocab is much longer now"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}