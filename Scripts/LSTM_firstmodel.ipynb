{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-basic",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyEQkMNVxPS7",
        "outputId": "d1b3bef2-b159-4c24-f2cc-e79689a4108e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "#@title Import Statements\n",
        "!pip install PyGithub\n",
        "\n",
        "# Package Imports\n",
        "import random\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from urllib.request import urlopen # The default requests package\n",
        "import requests # For making GitHub requests\n",
        "from pprint import pprint # For pretty printing\n",
        "from pathlib import Path # The Path class\n",
        "\n",
        "# For the more advanced requests\n",
        "import base64\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"./PyGithub\");\n",
        "from github import Github\n",
        "from getpass import getpass\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.7/dist-packages (1.54.1)\n",
            "Requirement already satisfied: pyjwt<2.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.7.1)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from PyGithub) (1.2.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.12.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HfS1LmNxiMp"
      },
      "source": [
        "#@title Function Definitions\n",
        "# Recursively Import the Data (AUTOMATIC)\n",
        "\n",
        "def _decode_and_write(file__, path_):\n",
        "    data = file__.decoded_content\n",
        "    data = data.decode('utf-8')[1:]\n",
        "    with open(path_, 'w') as writefile:\n",
        "        writefile.write(data) \n",
        "    data = data.splitlines()\n",
        "    data_rows = []\n",
        "    for count, word in enumerate(data):\n",
        "        if count>0:\n",
        "            data_rows.append(word.split(','))\n",
        "    data = pd.DataFrame(data_rows)\n",
        "    data = data.to_numpy()\n",
        "    return data\n",
        "\n",
        "\n",
        "def import_github(path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for importing the github file\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "    print(\"Importing Github cleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    RAP_DATA = []\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the Lyrics\n",
        "            RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "    \n",
        "    temp_path = Path(path_name)\n",
        "    if temp_path.is_file(): \n",
        "        if os.stat(path_name).st_size == 0:\n",
        "            write_bool2 = True\n",
        "        else: \n",
        "            write_bool2 = False\n",
        "    else: \n",
        "        write_bool2 = True\n",
        "    \n",
        "    if write_bool2: \n",
        "        for lyric in RAP_DATA: \n",
        "            try:\n",
        "                with open(path_name, 'w') as writefile: \n",
        "                    writefile.write(lyric)\n",
        "            except: \n",
        "                print(\"Error, file moved/deleted during write\")\n",
        "        print(\"{} is now up to date!\".format(path_name))\n",
        "    else: \n",
        "        print(\"{} is already up to date!\".format(path_name))\n",
        "    \n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        path = path[title_start + 6:title_start + title_len + 4]\n",
        "\n",
        "        print(\"Writing file {} {}\".format(counter, path))\n",
        "        temp_path = Path(path)\n",
        "        if temp_path.is_file():\n",
        "            with open(path,'w'): pass # Cheeky way to clear the file if it exists\n",
        "        \n",
        "        # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy array \n",
        "        data = file_.decoded_content\n",
        "        data = data.decode('utf-8')[1:]\n",
        "\n",
        "        with open(path, 'w') as writefile:\n",
        "            writefile.write(data) \n",
        "        print(\"All files now up to date!\")\n",
        "\n",
        "\n",
        "def update_github(write_bool=False, path_name=\"AllLyrics.txt\"):\n",
        "    \"\"\"\n",
        "    Function for updating the github file, by cleaning the lyrics, optional write to txt file. \n",
        "    write_bool: bool\n",
        "    path_name: str\n",
        "    output: None\n",
        "    \"\"\"\n",
        "    g = Github(getpass(\"Enter your PAT key \")) # Enter your PAT Key.\n",
        "    username = \"MikeMNelhams\"\n",
        "    main_branch_bool = input(\"Main Branch: Yes or No? \")\n",
        "    yes_synonyms = [\"yes\", \"y\", \"yh\", \"1\", \"true\"]\n",
        "    if main_branch_bool.lower() in yes_synonyms: \n",
        "        branch = \"master\" \n",
        "    else: \n",
        "        branch = \"PROTOTYPE\"\n",
        "\n",
        "    user = g.get_user(username)\n",
        "    r_proj_clone = 0\n",
        "    for repo in g.get_user().get_repos():\n",
        "        if repo.name == \"ai-group-project-Team-JMJM\":\n",
        "            r_proj_clone = repo\n",
        "            break\n",
        "        # To see all the available attributes and methods\n",
        "        print(dir(repo))\n",
        "    \n",
        "    if not r_proj_clone:\n",
        "        print(\"ai-group-project-Team-JMJM not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(\"Importing editing csv files...\")\n",
        "\n",
        "    # Split the long string into a list of lines, then split by words, then put into a csv, then to numpy arr\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/Other\", ref=branch)\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path \n",
        "        path = str(path)\n",
        "        title_start = path.find('Other')\n",
        "        title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len + 4]\n",
        "        print(\"Writing file {} {}\".format(counter, name))\n",
        "        if name.lower() == \"censors.csv\":\n",
        "            censors = _decode_and_write(file_, path)\n",
        "        elif name.lower() == \"capitals.csv\":\n",
        "            capitals = _decode_and_write(file_, path)\n",
        "        else: \n",
        "            _decode_and_write(file_, path)\n",
        "    print(\"All editing csv files are up to date!\")\n",
        "\n",
        "    print(\"Importing Github uncleaned text files...\")\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/UNCLEAN\", ref=branch)\n",
        "\n",
        "    RAP_DATA = []\n",
        "    rap_lyric_names = []\n",
        "\n",
        "    for file_ in contents:\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('UNCLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "            name = path[title_start + 8:title_start + title_len]\n",
        "            if name[-2:] == 'UC':\n",
        "                name = name[:-2]\n",
        "            rap_lyric_names.append(name) \n",
        "\n",
        "        # Append the Lyrics\n",
        "        RAP_DATA.append(file_.decoded_content.decode(\"utf-8\")) \n",
        "        \n",
        "    # Remove the \\ufeff at the beginning O(n)\n",
        "    for count, lyric in enumerate(RAP_DATA): \n",
        "        RAP_DATA[count] = lyric[1:]\n",
        "\n",
        "    # Censor the profanities O(n*m + n*m2) m > m2 xor m2 > m\n",
        "    for count in range(len(RAP_DATA)): \n",
        "        for i in range(len(censors[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(censors[i, 0]), str(censors[i, 1]))\n",
        "        for i in range(len(capitals[0:])):\n",
        "            RAP_DATA[count] = RAP_DATA[count].replace(str(capitals[i, 0]), str(capitals[i, 1]))\n",
        "\n",
        "    contents = r_proj_clone.get_contents(\"RapLyrics/CLEAN\", ref=branch)\n",
        "    cleaned_names = []\n",
        "    for counter, file_ in enumerate(contents):\n",
        "        path = file_.path\n",
        "        path = str(path) \n",
        "        print(\"File {} \".format(counter + 1) + path)\n",
        "        # Only choose the .txt files\n",
        "        if path[-4:] == '.txt':\n",
        "            # Append the name\n",
        "            title_start = path.find('CLEAN')\n",
        "            title_len = path[title_start:].find('.')\n",
        "        name = path[title_start + 6:title_start + title_len]\n",
        "        if name[-2:] == 'CL':\n",
        "            name = name[:-2]\n",
        "        cleaned_names.append(name) \n",
        "\n",
        "    # ALL OF THE EDITING IS DONE IN THE 'PROTOTYPE BRANCH' to avoid overwriting import changes\n",
        "    # If the (now cleaned) rap_lyrics name is new (not in cleaned_names), then we want to create that as a new file \n",
        "    # If the (now cleaned) rap_lyrics name is NOT new (not in cleaned_names), then we want to update the file\n",
        "    # print(rap_lyric_names)\n",
        "    # print(cleaned_names)\n",
        "    print(\"Commiting files to github...\")\n",
        "    for counter, new_name in enumerate(rap_lyric_names): \n",
        "        if new_name in cleaned_names: \n",
        "            duplicate = r_proj_clone.get_contents(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), ref=branch)\n",
        "            r_proj_clone.update_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], duplicate.sha, branch=branch)\n",
        "        else:\n",
        "            r_proj_clone.create_file(\"RapLyrics/CLEAN/{}CL.txt\".format(new_name), \"This was uploaded automatically via pipeline\", RAP_DATA[counter], branch=branch)\n",
        "\n",
        "    if write_bool: \n",
        "        print(\"Writing text file to: {}\".format(path_name))\n",
        "        with open(path_name, 'w') as writefile:\n",
        "            for lyric in RAP_DATA:\n",
        "                writefile.write(lyric)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtfMMwOdxmt_",
        "outputId": "6a0619ea-c70e-4188-88f1-71f9a1eae960"
      },
      "source": [
        "# Import all of Mike's lyrics. PATKEY: 5ae2446bd5828c9e27deb3865118d9e783aa6e15\n",
        "import_github()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your PAT key ··········\n",
            "Main Branch: Yes or No? Yes\n",
            "Importing Github cleaned text files...\n",
            "AllLyrics.txt is already up to date!\n",
            "Writing file 0 capitals.csv\n",
            "All files now up to date!\n",
            "Writing file 1 censors.csv\n",
            "All files now up to date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VjURXnfxsKJ"
      },
      "source": [
        "Text = open(\"AllLyrics.txt\", \"r\").read()\n",
        "# turn text to lower case to reduce vocabulary\n",
        "Text = Text.lower()\n",
        "with open(\"AllLyrics.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "# bars is a list containing each line in dataset in lowercase\n",
        "bars = [x.strip().lower() for x in content]\n",
        "stripped_bars = [word.split() for word in bars]\n",
        "# Vocabulary is a list of all words in the dataset\n",
        "Vocabulary = ''.join([i for i in Text if not i.isdigit()]).replace(\"\\n\",\" \").split(' ')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLpVvXPQ2rHL"
      },
      "source": [
        "no_of_bars = len(bars)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRyjmQeyaAG"
      },
      "source": [
        "# word_count is a function creating a list of words ranked in order of most used\n",
        "# could think about removing certain words to create more accurate raps as model won't learn well from words used very infrequently\n",
        "def word_count(lyrics):\n",
        "  a = {}\n",
        "  for word in Vocabulary:\n",
        "    if word in a:\n",
        "      a[word] += 1\n",
        "    else:\n",
        "      a[word] = 1\n",
        "  return a\n",
        "word_dict = word_count(Vocabulary)\n",
        "sort_dict = sorted(word_dict.items(), key = lambda x: x[1], reverse = True)\n",
        "# Top 20 words\n",
        "sort_dict1 = sort_dict[:40]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIweRLWJqfax"
      },
      "source": [
        "words = sorted(list(set(Vocabulary)))\n",
        "int_to_word = { i : words[i] for i in range(len(words))}\n",
        "# Need to reverse this at the end to reverse numbers back into words\n",
        "word_to_int = { words[i] : i for i in range(len(words))}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzfKleYrygft"
      },
      "source": [
        "# create a function that converts bars into a sequence of unique integers\n",
        "# List of all unique vocabulary in alphabetical order\n",
        "\n",
        "def words_to_integers(bar, Vocabulary):\n",
        "  encode = []\n",
        "  stripped_bar = [word.split() for word in bar]\n",
        "  for i in range(no_of_bars):\n",
        "    seq = []\n",
        "    seq.append([word_to_int[word] for word in stripped_bar[i]])\n",
        "    encode.append(seq)\n",
        "\n",
        "  encode = sum(encode, [])\n",
        "  return encode\n",
        "  \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RHaklDv5tZN",
        "outputId": "dcc9875a-9f0d-458b-a361-93e0c5c8726d"
      },
      "source": [
        "vocab_size = len(words) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiP3CQKQrIR7"
      },
      "source": [
        "def sentence_to_integer(bar):\n",
        "    stripped_bar = [word.split() for word in [bar]]\n",
        "    stripped_bar = sum(stripped_bar, [])\n",
        "    seq = []\n",
        "    seq.append([word_to_int[word] for word in stripped_bar])\n",
        "    seq = sum(seq, [])\n",
        "    return seq\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Log92J2ZdU"
      },
      "source": [
        "sequences = []\n",
        "for line in bars:\n",
        "    token_list = sentence_to_integer(line)\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i+1]\n",
        "        sequences.append(n_gram_seq)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABAKPmzErRT_",
        "outputId": "137ae324-5761-4884-fc1a-c73deb159695"
      },
      "source": [
        "padding_length = max([len(line) for line in sequences])\n",
        "print(padding_length)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VXihUw13aAL"
      },
      "source": [
        "def sentence_to_integer(bar):\n",
        "    stripped_bar = [word.split() for word in [bar]]\n",
        "    stripped_bar = sum(stripped_bar, [])\n",
        "    seq = []\n",
        "    seq.append([word_to_int[word] for word in stripped_bar])\n",
        "    seq = sum(seq, [])\n",
        "    return seq\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PV9FBI_EsEQ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences = np.array(pad_sequences(sequences, maxlen = padding_length, padding = 'pre'))\n",
        "# Remove last word from each line\n",
        "x_train = sequences[:,:-1]\n",
        "# Last word is used as the label\n",
        "y_train = sequences[:,-1]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = vocab_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYl-zrM3r1nI",
        "outputId": "fdca2eb4-506d-4eb7-a800-3263af243720"
      },
      "source": [
        "sequences[:,-1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2246, 4839,   68, ..., 1929, 2025, 4886], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWoLXV1Qr4E9",
        "outputId": "b13a8c54-f4d2-47d3-df15-e0a23cbde8e4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256, input_length = padding_length - 1))\n",
        "model.add(LSTM(128, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256)),\n",
        "model.add(Dropout(0.2)),\n",
        "model.add(Dense(vocab_size, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 65, 256)           1297920   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 65, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 65, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5070)              1302990   \n",
            "=================================================================\n",
            "Total params: 3,192,270\n",
            "Trainable params: 3,192,270\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4UNHRZtBrp",
        "outputId": "aa19c03d-8305-46be-bbb0-c97bc5e3fb99"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs = 200, batch_size = 128)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "320/320 [==============================] - 13s 31ms/step - loss: 6.9679 - accuracy: 0.0464\n",
            "Epoch 2/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 6.2793 - accuracy: 0.0512\n",
            "Epoch 3/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 6.0734 - accuracy: 0.0630\n",
            "Epoch 4/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 5.7897 - accuracy: 0.0790\n",
            "Epoch 5/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 5.5413 - accuracy: 0.0985\n",
            "Epoch 6/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 5.3806 - accuracy: 0.1156\n",
            "Epoch 7/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 5.1857 - accuracy: 0.1335\n",
            "Epoch 8/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 4.9951 - accuracy: 0.1513\n",
            "Epoch 9/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 4.8389 - accuracy: 0.1632\n",
            "Epoch 10/200\n",
            "320/320 [==============================] - 10s 30ms/step - loss: 4.6751 - accuracy: 0.1783\n",
            "Epoch 11/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 4.5165 - accuracy: 0.1941\n",
            "Epoch 12/200\n",
            "320/320 [==============================] - 10s 30ms/step - loss: 4.3855 - accuracy: 0.2065\n",
            "Epoch 13/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 4.2401 - accuracy: 0.2204\n",
            "Epoch 14/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 4.0915 - accuracy: 0.2338\n",
            "Epoch 15/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.9780 - accuracy: 0.2502\n",
            "Epoch 16/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.8760 - accuracy: 0.2564\n",
            "Epoch 17/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.7535 - accuracy: 0.2710\n",
            "Epoch 18/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.6558 - accuracy: 0.2823\n",
            "Epoch 19/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.5398 - accuracy: 0.2983\n",
            "Epoch 20/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.4584 - accuracy: 0.3119\n",
            "Epoch 21/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.3505 - accuracy: 0.3267\n",
            "Epoch 22/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.2551 - accuracy: 0.3449\n",
            "Epoch 23/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.1918 - accuracy: 0.3538\n",
            "Epoch 24/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 3.1166 - accuracy: 0.3625\n",
            "Epoch 25/200\n",
            "320/320 [==============================] - 10s 30ms/step - loss: 3.0528 - accuracy: 0.3735\n",
            "Epoch 26/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.9574 - accuracy: 0.3898\n",
            "Epoch 27/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.8925 - accuracy: 0.4001\n",
            "Epoch 28/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.8439 - accuracy: 0.4086\n",
            "Epoch 29/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.7829 - accuracy: 0.4193\n",
            "Epoch 30/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.7278 - accuracy: 0.4282\n",
            "Epoch 31/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.6779 - accuracy: 0.4358\n",
            "Epoch 32/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.6182 - accuracy: 0.4454\n",
            "Epoch 33/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.5904 - accuracy: 0.4500\n",
            "Epoch 34/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.5376 - accuracy: 0.4591\n",
            "Epoch 35/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.5201 - accuracy: 0.4637\n",
            "Epoch 36/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.4721 - accuracy: 0.4684\n",
            "Epoch 37/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.4404 - accuracy: 0.4735\n",
            "Epoch 38/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.4046 - accuracy: 0.4796\n",
            "Epoch 39/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.3433 - accuracy: 0.4871\n",
            "Epoch 40/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.3478 - accuracy: 0.4877\n",
            "Epoch 41/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.2963 - accuracy: 0.4996\n",
            "Epoch 42/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.2689 - accuracy: 0.4995\n",
            "Epoch 43/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.2478 - accuracy: 0.5057\n",
            "Epoch 44/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.1989 - accuracy: 0.5160\n",
            "Epoch 45/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.1820 - accuracy: 0.5147\n",
            "Epoch 46/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.1450 - accuracy: 0.5261\n",
            "Epoch 47/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.1166 - accuracy: 0.5288\n",
            "Epoch 48/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.0958 - accuracy: 0.5339\n",
            "Epoch 49/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.0749 - accuracy: 0.5391\n",
            "Epoch 50/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.0600 - accuracy: 0.5362\n",
            "Epoch 51/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.0254 - accuracy: 0.5434\n",
            "Epoch 52/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 2.0026 - accuracy: 0.5470\n",
            "Epoch 53/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.9773 - accuracy: 0.5519\n",
            "Epoch 54/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.9562 - accuracy: 0.5580\n",
            "Epoch 55/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.9406 - accuracy: 0.5597\n",
            "Epoch 56/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.9151 - accuracy: 0.5649\n",
            "Epoch 57/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8974 - accuracy: 0.5678\n",
            "Epoch 58/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8848 - accuracy: 0.5664\n",
            "Epoch 59/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8775 - accuracy: 0.5706\n",
            "Epoch 60/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8501 - accuracy: 0.5764\n",
            "Epoch 61/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8316 - accuracy: 0.5822\n",
            "Epoch 62/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.8085 - accuracy: 0.5856\n",
            "Epoch 63/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7899 - accuracy: 0.5852\n",
            "Epoch 64/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7692 - accuracy: 0.5880\n",
            "Epoch 65/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7686 - accuracy: 0.5880\n",
            "Epoch 66/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7490 - accuracy: 0.5883\n",
            "Epoch 67/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7501 - accuracy: 0.5952\n",
            "Epoch 68/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7350 - accuracy: 0.5947\n",
            "Epoch 69/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.7053 - accuracy: 0.6034\n",
            "Epoch 70/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6746 - accuracy: 0.6098\n",
            "Epoch 71/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6901 - accuracy: 0.6041\n",
            "Epoch 72/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6593 - accuracy: 0.6105\n",
            "Epoch 73/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6520 - accuracy: 0.6109\n",
            "Epoch 74/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6482 - accuracy: 0.6100\n",
            "Epoch 75/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6249 - accuracy: 0.6161\n",
            "Epoch 76/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.6108 - accuracy: 0.6172\n",
            "Epoch 77/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5959 - accuracy: 0.6209\n",
            "Epoch 78/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5727 - accuracy: 0.6267\n",
            "Epoch 79/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5857 - accuracy: 0.6246\n",
            "Epoch 80/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5829 - accuracy: 0.6265\n",
            "Epoch 81/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5554 - accuracy: 0.6286\n",
            "Epoch 82/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5550 - accuracy: 0.6290\n",
            "Epoch 83/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5283 - accuracy: 0.6366\n",
            "Epoch 84/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5215 - accuracy: 0.6363\n",
            "Epoch 85/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.5228 - accuracy: 0.6325\n",
            "Epoch 86/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4967 - accuracy: 0.6407\n",
            "Epoch 87/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4799 - accuracy: 0.6436\n",
            "Epoch 88/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4796 - accuracy: 0.6454\n",
            "Epoch 89/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4782 - accuracy: 0.6450\n",
            "Epoch 90/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4603 - accuracy: 0.6449\n",
            "Epoch 91/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4661 - accuracy: 0.6470\n",
            "Epoch 92/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4526 - accuracy: 0.6473\n",
            "Epoch 93/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4340 - accuracy: 0.6517\n",
            "Epoch 94/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4310 - accuracy: 0.6490\n",
            "Epoch 95/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4072 - accuracy: 0.6560\n",
            "Epoch 96/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4014 - accuracy: 0.6577\n",
            "Epoch 97/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4129 - accuracy: 0.6538\n",
            "Epoch 98/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3867 - accuracy: 0.6618\n",
            "Epoch 99/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.4028 - accuracy: 0.6575\n",
            "Epoch 100/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3827 - accuracy: 0.6599\n",
            "Epoch 101/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3606 - accuracy: 0.6648\n",
            "Epoch 102/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3418 - accuracy: 0.6723\n",
            "Epoch 103/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3614 - accuracy: 0.6658\n",
            "Epoch 104/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3590 - accuracy: 0.6676\n",
            "Epoch 105/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3264 - accuracy: 0.6740\n",
            "Epoch 106/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3314 - accuracy: 0.6707\n",
            "Epoch 107/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3126 - accuracy: 0.6757\n",
            "Epoch 108/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3064 - accuracy: 0.6760\n",
            "Epoch 109/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.3014 - accuracy: 0.6797\n",
            "Epoch 110/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2909 - accuracy: 0.6788\n",
            "Epoch 111/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2874 - accuracy: 0.6815\n",
            "Epoch 112/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2974 - accuracy: 0.6779\n",
            "Epoch 113/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2895 - accuracy: 0.6795\n",
            "Epoch 114/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2673 - accuracy: 0.6834\n",
            "Epoch 115/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2648 - accuracy: 0.6839\n",
            "Epoch 116/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2649 - accuracy: 0.6858\n",
            "Epoch 117/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2502 - accuracy: 0.6863\n",
            "Epoch 118/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2399 - accuracy: 0.6895\n",
            "Epoch 119/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2253 - accuracy: 0.6969\n",
            "Epoch 120/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2407 - accuracy: 0.6950\n",
            "Epoch 121/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2021 - accuracy: 0.7006\n",
            "Epoch 122/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2199 - accuracy: 0.6908\n",
            "Epoch 123/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2196 - accuracy: 0.6935\n",
            "Epoch 124/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.2013 - accuracy: 0.6980\n",
            "Epoch 125/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1881 - accuracy: 0.7000\n",
            "Epoch 126/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1955 - accuracy: 0.6985\n",
            "Epoch 127/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1957 - accuracy: 0.6976\n",
            "Epoch 128/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1876 - accuracy: 0.7023\n",
            "Epoch 129/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1760 - accuracy: 0.7011\n",
            "Epoch 130/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1740 - accuracy: 0.7034\n",
            "Epoch 131/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1735 - accuracy: 0.7034\n",
            "Epoch 132/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1630 - accuracy: 0.7032\n",
            "Epoch 133/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1657 - accuracy: 0.7049\n",
            "Epoch 134/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1572 - accuracy: 0.7052\n",
            "Epoch 135/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1413 - accuracy: 0.7085\n",
            "Epoch 136/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1501 - accuracy: 0.7085\n",
            "Epoch 137/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1246 - accuracy: 0.7172\n",
            "Epoch 138/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1192 - accuracy: 0.7158\n",
            "Epoch 139/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1315 - accuracy: 0.7111\n",
            "Epoch 140/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1215 - accuracy: 0.7162\n",
            "Epoch 141/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1316 - accuracy: 0.7116\n",
            "Epoch 142/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1192 - accuracy: 0.7148\n",
            "Epoch 143/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1067 - accuracy: 0.7186\n",
            "Epoch 144/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1099 - accuracy: 0.7174\n",
            "Epoch 145/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1057 - accuracy: 0.7192\n",
            "Epoch 146/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.1119 - accuracy: 0.7157\n",
            "Epoch 147/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0841 - accuracy: 0.7211\n",
            "Epoch 148/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0740 - accuracy: 0.7239\n",
            "Epoch 149/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0929 - accuracy: 0.7213\n",
            "Epoch 150/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0824 - accuracy: 0.7217\n",
            "Epoch 151/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0938 - accuracy: 0.7201\n",
            "Epoch 152/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0788 - accuracy: 0.7255\n",
            "Epoch 153/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0691 - accuracy: 0.7270\n",
            "Epoch 154/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0560 - accuracy: 0.7264\n",
            "Epoch 155/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0548 - accuracy: 0.7294\n",
            "Epoch 156/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0508 - accuracy: 0.7312\n",
            "Epoch 157/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0483 - accuracy: 0.7287\n",
            "Epoch 158/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0349 - accuracy: 0.7337\n",
            "Epoch 159/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0509 - accuracy: 0.7302\n",
            "Epoch 160/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0510 - accuracy: 0.7300\n",
            "Epoch 161/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0480 - accuracy: 0.7263\n",
            "Epoch 162/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0291 - accuracy: 0.7335\n",
            "Epoch 163/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0405 - accuracy: 0.7324\n",
            "Epoch 164/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0407 - accuracy: 0.7312\n",
            "Epoch 165/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0171 - accuracy: 0.7368\n",
            "Epoch 166/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0311 - accuracy: 0.7335\n",
            "Epoch 167/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0238 - accuracy: 0.7353\n",
            "Epoch 168/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0184 - accuracy: 0.7345\n",
            "Epoch 169/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0130 - accuracy: 0.7336\n",
            "Epoch 170/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0113 - accuracy: 0.7363\n",
            "Epoch 171/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9836 - accuracy: 0.7460\n",
            "Epoch 172/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 1.0005 - accuracy: 0.7382\n",
            "Epoch 173/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9887 - accuracy: 0.7437\n",
            "Epoch 174/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9947 - accuracy: 0.7403\n",
            "Epoch 175/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9981 - accuracy: 0.7408\n",
            "Epoch 176/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9969 - accuracy: 0.7403\n",
            "Epoch 177/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9698 - accuracy: 0.7477\n",
            "Epoch 178/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9911 - accuracy: 0.7402\n",
            "Epoch 179/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9768 - accuracy: 0.7445\n",
            "Epoch 180/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9862 - accuracy: 0.7425\n",
            "Epoch 181/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9807 - accuracy: 0.7432\n",
            "Epoch 182/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9684 - accuracy: 0.7476\n",
            "Epoch 183/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9708 - accuracy: 0.7451\n",
            "Epoch 184/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9830 - accuracy: 0.7464\n",
            "Epoch 185/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9744 - accuracy: 0.7452\n",
            "Epoch 186/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9779 - accuracy: 0.7439\n",
            "Epoch 187/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9814 - accuracy: 0.7465\n",
            "Epoch 188/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9663 - accuracy: 0.7487\n",
            "Epoch 189/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9536 - accuracy: 0.7514\n",
            "Epoch 190/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9519 - accuracy: 0.7501\n",
            "Epoch 191/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9706 - accuracy: 0.7463\n",
            "Epoch 192/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9337 - accuracy: 0.7542\n",
            "Epoch 193/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9506 - accuracy: 0.7518\n",
            "Epoch 194/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9450 - accuracy: 0.7512\n",
            "Epoch 195/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9344 - accuracy: 0.7563\n",
            "Epoch 196/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9495 - accuracy: 0.7517\n",
            "Epoch 197/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9293 - accuracy: 0.7566\n",
            "Epoch 198/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9280 - accuracy: 0.7551\n",
            "Epoch 199/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9219 - accuracy: 0.7565\n",
            "Epoch 200/200\n",
            "320/320 [==============================] - 10s 31ms/step - loss: 0.9189 - accuracy: 0.7585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ImBWp2JQtHYW",
        "outputId": "64fd9503-555c-4448-e2de-266d6f0e4b38"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO78Ex9O5lF_"
      },
      "source": [
        "def generateraplyrics():\n",
        "  seed_text  = random.choice(bars)\n",
        "  next_words = 100\n",
        "  token_list = sentence_to_integer(seed_text)\n",
        "  token_list = pad_sequences([token_list], maxlen = padding_length - 1, padding = 'pre')\n",
        "  predicted  = model.predict_classes(token_list, verbose = 0)\n",
        "  lyrics = int_to_word[predicted[0]]\n",
        "  for _ in range(next_words):\n",
        "      token_list = sentence_to_integer(lyrics)\n",
        "      token_list = pad_sequences([token_list], maxlen = padding_length - 1, padding = 'pre')\n",
        "      predicted  = model.predict_classes(token_list, verbose = 0)\n",
        "      lyrics += ' ' + int_to_word[predicted[0]]\n",
        "  return lyrics"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "ZsQYK-4RxjJB",
        "outputId": "4c2b8351-1f61-43c8-b6c4-f0ac07c73ac3"
      },
      "source": [
        "generateraplyrics()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'in the veins hard to explain how i maintain to put my back in the house so i can i wanna flaunt you thats right with the grime of my ninja frick with the ds crept in blastin him you dont want to slit the clits alot used to lick the clits a lot of problems never be the beamer with the goldie sound like a steelo not my steelo oh no thats not my my steelo oh i steelo not my steelo oh no thats not my no steelo bust my no dough day but this sittin bodies not my'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv4PFZPT56Kh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
