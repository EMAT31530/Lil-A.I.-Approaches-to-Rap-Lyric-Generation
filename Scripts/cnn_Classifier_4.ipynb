{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn Classifier 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbzvPR1mi6tz"
      },
      "source": [
        "# James\n",
        "\n",
        "# used same lyrics as I had in Classifier 2 for the bayes classifier - so will be 20000 words for each genre (80000 words across genres) for train \n",
        "# and test data with every 200 words roughly coming from the same song. Data inside is in order "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDk876JkEL4d",
        "outputId": "b6857463-5a55-4ebd-c70f-36c03072fbf3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MPQ9J8QEMaf"
      },
      "source": [
        "# All Rock\n",
        "rock1 = open('/content/drive/My Drive/Colab Notebooks/AllRock.txt', 'r').read()\n",
        "rock = ''.join([i for i in rock1 if not i.isdigit()]).replace(\"\\n\", \" \").lower().replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\",\", \"\").replace(\"\", \"\").replace(\"'\", \"\").replace(\")\", \"\").replace(\"(\", \"\").split(' ')\n",
        "# All Pop\n",
        "pop1 = open('/content/drive/My Drive/Colab Notebooks/AllPop.txt', 'r').read()\n",
        "pop = ''.join([i for i in pop1 if not i.isdigit()]).replace(\"\\n\", \" \").lower().replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\",\", \"\").replace(\"\", \"\").replace(\"'\", \"\").replace(\")\", \"\").replace(\"(\", \"\").split(' ')\n",
        "# All Country\n",
        "country1 = open('/content/drive/My Drive/Colab Notebooks/AllCountry.txt', 'r').read()\n",
        "country = ''.join([i for i in country1 if not i.isdigit()]).replace(\"\\n\", \" \").lower().replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\",\", \"\").replace(\"\", \"\").replace(\"'\", \"\").replace(\")\", \"\").replace(\"(\", \"\").split(' ')\n",
        "# All Rap\n",
        "rap1 = open('/content/drive/My Drive/Colab Notebooks/AllLyrics.txt', 'r').read()\n",
        "rap = ''.join([i for i in rap1 if not i.isdigit()]).replace(\"\\n\", \" \").lower().replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\",\", \"\").replace(\"\", \"\").replace(\"'\", \"\").replace(\")\", \"\").replace(\"(\", \"\").split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnmI7z3SxmNT"
      },
      "source": [
        "# We are going to prepare our data in folders as is done for the IMDB dataset - these are in the categories as seen above - each file is 200 words\n",
        "# there is a file for train and a file for test - within these there are 4 subfiles - 1 for each genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYo72v-mxR--"
      },
      "source": [
        "SONG_LENGTH = 200\n",
        "TRAIN_LENGTH = 20000 # having same test length\n",
        "# Rock\n",
        "Train_Rock = [rock[i*SONG_LENGTH:(i+1)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Country\n",
        "Train_Country = [country[i*SONG_LENGTH:(i+1)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Pop\n",
        "Train_Pop = [pop[i*SONG_LENGTH:(i+1)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Rap\n",
        "Train_Rap = [rap[i*SONG_LENGTH:(i+1)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "\n",
        "# Make Test data - same but every even 200 words\n",
        "# Rock\n",
        "Test_Rock = [rock[(i+1)*SONG_LENGTH:(i+2)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Country\n",
        "Test_Country = [country[(i+1)*SONG_LENGTH:(i+2)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Pop\n",
        "Test_Pop = [pop[(i+1)*SONG_LENGTH:(i+2)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]\n",
        "# Rap\n",
        "Test_Rap = [rap[(i+1)*SONG_LENGTH:(i+2)*SONG_LENGTH] for i in range(0,2*int(TRAIN_LENGTH/SONG_LENGTH),2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYRYILu5xSHB"
      },
      "source": [
        "# prepare by joining strings of 200 words\n",
        "train_rock = [' '.join(Train_Rock[i]) for i in range(len(Train_Rock))]\n",
        "train_country = [' '.join(Train_Country[i]) for i in range(len(Train_Country))]\n",
        "train_pop = [' '.join(Train_Pop[i]) for i in range(len(Train_Pop))]\n",
        "train_rap = [' '.join(Train_Rap[i]) for i in range(len(Train_Rap))]\n",
        "test_rock = [' '.join(Test_Rock[i]) for i in range(len(Train_Rock))]\n",
        "test_country = [' '.join(Test_Country[i]) for i in range(len(Train_Country))]\n",
        "test_pop = [' '.join(Test_Pop[i]) for i in range(len(Train_Pop))]\n",
        "test_rap = [' '.join(Test_Rap[i]) for i in range(len(Train_Rap))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqFe1SznEX1d"
      },
      "source": [
        "# Train classes\n",
        "Train_classRock = ['rock']*TRAIN_LENGTH\n",
        "Train_classCountry = ['country']*TRAIN_LENGTH\n",
        "Train_classPop = ['pop']*TRAIN_LENGTH\n",
        "Train_classRap = ['rap']*TRAIN_LENGTH\n",
        "# Test classes\n",
        "Test_classRock = ['rock']*TRAIN_LENGTH\n",
        "Test_classCountry = ['country']*TRAIN_LENGTH\n",
        "Test_classPop = ['pop']*TRAIN_LENGTH\n",
        "Test_classRap = ['rap']*TRAIN_LENGTH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2pRsu8ooFzs"
      },
      "source": [
        "# DONT KEEP RE RUNNING THIS IT WILL MAKE MORE FILES AND YOU WONT WANT THAT\n",
        "'''\n",
        "for i in range(len(Train_Rock)):\n",
        "  with open('/content/drive/My Drive/LyricData/Train/Rock/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(train_rock[i])\n",
        "for i in range(len(Train_Country)):\n",
        "  with open('/content/drive/My Drive/LyricData/Train/Country/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(train_country[i])\n",
        "for i in range(len(Train_Pop)):\n",
        "  with open('/content/drive/My Drive/LyricData/Train/Pop/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(train_pop[i])\n",
        "for i in range(len(Train_Rap)):\n",
        "  with open('/content/drive/My Drive/LyricData/Train/Rap/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(train_rap[i])\n",
        "for i in range(len(Train_Rock)):\n",
        "  with open('/content/drive/My Drive/LyricData/Test/Rock/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(test_rock[i])\n",
        "for i in range(len(Train_Country)):\n",
        "  with open('/content/drive/My Drive/LyricData/Test/Country/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(test_country[i])\n",
        "for i in range(len(Train_Pop)):\n",
        "  with open('/content/drive/My Drive/LyricData/Test/Pop/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(test_pop[i])\n",
        "for i in range(len(Train_Rap)):\n",
        "  with open('/content/drive/My Drive/LyricData/Test/Rap/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(test_rap[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNRLi3XzDIf6"
      },
      "source": [
        "# Making our Markov generated lyrics in same format\n",
        "'''\n",
        "Mar = ['Bumpin i meant for you call my ninja like',\n",
        " 'Biz dont take their baby mommas ninja frick you nasty boy you',\n",
        " 'Shifty sticks and pray and flee the frick all of you',\n",
        " 'Glocks but all ill die slow',\n",
        " 'Wondering if im askin blunt sip champagne range rover been outside for',\n",
        " 'And youre so take that crown two pounds you know',\n",
        " 'Publishing i thought i get witcha can i could cop',\n",
        " 'Miss the more cause you in the right one',\n",
        " 'Onyx and them hoes i love',\n",
        " 'Gat call me puff daddy biggie gots ta like',\n",
        " 'Everything around me shit b***** in ya imma stay yappin when',\n",
        " 'Hum all about fingers in the loot im',\n",
        " 'Rollem up heard whos this yeah keep on top sky is',\n",
        " 'Drunk of ninjaz from now drop to',\n",
        " 'Declinin windin like flypaper neighbor slow down',\n",
        " 'Expensive cars i tote my crew i only got enough heart',\n",
        " 'Lame dudes whos next move but the drugs to spit phrases thatll',\n",
        " 'Guy well its cool and your poop so hard to',\n",
        " 'Clap wit my life in ma little nasty boy',\n",
        " 'Dial you should too much better man played',\n",
        " 'Lali like that you frick doin all mcs have']\n",
        " \n",
        " \n",
        "for i in range(len(Mar)):\n",
        "  with open('/content/drive/My Drive/LyricData/Markov/'+str(i)+'.txt', 'w') as writefile:\n",
        "    writefile.write(Mar[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gimn88TzjpkP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kJkfNfYo3Xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0663bf27-4df6-427e-d685-f58d9e6e469e"
      },
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "  \"/content/drive/My Drive/LyricData/Train/\",\n",
        "  batch_size=batch_size\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "  \"/content/drive/My Drive/LyricData/Test/\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\n",
        "  \"Number of batches in raw_train_ds: %d\"\n",
        "  % tf.data.experimental.cardinality(raw_train_ds)\n",
        ")\n",
        "print(\n",
        "  \"Number of batches in raw_test_ds: %d\"\n",
        "  % tf.data.experimental.cardinality(raw_test_ds)\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 files belonging to 4 classes.\n",
            "Found 400 files belonging to 4 classes.\n",
            "Number of batches in raw_train_ds: 13\n",
            "Number of batches in raw_test_ds: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIV0pvFssUQK"
      },
      "source": [
        "# Data should be cleared before uploading\n",
        "def custom_standardisation(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "  return lowercase\n",
        "\n",
        "# Constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Now map strings to integers and set explicit maximum sequence length, since the CNNs don't work with ragged sequences\n",
        "vectorise_layer = TextVectorization(\n",
        "  standardize=custom_standardisation,\n",
        "  max_tokens=max_features,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorise_layer.adapt(text_ds)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJY5fPJWsVZL"
      },
      "source": [
        "def vectorise_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorise_layer(text), label\n",
        "\n",
        "# Vectorise data\n",
        "train_ds = raw_train_ds.map(vectorise_text)\n",
        "test_ds = raw_test_ds.map(vectorise_text)\n",
        "\n",
        "# buffer\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUC5TfmAtiv1"
      },
      "source": [
        "# model building\n",
        "# integer input for vocabulary indices\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# map vocab indices to space of dimensionality 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# single unit output layer\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuP6VPuP8DAS",
        "outputId": "21f85d20-2c27-49e7-8b47-0d44ea080c2f"
      },
      "source": [
        "epochs = 3\n",
        "# fit our model\n",
        "model.fit(train_ds, epochs=epochs)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "13/13 [==============================] - 3s 131ms/step - loss: 0.0077 - accuracy: 0.2025\n",
            "Epoch 2/3\n",
            "13/13 [==============================] - 2s 131ms/step - loss: -21.0923 - accuracy: 0.2827\n",
            "Epoch 3/3\n",
            "13/13 [==============================] - 2s 130ms/step - loss: -231.6975 - accuracy: 0.2827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff1e0a5f240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDSjQUCc8FoS",
        "outputId": "a3af66f4-e22d-4539-955f-1cab555f1f8f"
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 140s 3s/step - loss: -734.8064 - accuracy: 0.2500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-734.806396484375, 0.25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFTE9qbUFgzY",
        "outputId": "ee1db2fb-7eee-4715-c396-77faac1c5b49"
      },
      "source": [
        "# test some markov 6 generated lyrics - same as initially tested in Classifier 1 and 2\n",
        "raw_markov_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/LyricData/Markov/\", batch_size=batch_size\n",
        ")\n",
        "markov_ds = raw_markov_ds.map(vectorise_text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 21 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIiP_k_F8iC1",
        "outputId": "ebf7fcbf-adca-49e3-e90d-dd80cb1ae376"
      },
      "source": [
        "model.evaluate(markov_ds)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 9s 9s/step - loss: 1469.6125 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1469.612548828125, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQdAYFDOVKSP"
      },
      "source": [
        "# not exactly good... is this a question of more data needed? Also have only tested on 20 lines of markov generated text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6eRWbZTOxTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d752c17-93c1-4887-8eac-07239817cc08"
      },
      "source": [
        "model.predict(markov_ds)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4suI6jBU_Te"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}